#!/usr/bin/env python3
"""
ë‹¤ì¤‘ í™˜ê²½ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë¡œì»¬ CPU, GPU, í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œì˜ ì‹œìŠ¤í…œ ë™ì‘ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, Any, List, Optional
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

try:
    from src.utils.device_manager import DeviceManager
    from config.device_configs import get_optimal_config, DEVICE_CONFIGS
    import torch
    import numpy as np
except ImportError as e:
    print(f"Import ì˜¤ë¥˜: {e}")
    print("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install -r requirements_core.txt")
    sys.exit(1)

class MultiEnvironmentTester:
    """ë‹¤ì¤‘ í™˜ê²½ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = {}
        self.test_data = self._generate_test_data()
        
    def _generate_test_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±"""
        return {
            'small_tensor': torch.randn(10, 10),
            'medium_tensor': torch.randn(100, 100),
            'image_tensor': torch.randn(1, 3, 224, 224),
            'batch_tensor': torch.randn(8, 3, 224, 224)
        }
    
    def test_device_detection(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        test_name = "device_detection"
        print(f"\n=== {test_name} í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ìë™ ê°ì§€
            dm_auto = DeviceManager(device='auto')
            auto_device = dm_auto.device
            
            # CPU ê°•ì œ
            dm_cpu = DeviceManager(device='cpu')
            cpu_device = dm_cpu.device
            
            # CUDA í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                dm_cuda = DeviceManager(device='cuda')
                cuda_device = dm_cuda.device
            else:
                cuda_device = None
            
            # MPS í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
            if mps_available:
                dm_mps = DeviceManager(device='mps')
                mps_device = dm_mps.device
            else:
                mps_device = None
                
            result = {
                'status': 'success',
                'auto_device': str(auto_device),
                'cpu_device': str(cpu_device),
                'cuda_available': cuda_available,
                'cuda_device': str(cuda_device) if cuda_device else None,
                'mps_available': mps_available,
                'mps_device': str(mps_device) if mps_device else None,
                'summary': dm_auto.get_device_summary()
            }
            
            print(f"âœ… ìë™ ê°ì§€ ë””ë°”ì´ìŠ¤: {auto_device}")
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}")
            print(f"âœ… MPS ì‚¬ìš© ê°€ëŠ¥: {mps_available}")
            
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            print(f"âŒ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        self.results[test_name] = result
        return result
    
    def test_tensor_operations(self, device_manager: DeviceManager) -> Dict[str, Any]:
        """í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        device_type = device_manager.device.type
        test_name = f"tensor_operations_{device_type}"
        print(f"\n=== {test_name} í…ŒìŠ¤íŠ¸ ===")
        
        try:
            device = device_manager.device
            operations_results = {}
            
            # ê¸°ë³¸ í…ì„œ ì—°ì‚°
            for name, tensor in self.test_data.items():
                start_time = time.time()
                
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                tensor_device = tensor.to(device)
                
                # ê°„ë‹¨í•œ ì—°ì‚° ìˆ˜í–‰ (í…ì„œ ì°¨ì›ì— ë”°ë¼ ë‹¤ë¥¸ ì—°ì‚°)
                if tensor.dim() == 2:
                    # 2D í…ì„œ: í–‰ë ¬ ê³±ì…ˆ
                    result = torch.matmul(tensor_device, tensor_device.T)
                else:
                    # 4D í…ì„œ (ì´ë¯¸ì§€): element-wise ì—°ì‚°
                    result = tensor_device * tensor_device + 1.0
                    result = torch.mean(result, dim=[2, 3] if tensor.dim() == 4 else None, keepdim=True)
                
                # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™ (ê²€ì¦ìš©)
                result_cpu = result.cpu()
                
                # GPU ë™ê¸°í™” (í•„ìš”í•œ ê²½ìš°)
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                elif device.type == 'mps':
                    torch.mps.synchronize()
                
                elapsed_time = time.time() - start_time
                
                operations_results[name] = {
                    'success': True,
                    'time_ms': elapsed_time * 1000,
                    'input_shape': list(tensor.shape),
                    'output_shape': list(result.shape),
                    'memory_usage': self._get_memory_usage(device_manager)
                }
                
                print(f"âœ… {name}: {elapsed_time*1000:.2f}ms")
            
            result = {
                'status': 'success',
                'device': str(device),
                'operations': operations_results
            }
            
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc(),
                'device': str(device_manager.device)
            }
            print(f"âŒ í…ì„œ ì—°ì‚° ì‹¤íŒ¨ ({device_type}): {e}")
        
        self.results[test_name] = result
        return result
    
    def test_model_loading(self, device_manager: DeviceManager) -> Dict[str, Any]:
        """ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
        device_type = device_manager.device.type
        test_name = f"model_loading_{device_type}"
        print(f"\n=== {test_name} í…ŒìŠ¤íŠ¸ ===")
        
        try:
            device = device_manager.device
            
            # ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„±
            model = torch.nn.Sequential(
                torch.nn.Conv2d(3, 16, 3, padding=1),
                torch.nn.ReLU(),
                torch.nn.AdaptiveAvgPool2d((1, 1)),
                torch.nn.Flatten(),
                torch.nn.Linear(16, 2)
            )
            
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            start_time = time.time()
            model = model.to(device)
            model_load_time = time.time() - start_time
            
            # ì¶”ë¡  í…ŒìŠ¤íŠ¸
            test_input = self.test_data['image_tensor'].to(device)
            
            start_time = time.time()
            with torch.no_grad():
                output = model(test_input)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            elif device.type == 'mps':
                torch.mps.synchronize()
                
            inference_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            output_cpu = output.cpu()
            
            result = {
                'status': 'success',
                'device': str(device),
                'model_load_time_ms': model_load_time * 1000,
                'inference_time_ms': inference_time * 1000,
                'output_shape': list(output.shape),
                'memory_usage': self._get_memory_usage(device_manager)
            }
            
            print(f"âœ… ëª¨ë¸ ë¡œë”©: {model_load_time*1000:.2f}ms")
            print(f"âœ… ì¶”ë¡ : {inference_time*1000:.2f}ms")
            
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc(),
                'device': str(device_manager.device)
            }
            print(f"âŒ ëª¨ë¸ ë¡œë”©/ì¶”ë¡  ì‹¤íŒ¨ ({device_type}): {e}")
        
        self.results[test_name] = result
        return result
    
    def test_configuration_compatibility(self) -> Dict[str, Any]:
        """ì„¤ì • í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        test_name = "configuration_compatibility"
        print(f"\n=== {test_name} í…ŒìŠ¤íŠ¸ ===")
        
        try:
            config_results = {}
            
            # ê° ë””ë°”ì´ìŠ¤ íƒ€ì…ë³„ ì„¤ì • í…ŒìŠ¤íŠ¸
            for device_type, config in DEVICE_CONFIGS.items():
                try:
                    # DeviceManager ìƒì„± ì‹œë„
                    if device_type == 'cpu':
                        dm = DeviceManager(device='cpu')
                    elif device_type == 'cuda' and torch.cuda.is_available():
                        dm = DeviceManager(device='cuda')
                    elif device_type == 'mps' and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                        dm = DeviceManager(device='mps')
                    else:
                        config_results[device_type] = {'status': 'skipped', 'reason': 'device_not_available'}
                        continue
                    
                    # ì„¤ì • ê²€ì¦
                    dataloader_config = dm.create_dataloader_config()
                    memory_info = dm.get_memory_info()
                    
                    config_results[device_type] = {
                        'status': 'success',
                        'config': config.__dict__ if hasattr(config, '__dict__') else str(config),
                        'dataloader_config': dataloader_config,
                        'memory_info': memory_info
                    }
                    
                    print(f"âœ… {device_type} ì„¤ì • í˜¸í™˜")
                    
                except Exception as e:
                    config_results[device_type] = {
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"âŒ {device_type} ì„¤ì • ì˜¤ë¥˜: {e}")
            
            result = {
                'status': 'success',
                'device_configs': config_results
            }
            
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            print(f"âŒ ì„¤ì • í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.results[test_name] = result
        return result
    
    def test_fallback_mechanism(self) -> Dict[str, Any]:
        """í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
        test_name = "fallback_mechanism"
        print(f"\n=== {test_name} í…ŒìŠ¤íŠ¸ ===")
        
        try:
            fallback_results = {}
            
            # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë””ë°”ì´ìŠ¤ ìš”ì²­ â†’ CPU í´ë°±
            try:
                dm_invalid = DeviceManager(device='invalid_device')
                fallback_results['invalid_device'] = {
                    'status': 'success',
                    'fallback_device': str(dm_invalid.device),
                    'expected': 'cpu'
                }
                print(f"âœ… ì˜ëª»ëœ ë””ë°”ì´ìŠ¤ â†’ {dm_invalid.device} í´ë°±")
            except Exception as e:
                fallback_results['invalid_device'] = {
                    'status': 'error',
                    'error': str(e)
                }
            
            # CUDA ì—†ì„ ë•Œ â†’ CPU í´ë°±
            if not torch.cuda.is_available():
                try:
                    dm_cuda = DeviceManager(device='cuda')
                    fallback_results['cuda_unavailable'] = {
                        'status': 'success',
                        'fallback_device': str(dm_cuda.device),
                        'expected': 'cpu'
                    }
                    print(f"âœ… CUDA ì—†ìŒ â†’ {dm_cuda.device} í´ë°±")
                except Exception as e:
                    fallback_results['cuda_unavailable'] = {
                        'status': 'error',
                        'error': str(e)
                    }
            
            result = {
                'status': 'success',
                'fallback_tests': fallback_results
            }
            
        except Exception as e:
            result = {
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            print(f"âŒ í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.results[test_name] = result
        return result
    
    def _get_memory_usage(self, device_manager: DeviceManager) -> Dict[str, int]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        try:
            return device_manager.get_memory_info()
        except:
            return {'error': 'memory_info_unavailable'}
    
    def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª ë‹¤ì¤‘ í™˜ê²½ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        # 1. ë””ë°”ì´ìŠ¤ ê°ì§€ í…ŒìŠ¤íŠ¸
        self.test_device_detection()
        
        # 2. ì„¤ì • í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
        self.test_configuration_compatibility()
        
        # 3. í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸
        self.test_fallback_mechanism()
        
        # 4. ì‚¬ìš© ê°€ëŠ¥í•œ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        device_managers = []
        
        # CPUëŠ” í•­ìƒ í…ŒìŠ¤íŠ¸
        device_managers.append(DeviceManager(device='cpu'))
        
        # CUDA í…ŒìŠ¤íŠ¸
        if torch.cuda.is_available():
            device_managers.append(DeviceManager(device='cuda'))
        
        # MPS í…ŒìŠ¤íŠ¸
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device_managers.append(DeviceManager(device='mps'))
        
        # ê° ë””ë°”ì´ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for dm in device_managers:
            self.test_tensor_operations(dm)
            self.test_model_loading(dm)
        
        return self.results
    
    def generate_report(self) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ğŸ§ª ë‹¤ì¤‘ í™˜ê²½ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        report_lines.append("=" * 60)
        
        # ì „ì²´ í†µê³„
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results.values() if r.get('status') == 'success')
        
        report_lines.append(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        report_lines.append(f"  - ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        report_lines.append(f"  - ì„±ê³µ: {successful_tests}")
        report_lines.append(f"  - ì‹¤íŒ¨: {total_tests - successful_tests}")
        report_lines.append(f"  - ì„±ê³µë¥ : {successful_tests/total_tests*100:.1f}%")
        
        # ë””ë°”ì´ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼
        report_lines.append(f"\nğŸ–¥ï¸  ë””ë°”ì´ìŠ¤ ì •ë³´:")
        if 'device_detection' in self.results:
            device_info = self.results['device_detection']
            if device_info.get('status') == 'success':
                report_lines.append(f"  - ìë™ ì„ íƒ: {device_info.get('auto_device', 'N/A')}")
                report_lines.append(f"  - CUDA ì§€ì›: {device_info.get('cuda_available', False)}")
                report_lines.append(f"  - MPS ì§€ì›: {device_info.get('mps_available', False)}")
        
        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼
        report_lines.append(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìƒì„¸:")
        for test_name, result in self.results.items():
            status = result.get('status', 'unknown')
            if status == 'success':
                report_lines.append(f"  âœ… {test_name}")
            else:
                report_lines.append(f"  âŒ {test_name}: {result.get('error', 'Unknown error')}")
        
        # ì„±ëŠ¥ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        performance_tests = [k for k in self.results.keys() if 'tensor_operations' in k or 'model_loading' in k]
        if performance_tests:
            report_lines.append(f"\nâš¡ ì„±ëŠ¥ ì •ë³´:")
            for test_name in performance_tests:
                result = self.results[test_name]
                if result.get('status') == 'success':
                    device = result.get('device', 'unknown')
                    report_lines.append(f"  - {device}:")
                    
                    if 'operations' in result:
                        for op_name, op_result in result['operations'].items():
                            time_ms = op_result.get('time_ms', 0)
                            report_lines.append(f"    â€¢ {op_name}: {time_ms:.2f}ms")
                    
                    if 'inference_time_ms' in result:
                        inf_time = result['inference_time_ms']
                        report_lines.append(f"    â€¢ ëª¨ë¸ ì¶”ë¡ : {inf_time:.2f}ms")
        
        report_lines.append("\n" + "=" * 60)
        
        return "\n".join(report_lines)
    
    def save_results(self, filename: str = None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = int(time.time())
            filename = f"test_results_{timestamp}.json"
        
        results_dir = Path("test_results")
        results_dir.mkdir(exist_ok=True)
        
        filepath = results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
        
        # ë¦¬í¬íŠ¸ë„ ì €ì¥
        report_filename = filepath.with_suffix('.txt')
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(self.generate_report())
        
        print(f"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ë‹¤ì¤‘ í™˜ê²½ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
    parser.add_argument('--save', action='store_true', help='ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    try:
        tester = MultiEnvironmentTester()
        results = tester.run_all_tests()
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        report = tester.generate_report()
        print(report)
        
        # ìƒì„¸ ì¶œë ¥
        if args.verbose:
            print("\nğŸ” ìƒì„¸ ê²°ê³¼:")
            print(json.dumps(results, indent=2, ensure_ascii=False, default=str))
        
        # ê²°ê³¼ ì €ì¥
        if args.save:
            tester.save_results()
        
        # ì „ì²´ ì„±ê³µ ì—¬ë¶€ í™•ì¸
        success_count = sum(1 for r in results.values() if r.get('status') == 'success')
        total_count = len(results)
        
        if success_count == total_count:
            print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
            sys.exit(0)
        else:
            print(f"\nâš ï¸  {total_count - success_count}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        if args.verbose:
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()