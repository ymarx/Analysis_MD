"""
ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™” ì‹œìŠ¤í…œ

ëª¨ë¸ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report
)
from sklearn.model_selection import cross_val_score, GridSearchCV
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
import time
import psutil
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
import json
import pandas as pd
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ ë°ì´í„° í´ë˜ìŠ¤"""
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    auc_score: float = 0.0
    specificity: float = 0.0
    
    # ì¶”ê°€ ì§€í‘œ
    true_positives: int = 0
    true_negatives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    
    # ì„±ëŠ¥ ì§€í‘œ
    inference_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'accuracy': self.accuracy,
            'precision': self.precision,
            'recall': self.recall,
            'f1_score': self.f1_score,
            'auc_score': self.auc_score,
            'specificity': self.specificity,
            'true_positives': self.true_positives,
            'true_negatives': self.true_negatives,
            'false_positives': self.false_positives,
            'false_negatives': self.false_negatives,
            'inference_time_ms': self.inference_time_ms,
            'memory_usage_mb': self.memory_usage_mb
        }


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(self, model: Any, model_type: str = 'sklearn'):
        """
        ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            model_type: ëª¨ë¸ íƒ€ì… ('sklearn', 'pytorch')
        """
        self.model = model
        self.model_type = model_type
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if model_type == 'pytorch' and hasattr(model, 'to'):
            self.model.to(self.device)
        
        logger.info(f"ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™” - íƒ€ì…: {model_type}")
    
    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
            
        Returns:
            Tuple[np.ndarray, Optional[np.ndarray]]: (ì˜ˆì¸¡ê°’, í™•ë¥ ê°’)
        """
        if self.model_type == 'sklearn':
            predictions = self.model.predict(X)
            probabilities = None
            
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(X)[:, 1]
            elif hasattr(self.model, 'decision_function'):
                probabilities = self.model.decision_function(X)
            
        elif self.model_type == 'pytorch':
            self.model.eval()
            predictions = []
            probabilities = []
            
            with torch.no_grad():
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_size = 32
                for i in range(0, len(X), batch_size):
                    batch = X[i:i+batch_size]
                    
                    if isinstance(batch, np.ndarray):
                        batch_tensor = torch.from_numpy(batch).float().to(self.device)
                    else:
                        batch_tensor = batch.to(self.device)
                    
                    outputs = self.model(batch_tensor)
                    
                    if isinstance(outputs, dict):
                        logits = outputs['classification']
                    else:
                        logits = outputs
                    
                    probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
                    preds = torch.argmax(logits, dim=1).cpu().numpy()
                    
                    predictions.extend(preds)
                    probabilities.extend(probs)
            
            predictions = np.array(predictions)
            probabilities = np.array(probabilities)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
        
        return predictions, probabilities
    
    def evaluate(self, X: np.ndarray, y_true: np.ndarray) -> PerformanceMetrics:
        """
        ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€
        
        Args:
            X: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_true: ì‹¤ì œ ë ˆì´ë¸”
            
        Returns:
            PerformanceMetrics: ì„±ëŠ¥ ì§€í‘œ
        """
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        predictions, probabilities = self.predict(X)
        end_time = time.time()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        accuracy = accuracy_score(y_true, predictions)
        precision = precision_score(y_true, predictions, average='weighted', zero_division=0)
        recall = recall_score(y_true, predictions, average='weighted', zero_division=0)
        f1 = f1_score(y_true, predictions, average='weighted', zero_division=0)
        
        # AUC ì ìˆ˜
        auc = 0.0
        if probabilities is not None and len(np.unique(y_true)) > 1:
            try:
                auc = roc_auc_score(y_true, probabilities)
            except Exception as e:
                logger.warning(f"AUC ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_true, predictions)
        
        # ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš°
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        else:
            # ë‹¤ì¤‘ ë¶„ë¥˜ì¸ ê²½ìš° í‰ê·  íŠ¹ì´ë„
            specificity = 0.0
            tp = fp = fn = tn = 0
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
        metrics = PerformanceMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            auc_score=auc,
            specificity=specificity,
            true_positives=int(tp),
            true_negatives=int(tn),
            false_positives=int(fp),
            false_negatives=int(fn),
            inference_time_ms=(end_time - start_time) * 1000,
            memory_usage_mb=memory_after - memory_before
        )
        
        logger.info(f"ëª¨ë¸ í‰ê°€ ì™„ë£Œ - ì •í™•ë„: {accuracy:.4f}, F1: {f1:.4f}")
        
        return metrics


class VisualizationEngine:
    """ì‹œê°í™” ì—”ì§„"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        
    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray,
                             class_names: List[str] = None, title: str = "Confusion Matrix"):
        """í˜¼ë™ í–‰ë ¬ í”Œë¡¯"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names or ['Background', 'Target'],
                   yticklabels=class_names or ['Background', 'Target'])
        
        plt.title(title)
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        save_path = self.output_dir / f"confusion_matrix_{title.lower().replace(' ', '_')}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"í˜¼ë™ í–‰ë ¬ ì €ì¥: {save_path}")
    
    def plot_roc_curve(self, y_true: np.ndarray, y_scores: np.ndarray, title: str = "ROC Curve"):
        """ROC ê³¡ì„  í”Œë¡¯"""
        try:
            fpr, tpr, thresholds = roc_curve(y_true, y_scores)
            auc_score = roc_auc_score(y_true, y_scores)
            
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {auc_score:.2f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(title)
            plt.legend(loc="lower right")
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            save_path = self.output_dir / f"roc_curve_{title.lower().replace(' ', '_')}.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ROC ê³¡ì„  ì €ì¥: {save_path}")
            
        except Exception as e:
            logger.error(f"ROC ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    def plot_precision_recall_curve(self, y_true: np.ndarray, y_scores: np.ndarray,
                                   title: str = "Precision-Recall Curve"):
        """ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„  í”Œë¡¯"""
        try:
            precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
            
            plt.figure(figsize=(8, 6))
            plt.plot(recall, precision, color='blue', lw=2)
            plt.fill_between(recall, precision, alpha=0.2, color='blue')
            
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(title)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            save_path = self.output_dir / f"pr_curve_{title.lower().replace(' ', '_')}.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"PR ê³¡ì„  ì €ì¥: {save_path}")
            
        except Exception as e:
            logger.error(f"PR ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
    
    def plot_model_comparison(self, model_results: Dict[str, PerformanceMetrics],
                             title: str = "Model Performance Comparison"):
        """ëª¨ë¸ ë¹„êµ í”Œë¡¯"""
        if not model_results:
            return
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        models = list(model_results.keys())
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_score']
        
        data = []
        for model_name, result in model_results.items():
            for metric in metrics:
                data.append({
                    'Model': model_name,
                    'Metric': metric.replace('_', ' ').title(),
                    'Value': getattr(result, metric, 0.0)
                })
        
        df = pd.DataFrame(data)
        
        plt.figure(figsize=(12, 8))
        sns.barplot(data=df, x='Metric', y='Value', hue='Model')
        plt.title(title)
        plt.xticks(rotation=45)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        
        save_path = self.output_dir / f"model_comparison_{title.lower().replace(' ', '_')}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ëª¨ë¸ ë¹„êµ í”Œë¡¯ ì €ì¥: {save_path}")
    
    def plot_training_history(self, history: Dict[str, List[float]], title: str = "Training History"):
        """í›ˆë ¨ íˆìŠ¤í† ë¦¬ í”Œë¡¯"""
        if not history:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # ì†ì‹¤ í”Œë¡¯
        if 'train_loss' in history and 'val_loss' in history:
            axes[0].plot(history['train_loss'], label='Training Loss', color='blue')
            axes[0].plot(history['val_loss'], label='Validation Loss', color='red')
            axes[0].set_title('Model Loss')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
        
        # ì •í™•ë„ í”Œë¡¯
        if 'val_accuracy' in history:
            axes[1].plot(history['val_accuracy'], label='Validation Accuracy', color='green')
            axes[1].set_title('Model Accuracy')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        
        plt.suptitle(title)
        plt.tight_layout()
        
        save_path = self.output_dir / f"training_history_{title.lower().replace(' ', '_')}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"í›ˆë ¨ íˆìŠ¤í† ë¦¬ í”Œë¡¯ ì €ì¥: {save_path}")


class ModelOptimizer:
    """ëª¨ë¸ ìµœì í™”ê¸°"""
    
    def __init__(self):
        pass
    
    def optimize_hyperparameters(self, model, param_grid: Dict[str, List],
                                X_train: np.ndarray, y_train: np.ndarray,
                                cv_folds: int = 5) -> Dict[str, Any]:
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Args:
            model: ìµœì í™”í•  ëª¨ë¸
            param_grid: ë§¤ê°œë³€ìˆ˜ ê·¸ë¦¬ë“œ
            X_train: í›ˆë ¨ ë°ì´í„°
            y_train: í›ˆë ¨ ë ˆì´ë¸”
            cv_folds: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
            Dict: ìµœì í™” ê²°ê³¼
        """
        logger.info("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        
        try:
            grid_search = GridSearchCV(
                estimator=model,
                param_grid=param_grid,
                cv=cv_folds,
                scoring='f1_weighted',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(X_train, y_train)
            
            results = {
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'best_estimator': grid_search.best_estimator_,
                'cv_results': grid_search.cv_results_
            }
            
            logger.info(f"ìµœì í™” ì™„ë£Œ - ìµœê³  ì ìˆ˜: {grid_search.best_score_:.4f}")
            logger.info(f"ìµœì  ë§¤ê°œë³€ìˆ˜: {grid_search.best_params_}")
            
            return results
            
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def suggest_model_improvements(self, metrics: PerformanceMetrics) -> List[str]:
        """
        ëª¨ë¸ ê°œì„  ë°©ì•ˆ ì œì•ˆ
        
        Args:
            metrics: ì„±ëŠ¥ ì§€í‘œ
            
        Returns:
            List[str]: ê°œì„  ë°©ì•ˆ ë¦¬ìŠ¤íŠ¸
        """
        suggestions = []
        
        # ì •í™•ë„ ê¸°ë°˜ ì œì•ˆ
        if metrics.accuracy < 0.8:
            suggestions.append("ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ ê³ ë ¤ (ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬, ë” ë§ì€ íŠ¹ì§•)")
            
        if metrics.accuracy > 0.95 and metrics.f1_score < 0.9:
            suggestions.append("í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²° (SMOTE, í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •)")
        
        # ì •ë°€ë„/ì¬í˜„ìœ¨ ê¸°ë°˜ ì œì•ˆ
        if metrics.precision < 0.8:
            suggestions.append("False Positive ê°ì†Œ í•„ìš” - ë” ë³´ìˆ˜ì ì¸ ì„ê³„ê°’ ì„¤ì •")
            
        if metrics.recall < 0.8:
            suggestions.append("False Negative ê°ì†Œ í•„ìš” - ë” ë¯¼ê°í•œ ëª¨ë¸ ì‚¬ìš©")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì œì•ˆ
        if metrics.inference_time_ms > 100:
            suggestions.append("ì¶”ë¡  ì†ë„ ìµœì í™” - ëª¨ë¸ ê²½ëŸ‰í™”, ì–‘ìí™” ê³ ë ¤")
            
        if metrics.memory_usage_mb > 500:
            suggestions.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” - ëª¨ë¸ ì••ì¶•, í”„ë£¨ë‹ ê³ ë ¤")
        
        # AUC ê¸°ë°˜ ì œì•ˆ
        if metrics.auc_score < 0.8:
            suggestions.append("íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ê°œì„  - ë” ì˜ë¯¸ìˆëŠ” íŠ¹ì§• ì¶”ê°€")
        
        # ê¸°ë³¸ ì œì•ˆ
        if not suggestions:
            suggestions.append("í˜„ì¬ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ê³ ë ¤")
        
        return suggestions


class ComprehensiveEvaluator:
    """ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.visualizer = VisualizationEngine(output_dir)
        self.optimizer = ModelOptimizer()
        
        logger.info(f"ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    def evaluate_single_model(self, model: Any, model_name: str, model_type: str,
                             X_test: np.ndarray, y_test: np.ndarray) -> PerformanceMetrics:
        """ë‹¨ì¼ ëª¨ë¸ í‰ê°€"""
        logger.info(f"{model_name} ëª¨ë¸ í‰ê°€ ì‹œì‘")
        
        evaluator = ModelEvaluator(model, model_type)
        metrics = evaluator.evaluate(X_test, y_test)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì–»ê¸°
        predictions, probabilities = evaluator.predict(X_test)
        
        # ì‹œê°í™” ìƒì„±
        self.visualizer.plot_confusion_matrix(y_test, predictions, title=f"{model_name} Confusion Matrix")
        
        if probabilities is not None:
            self.visualizer.plot_roc_curve(y_test, probabilities, title=f"{model_name} ROC Curve")
            self.visualizer.plot_precision_recall_curve(y_test, probabilities, title=f"{model_name} PR Curve")
        
        logger.info(f"{model_name} í‰ê°€ ì™„ë£Œ")
        return metrics
    
    def evaluate_multiple_models(self, models_dict: Dict[str, Tuple[Any, str]],
                                X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, PerformanceMetrics]:
        """ë‹¤ì¤‘ ëª¨ë¸ í‰ê°€"""
        logger.info(f"{len(models_dict)}ê°œ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œì‘")
        
        results = {}
        
        for model_name, (model, model_type) in models_dict.items():
            try:
                metrics = self.evaluate_single_model(model, model_name, model_type, X_test, y_test)
                results[model_name] = metrics
            except Exception as e:
                logger.error(f"{model_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                results[model_name] = PerformanceMetrics()  # ê¸°ë³¸ê°’
        
        # ëª¨ë¸ ë¹„êµ ì‹œê°í™”
        self.visualizer.plot_model_comparison(results)
        
        return results
    
    def generate_evaluation_report(self, results: Dict[str, Any], 
                                 model_metrics: Dict[str, PerformanceMetrics]):
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.output_dir / 'evaluation_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ê¸°ë¬¼ íƒì§€ ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸\n\n")
            f.write(f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì „ì²´ ìš”ì•½
            f.write("## ğŸ“Š í‰ê°€ ìš”ì•½\n\n")
            if model_metrics:
                best_model = max(model_metrics.items(), key=lambda x: x[1].f1_score)
                f.write(f"- **ìµœê³  ì„±ëŠ¥ ëª¨ë¸**: {best_model[0]}\n")
                f.write(f"- **ìµœê³  F1 ì ìˆ˜**: {best_model[1].f1_score:.4f}\n")
                f.write(f"- **í‰ê°€ëœ ëª¨ë¸ ìˆ˜**: {len(model_metrics)}ê°œ\n\n")
            
            # ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼
            f.write("## ğŸ¤– ëª¨ë¸ë³„ ì„±ëŠ¥\n\n")
            for model_name, metrics in model_metrics.items():
                f.write(f"### {model_name}\n\n")
                f.write(f"| ì§€í‘œ | ê°’ |\n")
                f.write(f"|------|----|\n")
                f.write(f"| ì •í™•ë„ | {metrics.accuracy:.4f} |\n")
                f.write(f"| ì •ë°€ë„ | {metrics.precision:.4f} |\n")
                f.write(f"| ì¬í˜„ìœ¨ | {metrics.recall:.4f} |\n")
                f.write(f"| F1 ì ìˆ˜ | {metrics.f1_score:.4f} |\n")
                f.write(f"| AUC | {metrics.auc_score:.4f} |\n")
                f.write(f"| íŠ¹ì´ë„ | {metrics.specificity:.4f} |\n")
                f.write(f"| ì¶”ë¡  ì‹œê°„ | {metrics.inference_time_ms:.2f}ms |\n")
                f.write(f"| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | {metrics.memory_usage_mb:.2f}MB |\n\n")
                
                # ê°œì„  ë°©ì•ˆ
                suggestions = self.optimizer.suggest_model_improvements(metrics)
                if suggestions:
                    f.write("**ê°œì„  ë°©ì•ˆ:**\n")
                    for suggestion in suggestions:
                        f.write(f"- {suggestion}\n")
                    f.write("\n")
            
            # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
            f.write("## ğŸ¯ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n\n")
            if model_metrics:
                avg_accuracy = np.mean([m.accuracy for m in model_metrics.values()])
                avg_f1 = np.mean([m.f1_score for m in model_metrics.values()])
                
                f.write(f"- **í‰ê·  ì •í™•ë„**: {avg_accuracy:.4f}\n")
                f.write(f"- **í‰ê·  F1 ì ìˆ˜**: {avg_f1:.4f}\n\n")
                
                if avg_f1 > 0.9:
                    f.write("âœ… **ìš°ìˆ˜í•œ ì„±ëŠ¥** - ì‹¤ì œ ë°°í¬ ê³ ë ¤ ê°€ëŠ¥\n")
                elif avg_f1 > 0.8:
                    f.write("ğŸ”¶ **ì–‘í˜¸í•œ ì„±ëŠ¥** - ì¶”ê°€ ìµœì í™” í›„ ë°°í¬ ê¶Œì¥\n")
                else:
                    f.write("âš ï¸ **ì„±ëŠ¥ ê°œì„  í•„ìš”** - ëª¨ë¸ ì•„í‚¤í…ì²˜ ë˜ëŠ” ë°ì´í„° í’ˆì§ˆ ê²€í†  ê¶Œì¥\n")
        
        # JSON ê²°ê³¼ë„ ì €ì¥
        json_results = {}
        for model_name, metrics in model_metrics.items():
            json_results[model_name] = metrics.to_dict()
        
        with open(self.output_dir / 'evaluation_results.json', 'w') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")


# ì‚¬ìš© ì˜ˆì œ í•¨ìˆ˜
def run_comprehensive_evaluation(models_dict: Dict[str, Tuple[Any, str]],
                                X_test: np.ndarray, y_test: np.ndarray,
                                output_dir: Path) -> Dict[str, PerformanceMetrics]:
    """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
    evaluator = ComprehensiveEvaluator(output_dir)
    
    # ëª¨ë¸ í‰ê°€
    results = evaluator.evaluate_multiple_models(models_dict, X_test, y_test)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    evaluator.generate_evaluation_report({}, results)
    
    return results


if __name__ == "__main__":
    # ì˜ˆì œ ì‹¤í–‰
    import numpy as np
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from pathlib import Path
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X_test = np.random.random((100, 50))
    y_test = np.random.randint(0, 2, 100)
    
    # ëª¨ë¸ ìƒì„±
    models = {
        'RandomForest': (RandomForestClassifier(n_estimators=50, random_state=42), 'sklearn'),
        'SVM': (SVC(probability=True, random_state=42), 'sklearn')
    }
    
    # ëª¨ë¸ í›ˆë ¨ (ê°„ë‹¨í•œ ì˜ˆì œ)
    for name, (model, model_type) in models.items():
        X_train = np.random.random((200, 50))
        y_train = np.random.randint(0, 2, 200)
        model.fit(X_train, y_train)
    
    # í‰ê°€ ì‹¤í–‰
    output_dir = Path("data/results/evaluation_test")
    results = run_comprehensive_evaluation(models, X_test, y_test, output_dir)
    
    print("í‰ê°€ ì™„ë£Œ!")
    print(f"ê²°ê³¼ëŠ” {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")