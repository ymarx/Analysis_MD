#!/usr/bin/env python3
"""
ë¶„ì‚°ëœ ë¶„ì„ ê²°ê³¼ë“¤ í†µí•© ì •ë¦¬

ëª©ì : ì—¬ëŸ¬ ê³³ì— í©ì–´ì§„ ë¶„ì„ ê²°ê³¼ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ê³  í†µí•©
"""

import os
import shutil
from pathlib import Path
import json

def organize_scattered_results():
    """ë¶„ì‚°ëœ ë¶„ì„ ê²°ê³¼ë“¤ ì •ë¦¬"""

    print("="*70)
    print("ë¶„ì‚°ëœ ë¶„ì„ ê²°ê³¼ í†µí•© ì •ë¦¬")
    print("="*70)

    # ë¶„ì„ ê²°ê³¼ê°€ ìˆì„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ë“¤
    result_locations = [
        'outputs',
        'logs',
        'config',
        'scripts',
        'archive/analysis_results_backup'
    ]

    print(f"\nğŸ” ë¶„ì„ ê²°ê³¼ íƒì§€:")

    found_results = {}

    for location in result_locations:
        if os.path.exists(location):
            print(f"\n   ğŸ“ {location}/:")

            location_results = []
            for root, dirs, files in os.walk(location):
                for file in files:
                    if any(ext in file for ext in ['.png', '.jpg', '.md', '.json', '.csv', '.txt', '.npy']):
                        rel_path = os.path.relpath(os.path.join(root, file), location)
                        location_results.append(rel_path)
                        print(f"      ğŸ“„ {rel_path}")

            found_results[location] = location_results

    return found_results

def categorize_results(found_results):
    """ê²°ê³¼ íŒŒì¼ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""

    print(f"\nğŸ“Š ê²°ê³¼ íŒŒì¼ ë¶„ë¥˜:")

    categories = {
        'coordinate_analysis': [],
        'terrain_analysis': [],
        'ship_movement': [],
        'data_validation': [],
        'reports': [],
        'visualizations': [],
        'raw_data': []
    }

    for location, files in found_results.items():
        for file in files:
            file_lower = file.lower()

            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            if any(keyword in file_lower for keyword in ['coordinate', 'location', 'position', 'gps']):
                categories['coordinate_analysis'].append((location, file))
            elif any(keyword in file_lower for keyword in ['terrain', 'similarity', 'image', 'bmp']):
                categories['terrain_analysis'].append((location, file))
            elif any(keyword in file_lower for keyword in ['ship', 'movement', 'direction', 'path']):
                categories['ship_movement'].append((location, file))
            elif any(keyword in file_lower for keyword in ['validation', 'verification', 'check', 'test']):
                categories['data_validation'].append((location, file))
            elif file_lower.endswith(('.md', '.txt', '.pdf')):
                categories['reports'].append((location, file))
            elif file_lower.endswith(('.png', '.jpg', '.jpeg', '.svg')):
                categories['visualizations'].append((location, file))
            elif file_lower.endswith(('.npy', '.csv', '.json', '.pkl')):
                categories['raw_data'].append((location, file))

    # ë¶„ë¥˜ ê²°ê³¼ ì¶œë ¥
    for category, items in categories.items():
        if items:
            print(f"\n   ğŸ“‚ {category}: {len(items)}ê°œ")
            for location, file in items[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                print(f"      - {location}/{file}")
            if len(items) > 3:
                print(f"      ... ë° {len(items)-3}ê°œ ë”")

    return categories

def move_results_to_organized_structure(categories):
    """ë¶„ë¥˜ëœ ê²°ê³¼ë“¤ì„ ì •ë¦¬ëœ êµ¬ì¡°ë¡œ ì´ë™"""

    print(f"\nğŸ“¦ ê²°ê³¼ íŒŒì¼ ì´ë™:")

    moved_count = 0

    for category, items in categories.items():
        if not items:
            continue

        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
        target_dir = f"analysis_results/{category}"
        os.makedirs(target_dir, exist_ok=True)

        print(f"\n   ğŸ“‚ {category}:")

        for location, file in items:
            src_path = os.path.join(location, file)

            if os.path.exists(src_path):
                # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
                base_name = os.path.basename(file)
                counter = 1
                dest_path = os.path.join(target_dir, base_name)

                while os.path.exists(dest_path):
                    name, ext = os.path.splitext(base_name)
                    dest_path = os.path.join(target_dir, f"{name}_{counter}{ext}")
                    counter += 1

                try:
                    shutil.copy2(src_path, dest_path)
                    print(f"      âœ… {src_path} â†’ {dest_path}")
                    moved_count += 1
                except Exception as e:
                    print(f"      âŒ ì´ë™ ì‹¤íŒ¨: {src_path} - {e}")

    print(f"\n   ì´ ì´ë™ëœ íŒŒì¼: {moved_count}ê°œ")

def create_result_index():
    """ì •ë¦¬ëœ ê²°ê³¼ë“¤ì˜ ì¸ë±ìŠ¤ ìƒì„±"""

    print(f"\nğŸ“„ ê²°ê³¼ ì¸ë±ìŠ¤ ìƒì„±:")

    index_data = {}

    # analysis_results ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ë“¤ ì¸ë±ì‹±
    if os.path.exists('analysis_results'):
        for category in os.listdir('analysis_results'):
            category_path = os.path.join('analysis_results', category)
            if os.path.isdir(category_path):
                files = []
                for file in os.listdir(category_path):
                    file_path = os.path.join(category_path, file)
                    if os.path.isfile(file_path):
                        file_info = {
                            'name': file,
                            'path': file_path,
                            'size': os.path.getsize(file_path),
                            'modified': os.path.getmtime(file_path)
                        }
                        files.append(file_info)

                index_data[category] = {
                    'count': len(files),
                    'files': files
                }

    # JSON ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
    with open('analysis_results/RESULTS_INDEX.json', 'w', encoding='utf-8') as f:
        json.dump(index_data, f, indent=2, ensure_ascii=False)

    # ë§ˆí¬ë‹¤ìš´ ì¸ë±ìŠ¤ ìƒì„±
    markdown_content = """# ë¶„ì„ ê²°ê³¼ ì¸ë±ìŠ¤

ìƒì„±ì¼ì‹œ: {}

## ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ìš”ì•½

""".format(os.popen('date').read().strip())

    for category, info in index_data.items():
        markdown_content += f"### {category.replace('_', ' ').title()}\n\n"
        markdown_content += f"- **íŒŒì¼ ìˆ˜**: {info['count']}ê°œ\n"
        markdown_content += f"- **íŒŒì¼ ëª©ë¡**:\n"

        for file_info in info['files']:
            size_mb = file_info['size'] / (1024 * 1024)
            markdown_content += f"  - `{file_info['name']}` ({size_mb:.2f}MB)\n"

        markdown_content += "\n"

    with open('analysis_results/RESULTS_INDEX.md', 'w', encoding='utf-8') as f:
        f.write(markdown_content)

    print(f"   âœ… analysis_results/RESULTS_INDEX.json")
    print(f"   âœ… analysis_results/RESULTS_INDEX.md")

def identify_duplicate_analysis():
    """ì¤‘ë³µëœ ë¶„ì„ ê²°ê³¼ ì‹ë³„"""

    print(f"\nğŸ” ì¤‘ë³µ ë¶„ì„ ê²°ê³¼ ì‹ë³„:")

    # ìœ ì‚¬í•œ ì´ë¦„ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
    all_files = []

    if os.path.exists('analysis_results'):
        for root, dirs, files in os.walk('analysis_results'):
            for file in files:
                all_files.append(file)

    # íŒ¨í„´ë³„ ê·¸ë£¹í•‘
    patterns = {}
    for file in all_files:
        # ìˆ«ìë‚˜ ë‚ ì§œ ì œê±°í•˜ì—¬ ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ
        import re
        base_pattern = re.sub(r'[0-9_-]+', '', file.lower())
        base_pattern = re.sub(r'\.(png|jpg|md|json|csv|txt|npy)$', '', base_pattern)

        if base_pattern not in patterns:
            patterns[base_pattern] = []
        patterns[base_pattern].append(file)

    # ì¤‘ë³µ ê°€ëŠ¥ì„±ì´ ìˆëŠ” íŒ¨í„´ë“¤ ì¶œë ¥
    duplicates_found = False
    for pattern, files in patterns.items():
        if len(files) > 1:
            duplicates_found = True
            print(f"   ğŸ”„ ìœ ì‚¬ íŒ¨í„´ '{pattern}': {len(files)}ê°œ")
            for file in files:
                print(f"      - {file}")

    if not duplicates_found:
        print(f"   âœ… ì¤‘ë³µ íŒ¨í„´ ì—†ìŒ")

def check_module_status():
    """í•µì‹¬ ëª¨ë“ˆë“¤ì˜ í˜„ì¬ ìƒíƒœ í™•ì¸"""

    print(f"\nğŸ”§ í•µì‹¬ ëª¨ë“ˆ ìƒíƒœ í™•ì¸:")

    # ì¤‘ìš”í•œ ëª¨ë“ˆë“¤
    critical_modules = [
        ('src/data_processing/xtf_reader.py', 'XTF íŒŒì¼ ì½ê¸°'),
        ('src/data_processing/xtf_intensity_extractor.py', 'ê°•ë„ ë°ì´í„° ì¶”ì¶œ'),
        ('src/data_processing/preprocessor.py', 'ì „ì²˜ë¦¬'),
        ('pipeline/modules/xtf_reader.py', 'íŒŒì´í”„ë¼ì¸ XTF ë¦¬ë”'),
        ('pipeline/modules/xtf_extractor.py', 'íŒŒì´í”„ë¼ì¸ ì¶”ì¶œê¸°'),
        ('pipeline/unified_pipeline.py', 'í†µí•© íŒŒì´í”„ë¼ì¸')
    ]

    working_modules = []
    broken_modules = []

    for module_path, description in critical_modules:
        if os.path.exists(module_path):
            try:
                # ê°„ë‹¨í•œ import í…ŒìŠ¤íŠ¸
                with open(module_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # êµ¬ë¬¸ ê²€ì‚¬
                compile(content, module_path, 'exec')

                # ê¸°ë³¸ì ì¸ í´ë˜ìŠ¤/í•¨ìˆ˜ ì¡´ì¬ í™•ì¸
                has_class = 'class ' in content
                has_function = 'def ' in content

                status = "âœ… ì •ìƒ"
                if has_class or has_function:
                    working_modules.append((module_path, description))
                else:
                    status = "âš ï¸ ë¹ˆ íŒŒì¼"

                print(f"   {status} {module_path} - {description}")

            except Exception as e:
                broken_modules.append((module_path, description, str(e)))
                print(f"   âŒ ì˜¤ë¥˜ {module_path} - {description}: {e}")
        else:
            broken_modules.append((module_path, description, "íŒŒì¼ ì—†ìŒ"))
            print(f"   âŒ ì—†ìŒ {module_path} - {description}")

    print(f"\n   ğŸ“Š ëª¨ë“ˆ ìƒíƒœ:")
    print(f"      âœ… ì •ìƒ: {len(working_modules)}ê°œ")
    print(f"      âŒ ë¬¸ì œ: {len(broken_modules)}ê°œ")

    return working_modules, broken_modules

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("ğŸ“Š ë¶„ì‚°ëœ ë¶„ì„ ê²°ê³¼ í†µí•© ì •ë¦¬ ì‹œì‘")

    # 1. ë¶„ì‚°ëœ ê²°ê³¼ íƒì§€
    found_results = organize_scattered_results()

    # 2. ê²°ê³¼ ë¶„ë¥˜
    categories = categorize_results(found_results)

    # 3. ì •ë¦¬ëœ êµ¬ì¡°ë¡œ ì´ë™
    move_results_to_organized_structure(categories)

    # 4. ê²°ê³¼ ì¸ë±ìŠ¤ ìƒì„±
    create_result_index()

    # 5. ì¤‘ë³µ ë¶„ì„ ì‹ë³„
    identify_duplicate_analysis()

    # 6. ëª¨ë“ˆ ìƒíƒœ í™•ì¸
    working_modules, broken_modules = check_module_status()

    print(f"\n{'='*70}")
    print("âœ… ë¶„ì„ ê²°ê³¼ í†µí•© ì •ë¦¬ ì™„ë£Œ")
    print(f"{'='*70}")

    print(f"\nğŸ“‹ ì •ë¦¬ ê²°ê³¼:")
    print(f"   ğŸ“‚ ì •ë¦¬ëœ ì¹´í…Œê³ ë¦¬: {len([c for c in categories.values() if c])}ê°œ")
    print(f"   ğŸ“„ ì´ íŒŒì¼: {sum(len(files) for files in categories.values())}ê°œ")
    print(f"   âœ… ì •ìƒ ëª¨ë“ˆ: {len(working_modules)}ê°œ")
    print(f"   âŒ ë¬¸ì œ ëª¨ë“ˆ: {len(broken_modules)}ê°œ")

    if broken_modules:
        print(f"\nâš ï¸ ìˆ˜ì • í•„ìš”í•œ ëª¨ë“ˆë“¤:")
        for path, desc, error in broken_modules:
            print(f"   - {path}: {error}")

if __name__ == "__main__":
    main()