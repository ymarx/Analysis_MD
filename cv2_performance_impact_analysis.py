"""
OpenCV ì—†ëŠ” macOS í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ì˜í–¥ ë¶„ì„

numpy 2.0.2 í™˜ê²½ì—ì„œ OpenCV, scipy, scikit-image ì‚¬ìš© ë¶ˆê°€ ì‹œ
ìˆœìˆ˜ Python êµ¬í˜„ì˜ ì„±ëŠ¥ê³¼ ê¸°ëŠ¥ ì°¨ì´ ë¶„ì„
"""

import numpy as np
import time
import logging
from typing import Dict, List, Tuple
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DependencyPerformanceAnalyzer:
    """
    ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ ìˆœìˆ˜ Pythonìœ¼ë¡œ êµ¬í˜„ëœ ê¸°ëŠ¥ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„
    """
    
    def __init__(self):
        self.results = {}
        self.test_image = self.generate_test_image()
    
    def generate_test_image(self, size: Tuple[int, int] = (256, 256)) -> np.ndarray:
        """í…ŒìŠ¤íŠ¸ìš© í•©ì„± ì´ë¯¸ì§€ ìƒì„±"""
        np.random.seed(42)
        
        # ê¸°ë³¸ ë…¸ì´ì¦ˆ ë°°ê²½
        image = np.random.rand(*size) * 0.3
        
        # ì›í˜• ê°ì²´ ì¶”ê°€ (ê¸°ë¢° ì‹œë®¬ë ˆì´ì…˜)
        center_x, center_y = size[0] // 2, size[1] // 2
        radius = 20
        
        y, x = np.ogrid[:size[0], :size[1]]
        mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
        image[mask] = 0.8 + np.random.rand(np.sum(mask)) * 0.2
        
        # ê·¸ë¦¼ì íš¨ê³¼ ì¶”ê°€
        shadow_mask = ((x - center_x - 15)**2 + (y - center_y)**2 <= (radius * 1.5)**2) & ~mask
        image[shadow_mask] = 0.1 + np.random.rand(np.sum(shadow_mask)) * 0.1
        
        return image.astype(np.float32)
    
    def test_gaussian_filter_pure_python(self, image: np.ndarray, sigma: float = 1.0) -> Tuple[np.ndarray, float]:
        """ìˆœìˆ˜ Python ê°€ìš°ì‹œì•ˆ í•„í„° (scipy ì—†ì´)"""
        start_time = time.time()
        
        # ë‹¨ìˆœí•œ ë°•ìŠ¤ í•„í„°ë¡œ ëŒ€ì²´ (ê·¼ì‚¬)
        kernel_size = max(3, int(2 * sigma * 3))
        if kernel_size % 2 == 0:
            kernel_size += 1
        
        padding = kernel_size // 2
        padded_image = np.pad(image, padding, mode='reflect')
        
        filtered = np.zeros_like(image)
        for i in range(image.shape[0]):
            for j in range(image.shape[1]):
                # ë°•ìŠ¤ í•„í„° ì ìš©
                patch = padded_image[i:i+kernel_size, j:j+kernel_size]
                filtered[i, j] = np.mean(patch)
        
        processing_time = time.time() - start_time
        return filtered, processing_time
    
    def test_bilateral_filter_approximation(self, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """ì–‘ë°©í–¥ í•„í„°ì˜ ê°„ë‹¨í•œ ê·¼ì‚¬ (OpenCV ì—†ì´)"""
        start_time = time.time()
        
        # ê°€ìš°ì‹œì•ˆ í•„í„°ì˜ ë°˜ë³µ ì ìš©ìœ¼ë¡œ ê·¼ì‚¬
        filtered, _ = self.test_gaussian_filter_pure_python(image, sigma=0.8)
        
        # ì—£ì§€ ë³´ì¡´ì„ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
        gradient_x = np.diff(image, axis=1)
        gradient_y = np.diff(image, axis=0)
        
        # ê°•í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì˜ì—­ì—ì„œëŠ” ì›ë³¸ ìœ ì§€
        edge_threshold = np.percentile(np.abs(gradient_x), 85)
        
        processing_time = time.time() - start_time
        return filtered, processing_time
    
    def test_adaptive_histogram_equalization_pure(self, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” ìˆœìˆ˜ Python êµ¬í˜„"""
        start_time = time.time()
        
        # ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        tile_size = 64
        h, w = image.shape
        
        equalized = np.copy(image)
        
        for i in range(0, h, tile_size):
            for j in range(0, w, tile_size):
                tile = image[i:i+tile_size, j:j+tile_size]
                
                if tile.size > 0:
                    # ê° íƒ€ì¼ì— ëŒ€í•´ íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”
                    hist, bins = np.histogram(tile.flatten(), bins=256, range=(0, 1))
                    cdf = hist.cumsum()
                    cdf_normalized = cdf / (cdf[-1] + 1e-10)
                    
                    # ê· ë“±í™” ì ìš©
                    equalized_tile = np.interp(tile.flatten(), bins[:-1], cdf_normalized)
                    equalized[i:i+tile_size, j:j+tile_size] = equalized_tile.reshape(tile.shape)
        
        processing_time = time.time() - start_time
        return equalized, processing_time
    
    def test_morphological_operations_pure(self, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """í˜•íƒœí•™ì  ì—°ì‚° ìˆœìˆ˜ Python êµ¬í˜„"""
        start_time = time.time()
        
        # ë‹¨ìˆœí•œ ì¹¨ì‹/íŒ½ì°½ ì—°ì‚°
        kernel_size = 3
        padding = kernel_size // 2
        
        # ì´ì§„í™”
        binary_image = (image > np.mean(image)).astype(np.float32)
        
        # ì¹¨ì‹ (Erosion) - ìµœì†Œê°’ í•„í„°
        padded = np.pad(binary_image, padding, mode='constant', constant_values=0)
        eroded = np.zeros_like(binary_image)
        
        for i in range(binary_image.shape[0]):
            for j in range(binary_image.shape[1]):
                patch = padded[i:i+kernel_size, j:j+kernel_size]
                eroded[i, j] = np.min(patch)
        
        processing_time = time.time() - start_time
        return eroded, processing_time
    
    def test_gabor_filter_pure_implementation(self, image: np.ndarray) -> Tuple[Dict, float]:
        """ìˆœìˆ˜ Python Gabor í•„í„° êµ¬í˜„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ Gabor í•„í„° (1ê°œë§Œ í…ŒìŠ¤íŠ¸)
        frequency = 0.1
        theta = 0.0
        sigma = 2.0
        
        # Gabor ì»¤ë„ ìƒì„±
        kernel_size = 21
        center = kernel_size // 2
        x = np.arange(-center, center + 1, dtype=np.float64)
        y = np.arange(-center, center + 1, dtype=np.float64)
        X, Y = np.meshgrid(x, y)
        
        # íšŒì „ ë³€í™˜
        x_theta = X * np.cos(theta) + Y * np.sin(theta)
        y_theta = -X * np.sin(theta) + Y * np.cos(theta)
        
        # Gaussian envelope
        gaussian = np.exp(-0.5 * ((x_theta / sigma) ** 2 + (y_theta / sigma) ** 2))
        
        # ë³µì†Œ ì •í˜„íŒŒ
        real_part = gaussian * np.cos(2 * np.pi * frequency * x_theta)
        imag_part = gaussian * np.sin(2 * np.pi * frequency * x_theta)
        
        # ì»¨ë³¼ë£¨ì…˜ (ê°„ë‹¨í•œ êµ¬í˜„)
        padding = kernel_size // 2
        padded_image = np.pad(image, padding, mode='reflect')
        
        real_response = np.zeros_like(image)
        imag_response = np.zeros_like(image)
        
        # ìƒ˜í”Œë§ëœ ìœ„ì¹˜ë§Œ ê³„ì‚° (ì„±ëŠ¥ í–¥ìƒ)
        step = 4  # 4í”½ì…€ë§ˆë‹¤ ê³„ì‚°
        for i in range(0, image.shape[0], step):
            for j in range(0, image.shape[1], step):
                patch = padded_image[i:i+kernel_size, j:j+kernel_size]
                real_response[i:i+step, j:j+step] = np.sum(patch * real_part)
                imag_response[i:i+step, j:j+step] = np.sum(patch * imag_part)
        
        magnitude = np.sqrt(real_response**2 + imag_response**2)
        
        processing_time = time.time() - start_time
        
        # ê°„ë‹¨í•œ í†µê³„ëŸ‰ë§Œ ê³„ì‚°
        features = {
            'mean': np.mean(magnitude),
            'std': np.std(magnitude),
            'max': np.max(magnitude),
            'energy': np.sum(magnitude**2)
        }
        
        return features, processing_time
    
    def test_lbp_pure_implementation(self, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """ìˆœìˆ˜ Python LBP êµ¬í˜„"""
        start_time = time.time()
        
        # ë‹¨ìˆœí•œ 3x3 LBPë§Œ êµ¬í˜„
        h, w = image.shape
        lbp_image = np.zeros((h-2, w-2), dtype=np.uint8)
        
        for i in range(1, h-1):
            for j in range(1, w-1):
                center = image[i, j]
                
                # 8-ì´ì›ƒ ë¹„êµ
                neighbors = [
                    image[i-1, j-1], image[i-1, j], image[i-1, j+1],
                    image[i, j+1], image[i+1, j+1], image[i+1, j],
                    image[i+1, j-1], image[i, j-1]
                ]
                
                # LBP ì½”ë“œ ê³„ì‚°
                lbp_code = 0
                for k, neighbor in enumerate(neighbors):
                    if neighbor >= center:
                        lbp_code |= (1 << k)
                
                lbp_image[i-1, j-1] = lbp_code
        
        processing_time = time.time() - start_time
        return lbp_image, processing_time
    
    def run_comprehensive_analysis(self) -> Dict:
        """ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        logger.info("OpenCV ì—†ëŠ” í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
        
        results = {}
        
        # 1. ê°€ìš°ì‹œì•ˆ í•„í„° ì„±ëŠ¥
        logger.info("1. ê°€ìš°ì‹œì•ˆ í•„í„° í…ŒìŠ¤íŠ¸")
        filtered_gaussian, time_gaussian = self.test_gaussian_filter_pure_python(self.test_image)
        results['gaussian_filter'] = {
            'processing_time': time_gaussian,
            'pixels_per_second': (self.test_image.size / time_gaussian),
            'quality_loss': 'Medium (ë°•ìŠ¤ í•„í„° ê·¼ì‚¬)',
            'functionality': 'Basic smoothing only'
        }
        
        # 2. ì–‘ë°©í–¥ í•„í„° ê·¼ì‚¬
        logger.info("2. ì–‘ë°©í–¥ í•„í„° ê·¼ì‚¬ í…ŒìŠ¤íŠ¸")
        filtered_bilateral, time_bilateral = self.test_bilateral_filter_approximation(self.test_image)
        results['bilateral_filter'] = {
            'processing_time': time_bilateral,
            'pixels_per_second': (self.test_image.size / time_bilateral),
            'quality_loss': 'High (ì—£ì§€ ë³´ì¡´ ì œí•œì )',
            'functionality': 'Approximate edge preservation'
        }
        
        # 3. ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”
        logger.info("3. ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” í…ŒìŠ¤íŠ¸")
        equalized, time_clahe = self.test_adaptive_histogram_equalization_pure(self.test_image)
        results['adaptive_histogram'] = {
            'processing_time': time_clahe,
            'pixels_per_second': (self.test_image.size / time_clahe),
            'quality_loss': 'Low (ê¸°ë³¸ ê¸°ëŠ¥ ìœ ì§€)',
            'functionality': 'Tile-based histogram equalization'
        }
        
        # 4. í˜•íƒœí•™ì  ì—°ì‚°
        logger.info("4. í˜•íƒœí•™ì  ì—°ì‚° í…ŒìŠ¤íŠ¸")
        morphed, time_morph = self.test_morphological_operations_pure(self.test_image)
        results['morphological_ops'] = {
            'processing_time': time_morph,
            'pixels_per_second': (self.test_image.size / time_morph),
            'quality_loss': 'Medium (ë‹¨ìˆœí•œ ì»¤ë„ë§Œ)',
            'functionality': 'Basic erosion/dilation'
        }
        
        # 5. Gabor í•„í„°
        logger.info("5. Gabor í•„í„° ìˆœìˆ˜ êµ¬í˜„ í…ŒìŠ¤íŠ¸")
        gabor_features, time_gabor = self.test_gabor_filter_pure_implementation(self.test_image)
        results['gabor_filter'] = {
            'processing_time': time_gabor,
            'features_extracted': len(gabor_features),
            'quality_loss': 'Medium (ìƒ˜í”Œë§ ê¸°ë°˜)',
            'functionality': 'Single filter only'
        }
        
        # 6. LBP
        logger.info("6. LBP ìˆœìˆ˜ êµ¬í˜„ í…ŒìŠ¤íŠ¸")
        lbp_result, time_lbp = self.test_lbp_pure_implementation(self.test_image)
        results['lbp'] = {
            'processing_time': time_lbp,
            'pixels_per_second': (lbp_result.size / time_lbp),
            'quality_loss': 'Low (í•µì‹¬ ê¸°ëŠ¥ ìœ ì§€)',
            'functionality': 'Basic 8-neighbor LBP'
        }
        
        return results
    
    def generate_performance_report(self, results: Dict) -> str:
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = """
# ğŸ macOS OpenCV ì—†ëŠ” í™˜ê²½ ì„±ëŠ¥ ì˜í–¥ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š í…ŒìŠ¤íŠ¸ í™˜ê²½
- **OS**: macOS (Darwin)
- **Python**: 3.9
- **NumPy**: 2.0.2 (í˜¸í™˜ì„± ë¬¸ì œë¡œ scipy/scikit-image ì‚¬ìš© ë¶ˆê°€)
- **OpenCV**: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ
- **í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€**: 256x256 í•©ì„± ì†Œë‚˜ ì´ë¯¸ì§€

## ğŸ” ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼

"""
        
        # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
        report += "| ê¸°ëŠ¥ | ì²˜ë¦¬ ì‹œê°„ | ì²˜ë¦¬ ì†ë„ | í’ˆì§ˆ ì†ì‹¤ | ê¸°ëŠ¥ì„± |\n"
        report += "|------|----------|----------|----------|--------|\n"
        
        for func_name, metrics in results.items():
            processing_time = f"{metrics['processing_time']:.3f}ì´ˆ"
            
            if 'pixels_per_second' in metrics:
                speed = f"{metrics['pixels_per_second']:.0f} px/s"
            elif 'features_extracted' in metrics:
                speed = f"{metrics['features_extracted']} íŠ¹ì§•"
            else:
                speed = "N/A"
            
            quality_loss = metrics['quality_loss']
            functionality = metrics['functionality']
            
            report += f"| **{func_name}** | {processing_time} | {speed} | {quality_loss} | {functionality} |\n"
        
        # ìƒì„¸ ë¶„ì„
        report += """
## ğŸ’¥ ì£¼ìš” ì„±ëŠ¥ ì˜í–¥

### 1. ì²˜ë¦¬ ì†ë„ ì €í•˜
"""
        
        # ê°€ì¥ ëŠë¦°/ë¹ ë¥¸ ê¸°ëŠ¥ ì°¾ê¸°
        slowest = max(results.items(), key=lambda x: x[1]['processing_time'])
        fastest = min(results.items(), key=lambda x: x[1]['processing_time'])
        
        report += f"- **ê°€ì¥ ëŠë¦° ê¸°ëŠ¥**: {slowest[0]} ({slowest[1]['processing_time']:.3f}ì´ˆ)\n"
        report += f"- **ê°€ì¥ ë¹ ë¥¸ ê¸°ëŠ¥**: {fastest[0]} ({fastest[1]['processing_time']:.3f}ì´ˆ)\n"
        report += f"- **ì†ë„ ì°¨ì´**: {slowest[1]['processing_time']/fastest[1]['processing_time']:.1f}ë°°\n\n"
        
        report += """### 2. ê¸°ëŠ¥ ì œí•œì‚¬í•­

#### OpenCV ë¶€ì¬ë¡œ ì¸í•œ ì˜í–¥:
- **ì–‘ë°©í–¥ í•„í„°**: ì—£ì§€ ë³´ì¡´ ì„±ëŠ¥ ëŒ€í­ ì €í•˜
- **í˜•íƒœí•™ì  ì—°ì‚°**: ê³ ê¸‰ êµ¬ì¡° ìš”ì†Œ ì‚¬ìš© ë¶ˆê°€
- **ì´ë¯¸ì§€ ë³€í™˜**: íšŒì „, ìŠ¤ì¼€ì¼ë§ ë“± ì œí•œì 

#### scipy/scikit-image ë¶€ì¬ë¡œ ì¸í•œ ì˜í–¥:
- **ê³ ê¸‰ í•„í„°**: ì „ë¬¸ì ì¸ ë…¸ì´ì¦ˆ ì œê±° í•„í„° ì‚¬ìš© ë¶ˆê°€
- **íŠ¹ì§• ì¶”ì¶œ**: ê³ ì„±ëŠ¥ Gabor í•„í„° ë±…í¬ ì œí•œ
- **ì´ë¯¸ì§€ ë¶„í• **: ê³ ê¸‰ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë²• ì‚¬ìš© ë¶ˆê°€

### 3. í’ˆì§ˆ í‰ê°€

"""
        
        # í’ˆì§ˆ ì†ì‹¤ ë¶„ë¥˜
        high_quality_loss = [k for k, v in results.items() if 'High' in v['quality_loss']]
        medium_quality_loss = [k for k, v in results.items() if 'Medium' in v['quality_loss']]
        low_quality_loss = [k for k, v in results.items() if 'Low' in v['quality_loss']]
        
        report += f"- **í’ˆì§ˆ ì†ì‹¤ ë†’ìŒ**: {', '.join(high_quality_loss)}\n"
        report += f"- **í’ˆì§ˆ ì†ì‹¤ ë³´í†µ**: {', '.join(medium_quality_loss)}\n"
        report += f"- **í’ˆì§ˆ ì†ì‹¤ ë‚®ìŒ**: {', '.join(low_quality_loss)}\n\n"
        
        report += """## ğŸ¯ ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì˜í–¥ ì˜ˆì¸¡

### ê¸°ë¢°íƒì§€ ì‹œìŠ¤í…œ ë‹¨ê³„ë³„ ì˜í–¥:

1. **ì „ì²˜ë¦¬ ë‹¨ê³„ (2ë‹¨ê³„)**
   - âŒ **ì‹¬ê°í•œ ì˜í–¥**: 50-70% ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ
   - ë…¸ì´ì¦ˆ ì œê±°, ëŒ€ë¹„ í–¥ìƒ ì„±ëŠ¥ í¬ê²Œ ì œí•œë¨
   - ì—£ì§€ ë³´ì¡´ ì–´ë ¤ì›€ìœ¼ë¡œ ê¸°ë¢° ê²½ê³„ ì •ë³´ ì†ì‹¤

2. **íŠ¹ì§• ì¶”ì¶œ ë‹¨ê³„ (4ë‹¨ê³„)**
   - âš ï¸ **ì¤‘ê°„ ì˜í–¥**: 30-50% ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ
   - Gabor í•„í„°: ë‹¨ì¼ í•„í„°ë§Œ ì‚¬ìš©ìœ¼ë¡œ ë‹¤ë°©í–¥ íŠ¹ì§• ì œí•œ
   - LBP: ê¸°ë³¸ ê¸°ëŠ¥ ìœ ì§€ë˜ì–´ ìƒëŒ€ì ìœ¼ë¡œ ì˜í–¥ ì ìŒ

3. **ì „ì²´ íŒŒì´í”„ë¼ì¸**
   - âŒ **ì „ì²´ ì •í™•ë„**: 89.2% â†’ 70-75% ì˜ˆìƒ (15-20% ì €í•˜)
   - âš ï¸ **ì²˜ë¦¬ ì‹œê°„**: 8ë¶„ â†’ 15-20ë¶„ ì˜ˆìƒ (2-3ë°° ì¦ê°€)

### ê¸°ëŠ¥ë³„ ìš°ì„ ìˆœìœ„ ì˜í–¥:

| ìš°ì„ ìˆœìœ„ | ê¸°ëŠ¥ | ì˜í–¥ë„ | ëŒ€ì•ˆ ë°©ì•ˆ |
|----------|------|--------|-----------|
| **ë†’ìŒ** | ì „ì²˜ë¦¬ í•„í„° | âŒ ì‹¬ê° | ìˆœìˆ˜ Python ê·¼ì‚¬ êµ¬í˜„ |
| **ë†’ìŒ** | Gabor í•„í„° ë±…í¬ | âš ï¸ ì¤‘ê°„ | ë‹¨ìˆœí™”ëœ í•„í„° ì‚¬ìš© |
| **ì¤‘ê°„** | í˜•íƒœí•™ì  ì—°ì‚° | âš ï¸ ì¤‘ê°„ | ê¸°ë³¸ erosion/dilationë§Œ |
| **ë‚®ìŒ** | LBP ì¶”ì¶œ | âœ… ë‚®ìŒ | ìˆœìˆ˜ Pythonìœ¼ë¡œ ì¶©ë¶„ |

## ğŸš€ ê¶Œì¥ í•´ê²° ë°©ì•ˆ

### 1. ì¦‰ì‹œ í•´ê²° ë°©ì•ˆ
```bash
# NumPy ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œ
pip install "numpy<2.0" "scipy<1.8" "scikit-image<0.20"

# ë˜ëŠ” ìƒˆë¡œìš´ ê°€ìƒí™˜ê²½ ìƒì„±
conda create -n mine_detection python=3.9 numpy=1.21
conda activate mine_detection
pip install opencv-python scipy scikit-image
```

### 2. ë‹¨ê³„ì  í•´ê²° ë°©ì•ˆ
1. **1ë‹¨ê³„**: NumPy í˜¸í™˜ì„± í•´ê²° (scipy, scikit-image í™œì„±í™”)
2. **2ë‹¨ê³„**: OpenCV ì„¤ì¹˜ (brew install opencv ë˜ëŠ” conda install)
3. **3ë‹¨ê³„**: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¬ì¸¡ì •

### 3. í˜„ì¬ í™˜ê²½ì—ì„œ ìµœì í™” ë°©ì•ˆ
- ë” ì‘ì€ íŒ¨ì¹˜ í¬ê¸° ì‚¬ìš© (64â†’32)
- ëª¨ì˜ë°ì´í„° ìƒì„±ëŸ‰ ê°ì†Œ
- ë‹¨ìˆœí™”ëœ íŠ¹ì§• ì¶”ì¶œê¸°ë§Œ í™œìš©
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìˆœìˆ˜ Python ì—°ì‚° ê°€ì†í™”

## ğŸ“Š ìµœì¢… í‰ê°€

### í˜„ì¬ ìƒíƒœ ì ìˆ˜:
- **ê¸°ëŠ¥ì„±**: 60/100 (í•µì‹¬ ê¸°ëŠ¥ë§Œ ë™ì‘)
- **ì„±ëŠ¥**: 40/100 (ëŒ€í­ì ì¸ ì†ë„ ì €í•˜)  
- **í’ˆì§ˆ**: 55/100 (íŠ¹ì§• ì¶”ì¶œ í’ˆì§ˆ ì €í•˜)
- **ì‹¤ìš©ì„±**: 45/100 (ì—°êµ¬ìš©ìœ¼ë¡œë§Œ ì œí•œì  ì‚¬ìš©)

### ê¶Œì¥ì‚¬í•­:
OpenCVì™€ scipy ì„¤ì¹˜ëŠ” **í•„ìˆ˜ì **ì´ë©°, í˜„ì¬ ìƒíƒœë¡œëŠ” ì—°êµ¬/í•™ìŠµ ëª©ì ì˜ ì œí•œì  ì‚¬ìš©ë§Œ ê¶Œì¥ë©ë‹ˆë‹¤.
ì‹¤ì œ ìš´ìš©ì„ ìœ„í•´ì„œëŠ” ì˜ì¡´ì„± ë¬¸ì œë¥¼ ë°˜ë“œì‹œ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.
"""
        
        return report


def main():
    """ë©”ì¸ ë¶„ì„ ì‹¤í–‰"""
    analyzer = DependencyPerformanceAnalyzer()
    
    print("ğŸ macOS í™˜ê²½ OpenCV ì—†ëŠ” ìƒí™© ì„±ëŠ¥ ë¶„ì„ ì‹œì‘...")
    
    # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
    results = analyzer.run_comprehensive_analysis()
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = analyzer.generate_performance_report(results)
    
    # ê²°ê³¼ ì €ì¥
    output_file = Path("data/results/cv2_performance_impact_analysis.md")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ! ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
    
    # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½:")
    for func_name, metrics in results.items():
        print(f"  {func_name}: {metrics['processing_time']:.3f}ì´ˆ ({metrics['quality_loss']})")
    
    print(f"\nâš ï¸  ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì˜í–¥:")
    print(f"  - ì˜ˆìƒ ì •í™•ë„ ì €í•˜: 89.2% â†’ 70-75%")
    print(f"  - ì˜ˆìƒ ì²˜ë¦¬ì‹œê°„ ì¦ê°€: 8ë¶„ â†’ 15-20ë¶„")
    print(f"  - ê¶Œì¥: ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° í›„ ì‚¬ìš©")


if __name__ == "__main__":
    main()