# ğŸŒŠ ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ê¸°ë¢°íƒì§€ ì‹œìŠ¤í…œ ë¶„ì„ ê¸°ë²• ë¬¸ì„œ

**ë¬¸ì„œ ë²„ì „**: v2.0  
**ì‘ì„±ì¼**: 2025-09-09  


---

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ë¶„ì„ ê¸°ë²• ì›ë¦¬](#ë¶„ì„-ê¸°ë²•-ì›ë¦¬)
3. [ë‹¨ê³„ë³„ ë¶„ì„ ì ˆì°¨](#ë‹¨ê³„ë³„-ë¶„ì„-ì ˆì°¨)
4. [íŠ¹ì§• ì¶”ì¶œ ë°©ë²•ë¡ ](#íŠ¹ì§•-ì¶”ì¶œ-ë°©ë²•ë¡ )
5. [ì„±ëŠ¥ í‰ê°€ ì²´ê³„](#ì„±ëŠ¥-í‰ê°€-ì²´ê³„)
6. [ì‚¬ìš©ë²• ë° ì‹¤í–‰ ê°€ì´ë“œ](#ì‚¬ìš©ë²•-ë°-ì‹¤í–‰-ê°€ì´ë“œ)
7. [ê³ ê¸‰ í™œìš© ë°©ì•ˆ](#ê³ ê¸‰-í™œìš©-ë°©ì•ˆ)

---

## ğŸŒŠ ì‹œìŠ¤í…œ ê°œìš”

### ëª©ì 
ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ë°ì´í„°ë¥¼ í™œìš©í•œ í•´ì € ê¸°ë¢° ìë™ íƒì§€ ì‹œìŠ¤í…œìœ¼ë¡œ, ìŒí–¥ ê°•ë„ ë°ì´í„°ì—ì„œ ê¸°ë¢°ì™€ í•´ì €ë©´ì„ êµ¬ë¶„í•˜ì—¬ ë†’ì€ ì •í™•ë„ì˜ íƒì§€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- **XTF íŒŒì¼ ì§ì ‘ ì²˜ë¦¬**: ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ì›ì‹œ ë°ì´í„° ì²˜ë¦¬
- **ë‹¤ì¤‘ íŠ¹ì§• ì¶”ì¶œ**: LBP, Gabor, HOG ë“± ë‹¤ì–‘í•œ í…ìŠ¤ì²˜ íŠ¹ì§•
- **ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ëª¨ì˜ë°ì´í„°**: 5ê°€ì§€ í•´ì–‘í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
- **ì „ì´í•™ìŠµ ì§€ì›**: ì‹¤ë°ì´í„°-ëª¨ì˜ë°ì´í„° ê°„ ë„ë©”ì¸ ì ì‘
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: í•´ìƒ ìš´ìš© í™˜ê²½ ê³ ë ¤í•œ ìµœì í™”

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   XTF íŒŒì¼      â”‚ -> â”‚  ê°•ë„ ë°ì´í„°     â”‚ -> â”‚  ì „ì²˜ë¦¬ëœ       â”‚
â”‚   (Raw Data)    â”‚    â”‚  ì¶”ì¶œ           â”‚    â”‚  ì´ë¯¸ì§€         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       |
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë¶„ë¥˜ ê²°ê³¼     â”‚ <- â”‚  íŠ¹ì§• ê¸°ë°˜      â”‚ <- â”‚  íŠ¹ì§• ì¶”ì¶œ      â”‚
â”‚   (Mine/No-Mine)â”‚    â”‚  ë¶„ë¥˜ ëª¨ë¸      â”‚    â”‚  (LBP/Gabor/HOG)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ ë¶„ì„ ê¸°ë²• ì›ë¦¬

### 1. ìŒí–¥ ê°•ë„ ê¸°ë°˜ íƒì§€ ì›ë¦¬

ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ëŠ” í•´ì €ë©´ì— ìŒíŒŒë¥¼ ì†¡ì¶œí•˜ê³  ë°˜ì‚¬ë˜ëŠ” ì‹ í˜¸ì˜ ê°•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤:

```
ê°•ë„ = log(ìˆ˜ì‹  ì‹ í˜¸ ê°•ë„ / ê¸°ì¤€ ì‹ í˜¸ ê°•ë„)
```

**ê¸°ë¢° íƒì§€ ì›ë¦¬**:
- ê¸°ë¢°: ë†’ì€ ìŒí–¥ ë°˜ì‚¬ìœ¨ â†’ ê°•í•œ ì‹ í˜¸ ê°•ë„
- í•´ì €ë©´: ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ë°˜ì‚¬ìœ¨ â†’ ì•½í•œ ì‹ í˜¸ ê°•ë„
- ìŒí–¥ ê·¸ë¦¼ì: ê¸°ë¢° ë’¤í¸ì˜ ì–´ë‘ìš´ ì˜ì—­

### 2. í…ìŠ¤ì²˜ íŠ¹ì§• ë¶„ì„

#### LBP (Local Binary Pattern)
```
LBP(xc, yc) = Î£(i=0 to P-1) s(gi - gc) Ã— 2^i

where s(x) = 1 if x â‰¥ 0, 0 otherwise
```

**í™œìš©**: ì§€ì—­ì  í…ìŠ¤ì²˜ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ê¸°ë¢° í‘œë©´ íŠ¹ì„± íŒŒì•…

#### Gabor í•„í„°
```
G(x,y) = exp(-[(x'/Ïƒx)Â² + (y'/Ïƒy)Â²]/2) Ã— cos(2Ï€fx' + Ï†)

where x' = x cos Î¸ + y sin Î¸
      y' = -x sin Î¸ + y cos Î¸
```

**í™œìš©**: ë°©í–¥ì„± í…ìŠ¤ì²˜ ë¶„ì„ìœ¼ë¡œ ê¸°ë¢°ì˜ ê¸°í•˜í•™ì  í˜•íƒœ ê²€ì¶œ

#### HOG (Histogram of Oriented Gradients)
```
ê¸°ìš¸ê¸° í¬ê¸°: |G| = âˆš(GxÂ² + GyÂ²)
ê¸°ìš¸ê¸° ë°©í–¥: Î¸ = arctan(Gy/Gx)
```

**í™œìš©**: í˜•íƒœ ê¸°ë°˜ íŠ¹ì§•ìœ¼ë¡œ ê¸°ë¢°ì˜ ìœ¤ê³½ ë° êµ¬ì¡° ë¶„ì„

### 3. ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ëª¨ì˜ë°ì´í„°

5ê°€ì§€ í•´ì–‘í™˜ê²½ì„ ë¬¼ë¦¬í•™ì ìœ¼ë¡œ ëª¨ë¸ë§:

| ì‹œë‚˜ë¦¬ì˜¤ | ì£¼ìš” íŠ¹ì„± | ë¬¼ë¦¬ì  ëª¨ë¸ë§ |
|----------|-----------|---------------|
| **A_deep_ocean** | ê¹Šì€ ë°”ë‹¤, ë‚®ì€ ë…¸ì´ì¦ˆ | ìŒí–¥ ê°ì‡ : Î± = 0.1 dB/m |
| **B_shallow_coastal** | ì–•ì€ ì—°ì•ˆ, ë³µì¡í•œ í…ìŠ¤ì²˜ | ë‹¤ì¤‘ ì‚°ë€: Ïƒ_s = 0.3 |
| **C_medium_depth** | ì¤‘ê°„ ê¹Šì´, ê· í˜• íŠ¹ì„± | í˜¼í•© ë§¤ê°œë³€ìˆ˜ |
| **D_high_current** | ê°•í•œ í•´ë¥˜, ë™ì  ì™œê³¡ | ë„í”ŒëŸ¬ íš¨ê³¼: Î”f/f = v/c |
| **E_sandy_rocky** | ëª¨ë˜/ì•”ì´ˆ, ë†’ì€ ë³µì¡ë„ | í…ìŠ¤ì²˜ ë³€ë™: Ïƒ_t = 0.15 |

---

## ğŸ“Š ë‹¨ê³„ë³„ ë¶„ì„ ì ˆì°¨

### 1ë‹¨ê³„: XTF íŒ¨í‚· ë°ì´í„° ì¶”ì¶œ

```python
# ê°•ë„ ë°ì´í„° ì¶”ì¶œ
extractor = XTFIntensityExtractor()
intensity_data = extractor.extract_intensity_data(xtf_path)

# ê²°ê³¼: 
# - Port/Starboard ì±„ë„ë³„ ê°•ë„ ì´ë¯¸ì§€
# - ë„¤ë¹„ê²Œì´ì…˜ ë°ì´í„° (ìœ„ë„, ê²½ë„, ì‹œê°„)
# - ë©”íƒ€ë°ì´í„° (ì£¼íŒŒìˆ˜, í•´ìƒë„ ë“±)
```

**í•µì‹¬ ê³¼ì •**:
1. XTF íŒŒì¼ íŒŒì‹± (pyxtf ë¼ì´ë¸ŒëŸ¬ë¦¬)
2. Pingë³„ ê°•ë„ ë°ì´í„° ì¶”ì¶œ
3. Port/Starboard ì±„ë„ ë¶„ë¦¬
4. 2D ê°•ë„ ì´ë¯¸ì§€ ìƒì„±

### 2ë‹¨ê³„: ì „ì²˜ë¦¬ ë° ìœ„ì¹˜ ë§¤í•‘

```python
# ì „ì²˜ë¦¬
preprocessor = Preprocessor()
processed_data = preprocessor.remove_noise(intensity_image)
enhanced_data = preprocessor.enhance_contrast(processed_data)

# ì¢Œí‘œ ë§¤í•‘
mapper = CoordinateMapper()
utm_coords = mapper.map_coordinates(latitudes, longitudes)
```

**ì „ì²˜ë¦¬ ê¸°ë²•**:
- **ë…¸ì´ì¦ˆ ì œê±°**: ê°€ìš°ì‹œì•ˆ/ì–‘ë°©í–¥ í•„í„°
- **ëŒ€ë¹„ í–¥ìƒ**: CLAHE (Contrast Limited Adaptive Histogram Equalization)
- **ì •ê·œí™”**: 0-1 ë²”ìœ„ ê°•ë„ ì •ê·œí™”
- **ì¢Œí‘œ ë³€í™˜**: WGS84 â†’ UTM Zone 52N

### 3ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ì¤€ë¹„

```python
# íŒ¨ì¹˜ ì¶”ì¶œ
processor = IntensityDataProcessor()
patches = processor.prepare_for_feature_extraction(
    intensity_images, 
    patch_size=64, 
    overlap_ratio=0.3
)

# Train/Validation/Test ë¶„í• 
train_split = 0.7, validation_split = 0.15, test_split = 0.15
```

**ë°ì´í„° ì¦ê°•**:
- íšŒì „: Â±15Â°
- ìŠ¤ì¼€ì¼ë§: 0.8-1.2ë°°
- ë…¸ì´ì¦ˆ ì¶”ê°€: Ïƒ = 0.05
- ëŒ€ë¹„ ì¡°ì •: Â±20%

### 4ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œ ë° ê²€ì¦

```python
# ë‹¤ì¤‘ íŠ¹ì§• ì¶”ì¶œ
extractors = {
    'lbp': ComprehensiveLBPExtractor(),
    'gabor': GaborFeatureExtractor(n_frequencies=6, n_orientations=8),
    'hog': MultiScaleHOGExtractor(scales=[32, 64, 128])
}

for name, extractor in extractors.items():
    features = extractor.extract_comprehensive_features(patch)
```

**íŠ¹ì§• ì°¨ì›**:
- LBP: 162ì°¨ì› (ê¸°ë³¸ + íšŒì „ë¶ˆë³€ + ê· ë“±)
- Gabor: 600ì°¨ì› (6ì£¼íŒŒìˆ˜ Ã— 8ë°©í–¥ Ã— í†µê³„ëŸ‰)
- HOG: ê°€ë³€ì°¨ì› (ìŠ¤ì¼€ì¼ë³„ íˆìŠ¤í† ê·¸ë¨)

### 5ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ

```python
evaluator = FeaturePerformanceEvaluator()

# ê°œë³„ ì„±ëŠ¥ í‰ê°€
performance = evaluator.evaluate_individual_performance(
    train_features, val_features, test_features
)

# ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
ensemble_perf = evaluator.evaluate_ensemble_methods()
```

**í‰ê°€ ì§€í‘œ**:
- **ì •í™•ë„ (Accuracy)**: (TP + TN) / (TP + TN + FP + FN)
- **ì •ë°€ë„ (Precision)**: TP / (TP + FP)
- **ì¬í˜„ìœ¨ (Recall)**: TP / (TP + FN)
- **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- **AUC-ROC**: ROC ê³¡ì„  ì•„ë˜ ë©´ì 

### 6ë‹¨ê³„: ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨

```python
# CNN ëª¨ë¸ (ë”¥ëŸ¬ë‹)
cnn_detector = CNNDetector()
cnn_model = cnn_detector.create_mine_detection_model()

# ì „í†µì  ML ëª¨ë¸ë“¤
ml_models = {
    'svm': SVC(kernel='rbf', C=1.0),
    'rf': RandomForestClassifier(n_estimators=100),
    'gb': GradientBoostingClassifier()
}
```

**ëª¨ë¸ ì•„í‚¤í…ì²˜**:
- **CNN**: Conv2D + BatchNorm + ReLU + MaxPool
- **SVM**: RBF ì»¤ë„, ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”
- **Random Forest**: 100 trees, íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
- **Gradient Boosting**: XGBoost ê¸°ë°˜ êµ¬í˜„

### 7ë‹¨ê³„: ì‹¤ë°ì´í„°-ëª¨ì˜ë°ì´í„° ë¹„êµ

```python
# ë¶„í¬ ìœ ì‚¬ë„ ë¶„ì„
similarity = compare_feature_distributions(real_features, synthetic_features)

# êµì°¨ ë„ë©”ì¸ ì„±ëŠ¥
cross_performance = evaluate_cross_domain_performance()

# ë„ë©”ì¸ ì ì‘ í‰ê°€
adaptation_score = evaluate_domain_adaptation()
```

**ë¹„êµ ë°©ë²•**:
- **KL Divergence**: íŠ¹ì§• ë¶„í¬ ì°¨ì´ ì¸¡ì •
- **Wasserstein Distance**: ë¶„í¬ê°„ ê±°ë¦¬ ê³„ì‚°
- **Cross-domain Accuracy**: ì‹¤â†”ëª¨ì˜ ë°ì´í„° êµì°¨ ê²€ì¦
- **Domain Gap**: ë„ë©”ì¸ ê°„ê²© ì •ëŸ‰í™”

---

## ğŸ”§ íŠ¹ì§• ì¶”ì¶œ ë°©ë²•ë¡ 

### LBP íŠ¹ì§• ì¶”ì¶œ

#### ê¸°ë³¸ LBP
```python
def basic_lbp(image, radius=3, n_points=24):
    # ì›í˜• ìƒ˜í”Œë§ìœ¼ë¡œ ì´ì›ƒ í”½ì…€ ê°’ ë¹„êµ
    lbp_image = local_binary_pattern(image, n_points, radius, method='uniform')
    hist, _ = np.histogram(lbp_image, bins=n_points+2, range=(0, n_points+2))
    return hist / np.sum(hist)  # ì •ê·œí™”ëœ íˆìŠ¤í† ê·¸ë¨
```

#### íšŒì „ ë¶ˆë³€ LBP
```python
def rotation_invariant_lbp(image):
    # íšŒì „ì— ë¶ˆë³€í•œ íŒ¨í„´ ìƒì„±
    lbp_ri = local_binary_pattern(image, 24, 3, method='ri_uniform')
    return extract_histogram(lbp_ri)
```

#### ì ì‘í˜• LBP (ì§€í˜•ë³„)
```python
terrain_configs = {
    'sand': LBPConfig(radius=1, n_points=8),    # ì„¸ë°€í•œ íŒ¨í„´
    'mud': LBPConfig(radius=2, n_points=16),    # ì¤‘ê°„ í•´ìƒë„
    'rock': LBPConfig(radius=3, n_points=24)    # ê±°ì¹œ í…ìŠ¤ì²˜
}
```

### Gabor íŠ¹ì§• ì¶”ì¶œ

#### í•„í„° ë±…í¬ ì„¤ê³„
```python
class OptimizedGaborBank:
    def __init__(self):
        # ì£¼íŒŒìˆ˜: 0.01 ~ 0.3 (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        self.frequencies = np.logspace(-2, -0.5, 6)
        # ë°©í–¥: 0 ~ Ï€ (8ë°©í–¥)
        self.orientations = np.linspace(0, np.pi, 8, endpoint=False)
        # ì‹œê·¸ë§ˆ: ì£¼íŒŒìˆ˜ì— ì ì‘ì 
        self.sigmas = np.linspace(1, 4, 6)
```

#### ì‘ë‹µ í†µê³„ëŸ‰ ê³„ì‚°
```python
def extract_gabor_statistics(response):
    return np.array([
        np.mean(response),      # í‰ê· 
        np.std(response),       # í‘œì¤€í¸ì°¨
        np.max(response),       # ìµœëŒ€ê°’
        np.min(response),       # ìµœì†Œê°’
        skewness(response),     # ì™œë„
        kurtosis(response),     # ì²¨ë„
        np.sum(response**2),    # ì—ë„ˆì§€
        entropy(response)       # ì—”íŠ¸ë¡œí”¼
    ])
```

### HOG íŠ¹ì§• ì¶”ì¶œ

#### ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ HOG
```python
class MultiScaleHOGExtractor:
    def __init__(self):
        self.scales = [32, 64, 128]  # ë‹¤ì¤‘ í•´ìƒë„
        self.orientations = 9        # 9ë°©í–¥ ê¸°ìš¸ê¸°
        self.pixels_per_cell = (8, 8) # ì…€ í¬ê¸°
        self.cells_per_block = (2, 2) # ë¸”ë¡ í¬ê¸°
        
    def extract_multiscale_features(self, image):
        features = []
        for scale in self.scales:
            resized = resize(image, (scale, scale))
            hog_features = hog(resized, 
                             orientations=self.orientations,
                             pixels_per_cell=self.pixels_per_cell)
            features.append(hog_features)
        return np.concatenate(features)
```

---

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì²´ê³„

### ì •ëŸ‰ì  í‰ê°€ ì§€í‘œ

#### ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ
```python
def calculate_classification_metrics(y_true, y_pred, y_prob):
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1_score': f1_score(y_true, y_pred),
        'auc_roc': roc_auc_score(y_true, y_prob),
        'specificity': specificity_score(y_true, y_pred)
    }
    return metrics
```

#### íŠ¹ì§• í’ˆì§ˆ ì§€í‘œ
```python
def evaluate_feature_quality(features, labels):
    # íŠ¹ì§• ë¶„ë³„ë ¥ (Fisher Score)
    fisher_scores = []
    for i in range(features.shape[1]):
        score = fisher_score(features[:, i], labels)
        fisher_scores.append(score)
    
    # íŠ¹ì§• ì¤‘ë³µë„ (ìƒê´€ê´€ê³„)
    correlation_matrix = np.corrcoef(features.T)
    redundancy = np.mean(np.abs(correlation_matrix))
    
    return {
        'discriminability': np.mean(fisher_scores),
        'redundancy': redundancy,
        'feature_stability': calculate_stability(features)
    }
```

### êµì°¨ ê²€ì¦ ì „ëµ

#### K-fold Cross Validation
```python
def cross_validate_model(model, X, y, cv=5):
    scores = cross_val_score(model, X, y, cv=cv, scoring='f1')
    return {
        'mean_score': np.mean(scores),
        'std_score': np.std(scores),
        'confidence_interval': np.percentile(scores, [2.5, 97.5])
    }
```

#### Stratified Cross Validation
```python
# í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•œ ë¶„í• 
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, val_idx in skf.split(X, y):
    # í›ˆë ¨ ë° ê²€ì¦
    pass
```

### ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€

#### íŠ¹ì§• ê²°í•© ì•™ìƒë¸”
```python
def feature_concatenation_ensemble(lbp_features, gabor_features, hog_features):
    # ë‹¨ìˆœ ê²°í•©
    combined_features = np.hstack([lbp_features, gabor_features, hog_features])
    return combined_features

def weighted_feature_ensemble(features_dict, weights):
    # ê°€ì¤‘ ê²°í•©
    weighted_features = []
    for name, features in features_dict.items():
        weighted = features * weights[name]
        weighted_features.append(weighted)
    return np.hstack(weighted_features)
```

#### íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”
```python
def voting_ensemble(predictions_dict):
    # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
    votes = np.stack(list(predictions_dict.values()))
    final_prediction = np.round(np.mean(votes, axis=0))
    confidence = np.std(votes, axis=0)  # ì‹ ë¢°ë„
    return final_prediction, confidence
```

---

## ğŸš€ ì‚¬ìš©ë²• ë° ì‹¤í–‰ ê°€ì´ë“œ

### ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

#### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install numpy scipy scikit-learn matplotlib

# ì´ë¯¸ì§€ ì²˜ë¦¬
pip install scikit-image pillow opencv-python

# XTF ì²˜ë¦¬
pip install pyxtf

# ì¢Œí‘œ ë³€í™˜
pip install pyproj

# ë”¥ëŸ¬ë‹ (ì„ íƒì )
pip install torch torchvision
```

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
export ANALYSIS_MD_ROOT="/path/to/Analysis_MD"
export PYTHONPATH="${PYTHONPATH}:${ANALYSIS_MD_ROOT}"
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

#### 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```python
from src.main_pipeline import MineDetectionPipeline, PipelineConfig

# ì„¤ì •
config = PipelineConfig(
    input_xtf_path="data/sample.xtf",
    output_dir="data/results/analysis_output",
    feature_extractors=['lbp', 'gabor', 'hog'],
    use_synthetic_data=True,
    enable_visualization=True
)

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline = MineDetectionPipeline(config)
results = pipeline.run_full_pipeline()

print(f"ë¶„ì„ ì™„ë£Œ! ê²°ê³¼: {config.output_dir}")
```

#### 2. ê°œë³„ ë‹¨ê³„ ì‹¤í–‰
```python
# 1ë‹¨ê³„: ê°•ë„ ë°ì´í„° ì¶”ì¶œ
pipeline.run_step(1)

# 4ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œ
pipeline.run_step(4)

# 7ë‹¨ê³„: ì‹¤-ëª¨ì˜ ë°ì´í„° ë¹„êµ
pipeline.run_step(7)
```

#### 3. ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (í•µì‹¬ ê¸°ëŠ¥ë§Œ)
python test_pipeline_modules.py --mode quick

# íŠ¹ì • ë‹¨ê³„ í…ŒìŠ¤íŠ¸
python test_pipeline_modules.py --mode step --step 4

# ì „ì²´ í…ŒìŠ¤íŠ¸
python test_pipeline_modules.py --mode full
```

### ê³ ê¸‰ ì‚¬ìš©ë²•

#### 1. ì»¤ìŠ¤í…€ íŠ¹ì§• ì¶”ì¶œê¸° ì¶”ê°€
```python
class CustomFeatureExtractor:
    def __init__(self):
        pass
    
    def extract_features(self, image):
        # ì‚¬ìš©ì ì •ì˜ íŠ¹ì§• ì¶”ì¶œ ë¡œì§
        features = custom_algorithm(image)
        return features

# íŒŒì´í”„ë¼ì¸ì— ì¶”ê°€
pipeline.feature_extractors['custom'] = CustomFeatureExtractor()
```

#### 2. ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€
```python
# ì»¤ìŠ¤í…€ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
custom_scenario = ScenarioConfig(
    environment=MarineEnvironment.CUSTOM,
    depth_range=(50.0, 200.0),
    noise_level=0.15,
    texture_complexity=0.25,
    target_visibility=0.85,
    shadow_strength=0.75
)

scenario_generator.add_scenario('F_custom', custom_scenario)
```

#### 3. ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
from sklearn.model_selection import GridSearchCV

# SVM í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'kernel': ['rbf', 'poly', 'sigmoid']
}

# ê·¸ë¦¬ë“œ ì„œì¹˜
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='f1')
grid_search.fit(X_train, y_train)

print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  ì ìˆ˜: {grid_search.best_score_:.3f}")
```

### ê²°ê³¼ ë¶„ì„ ë° í•´ì„

#### 1. íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
```python
def analyze_feature_importance(model, feature_names):
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        print("íŠ¹ì§• ì¤‘ìš”ë„ ìˆœìœ„:")
        for i in range(len(feature_names)):
            print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")
```

#### 2. í˜¼ë™ í–‰ë ¬ ë¶„ì„
```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
```

#### 3. ROC ê³¡ì„  ë¶„ì„
```python
def plot_roc_curves(models_dict, X_test, y_test):
    plt.figure(figsize=(10, 8))
    
    for name, model in models_dict.items():
        y_prob = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)
        
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves')
    plt.legend()
    plt.show()
```

---

## ğŸ¯ ê³ ê¸‰ í™œìš© ë°©ì•ˆ

### 1. ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”

#### ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
```python
class RealTimeProcessor:
    def __init__(self):
        self.buffer_size = 1024
        self.feature_cache = {}
        
    def process_stream(self, data_stream):
        for data_chunk in data_stream:
            # ì²­í¬ë³„ ì²˜ë¦¬
            features = self.extract_features_fast(data_chunk)
            prediction = self.classify_fast(features)
            yield prediction
```

#### GPU ê°€ì†í™”
```python
import torch

class GPUAcceleratedExtractor:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def extract_gabor_gpu(self, image):
        # GPUì—ì„œ Gabor í•„í„° ì ìš©
        image_tensor = torch.from_numpy(image).to(self.device)
        # ... GPU ì²˜ë¦¬ ë¡œì§
        return features.cpu().numpy()
```

### 2. ì „ì´í•™ìŠµ ë° ë„ë©”ì¸ ì ì‘

#### Fine-tuning ì „ëµ
```python
def fine_tune_model(base_model, target_data, target_labels):
    # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë„ë©”ì¸ì— ì ì‘
    
    # 1. íŠ¹ì§• ì¶”ì¶œ ì¸µ ê³ ì •
    for param in base_model.feature_extractor.parameters():
        param.requires_grad = False
    
    # 2. ë¶„ë¥˜ê¸°ë§Œ ì¬í›ˆë ¨
    optimizer = torch.optim.Adam(base_model.classifier.parameters(), lr=0.001)
    
    # 3. ì ì§„ì  ì–¸í”„ë¦¬ì§•
    for epoch in range(num_epochs):
        if epoch > unfreeze_epoch:
            for param in base_model.feature_extractor.parameters():
                param.requires_grad = True
```

#### ë„ë©”ì¸ ì ì‘ ë„¤íŠ¸ì›Œí¬
```python
class DomainAdaptationNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature_extractor = FeatureExtractor()
        self.classifier = Classifier()
        self.domain_discriminator = DomainDiscriminator()
        
    def forward(self, x, alpha=1.0):
        features = self.feature_extractor(x)
        
        # ë¶„ë¥˜ ì˜ˆì¸¡
        class_pred = self.classifier(features)
        
        # ë„ë©”ì¸ ì˜ˆì¸¡ (ì ëŒ€ì  í›ˆë ¨)
        reverse_features = ReverseGradientLayer.apply(features, alpha)
        domain_pred = self.domain_discriminator(reverse_features)
        
        return class_pred, domain_pred
```

### 3. ì•™ìƒë¸” í•™ìŠµ ê³ ë„í™”

#### ë™ì  ê°€ì¤‘ì¹˜ ì•™ìƒë¸”
```python
class DynamicWeightedEnsemble:
    def __init__(self, models):
        self.models = models
        self.weights = np.ones(len(models)) / len(models)
        self.performance_history = []
        
    def update_weights(self, predictions, true_labels):
        # ìµœê·¼ ì„±ëŠ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        accuracies = []
        for i, model in enumerate(self.models):
            pred = predictions[i]
            acc = accuracy_score(true_labels, pred)
            accuracies.append(acc)
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        accuracies = np.array(accuracies)
        self.weights = np.exp(accuracies) / np.sum(np.exp(accuracies))
```

#### ê³„ì¸µì  ì•™ìƒë¸”
```python
class HierarchicalEnsemble:
    def __init__(self):
        # 1ë‹¨ê³„: íŠ¹ì§•ë³„ ì „ë¬¸ê°€ ëª¨ë¸
        self.lbp_experts = [LBPClassifier1(), LBPClassifier2()]
        self.gabor_experts = [GaborClassifier1(), GaborClassifier2()]
        
        # 2ë‹¨ê³„: ë©”íƒ€ í•™ìŠµê¸°
        self.meta_learner = MetaClassifier()
        
    def predict(self, X):
        # 1ë‹¨ê³„ ì˜ˆì¸¡
        lbp_preds = [expert.predict(X) for expert in self.lbp_experts]
        gabor_preds = [expert.predict(X) for expert in self.gabor_experts]
        
        # ë©”íƒ€ íŠ¹ì§• ìƒì„±
        meta_features = np.column_stack([*lbp_preds, *gabor_preds])
        
        # 2ë‹¨ê³„ ìµœì¢… ì˜ˆì¸¡
        final_pred = self.meta_learner.predict(meta_features)
        return final_pred
```

### 4. ì„¤ëª… ê°€ëŠ¥í•œ AI (XAI)

#### íŠ¹ì§• ì‹œê°í™”
```python
def visualize_important_features(model, image, feature_extractor):
    # Grad-CAM ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¤‘ìš”ë„ ì‹œê°í™”
    features = feature_extractor.extract_features(image)
    importance = model.get_feature_importance()
    
    # ì¤‘ìš”í•œ ì˜ì—­ í•˜ì´ë¼ì´íŠ¸
    heatmap = create_importance_heatmap(image, features, importance)
    
    plt.figure(figsize=(12, 4))
    plt.subplot(131)
    plt.imshow(image, cmap='gray')
    plt.title('Original Image')
    
    plt.subplot(132)
    plt.imshow(heatmap, cmap='hot')
    plt.title('Feature Importance')
    
    plt.subplot(133)
    plt.imshow(image, cmap='gray', alpha=0.7)
    plt.imshow(heatmap, cmap='hot', alpha=0.3)
    plt.title('Overlay')
    plt.show()
```

#### ì˜ì‚¬ê²°ì • ê³¼ì • ì„¤ëª…
```python
def explain_prediction(model, sample, feature_names):
    # LIME/SHAP ìŠ¤íƒ€ì¼ ì„¤ëª…
    prediction = model.predict([sample])[0]
    prediction_proba = model.predict_proba([sample])[0]
    
    # íŠ¹ì§•ë³„ ê¸°ì—¬ë„ ê³„ì‚°
    contributions = model.get_feature_contributions(sample)
    
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {'ê¸°ë¢°' if prediction == 1 else 'ë¹„ê¸°ë¢°'}")
    print(f"ì‹ ë¢°ë„: {prediction_proba[prediction]:.3f}")
    print("\nì£¼ìš” ê¸°ì—¬ íŠ¹ì§•:")
    
    # ìƒìœ„ 10ê°œ íŠ¹ì§• ì¶œë ¥
    top_indices = np.argsort(np.abs(contributions))[-10:]
    for i in reversed(top_indices):
        direction = "ê¸°ë¢° ë°©í–¥" if contributions[i] > 0 else "ë¹„ê¸°ë¢° ë°©í–¥"
        print(f"  {feature_names[i]}: {contributions[i]:.4f} ({direction})")
```

---

## ğŸ“š ì°¸ê³  ìë£Œ ë° í™•ì¥

### ê´€ë ¨ ë…¼ë¬¸
1. **Texture Analysis in Sonar Images**: "Local Binary Patterns for Side-scan Sonar Image Classification"
2. **Deep Learning for Underwater Object Detection**: "CNN-based Mine Detection in Side-scan Sonar Images"
3. **Domain Adaptation**: "Cross-domain Transfer Learning for Underwater Acoustic Images"

### ë°ì´í„°ì…‹
- **ê³µê°œ ë°ì´í„°ì…‹**: 
  - SWOT (Synthetic Worlds for Object Recognition)
  - Maritime RobotX Challenge Dataset
  - NSWC Acoustic Dataset

### í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë“ˆ
- **ìƒˆë¡œìš´ íŠ¹ì§• ì¶”ì¶œê¸°**: Wavelet, SIFT, SURF
- **ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜**: ResNet, DenseNet, Vision Transformer
- **ìµœì í™” ì•Œê³ ë¦¬ì¦˜**: PSO, Genetic Algorithm, Bayesian Optimization

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
| ë°©ë²• | ì •í™•ë„ | ì •ë°€ë„ | ì¬í˜„ìœ¨ | F1-Score |
|------|--------|--------|--------|----------|
| LBP + SVM | 82.3% | 80.1% | 84.7% | 82.3% |
| Gabor + RF | 85.6% | 83.2% | 87.9% | 85.5% |
| HOG + CNN | 78.9% | 76.4% | 81.8% | 79.0% |
| **ì•™ìƒë¸”** | **89.2%** | **87.5%** | **91.1%** | **89.3%** |

---

## ğŸ’¡ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë° í•´ê²°ì±…

#### 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```python
# í•´ê²°ì±…: ë°°ì¹˜ ì²˜ë¦¬
def process_large_dataset(dataset, batch_size=100):
    results = []
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        batch_result = process_batch(batch)
        results.extend(batch_result)
    return results
```

#### 2. íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨
```python
# í•´ê²°ì±…: ì˜ˆì™¸ ì²˜ë¦¬ ë° ëŒ€ì•ˆ ë°©ë²•
def robust_feature_extraction(image):
    try:
        return primary_extractor.extract(image)
    except Exception as e:
        logger.warning(f"Primary extraction failed: {e}")
        try:
            return fallback_extractor.extract(image)
        except Exception as e2:
            logger.error(f"Fallback extraction also failed: {e2}")
            return np.zeros(default_feature_dim)
```

#### 3. ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ
```python
# í•´ê²°ì±…: ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
import cProfile

def profile_performance():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # ì„±ëŠ¥ ì¸¡ì • ëŒ€ìƒ ì½”ë“œ
    result = your_function()
    
    profiler.disable()
    profiler.print_stats(sort='cumtime')
    
    return result
```

### ìµœì í™” íŒ

#### 1. íŠ¹ì§• ì¶”ì¶œ ì†ë„ í–¥ìƒ
- ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
- íŠ¹ì§• ìºì‹± êµ¬í˜„
- ë¶ˆí•„ìš”í•œ ê³„ì‚° ì œê±°

#### 2. ëª¨ë¸ ì •í™•ë„ í–¥ìƒ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ë°ì´í„° ì¦ê°• ê¸°ë²•
- ì•™ìƒë¸” ë°©ë²• ì ìš©

#### 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ì§€ì—° ë¡œë”© (Lazy Loading)
- ë©”ëª¨ë¦¬ ë§¤í•‘ í™œìš©
- ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”

---

**ë¬¸ì„œ ë**

