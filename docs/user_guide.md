# ğŸš€ ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ê¸°ë¢°íƒì§€ ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ì´ë“œ

**ë²„ì „**: v2.0  
**ì—…ë°ì´íŠ¸**: 2025-09-09  
**ë‚œì´ë„**: ì´ˆê¸‰ ~ ê³ ê¸‰  

---

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •
```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /path/to/Analysis_MD

# Python ê²½ë¡œ ì„¤ì •
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install numpy scipy scikit-learn scikit-image
pip install pyxtf pyproj matplotlib
```

### 2ë‹¨ê³„: ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ë¹ ë¥¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python test_pipeline_modules.py --mode quick

# ê²°ê³¼ í™•ì¸
# âœ… imports: PASS
# âœ… intensity_extraction: PASS  
# âœ… feature_extraction: PASS
```

### 3ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```python
from src.main_pipeline import MineDetectionPipeline, PipelineConfig

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
config = PipelineConfig(
    output_dir="data/results/my_analysis",
    use_synthetic_data=True,
    feature_extractors=['lbp', 'gabor']
)

pipeline = MineDetectionPipeline(config)
results = pipeline.run_full_pipeline()

print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Analysis_MD/
â”œâ”€â”€ ğŸ“‚ src/                      # í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ ğŸ“‚ data_processing/      # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ ğŸ“‚ feature_extraction/   # íŠ¹ì§• ì¶”ì¶œ
â”‚   â”œâ”€â”€ ğŸ“‚ models/              # ë¶„ë¥˜ ëª¨ë¸
â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/          # ì„±ëŠ¥ í‰ê°€
â”‚   â”œâ”€â”€ ğŸ“‚ data_simulation/     # ëª¨ì˜ë°ì´í„°
â”‚   â”œâ”€â”€ ğŸ“‚ data_augmentation/   # ë°ì´í„° ì¦ê°•
â”‚   â”œâ”€â”€ ğŸ“‚ training/            # ëª¨ë¸ í›ˆë ¨
â”‚   â”œâ”€â”€ ğŸ“‚ utils/               # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ ğŸ“‚ interactive/         # ëŒ€í™”í˜• ë„êµ¬
â”‚   â””â”€â”€ ğŸ“„ main_pipeline.py     # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ ğŸ“‚ data/                     # ë°ì´í„° ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ ğŸ“‚ processed/           # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ ğŸ“‚ results/             # ë¶„ì„ ê²°ê³¼
â”œâ”€â”€ ğŸ“‚ datasets/                 # ì‹¤ì œ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ ğŸ“‚ Pohang_Eardo_1_*/    # í¬í•­ ì´ì–´ë„ ë°ì´í„°
â”‚   â””â”€â”€ ğŸ“‚ */original/          # ì›ë³¸ ë°ì´í„°
â”‚       â””â”€â”€ ğŸ“‚ */simulation/    # ëª¨ì˜ ë°ì´í„°
â”œâ”€â”€ ğŸ“‚ [ìƒ˜í”Œ]ë°ì´í„°/             # ìƒ˜í”Œ ë°ì´í„°
â”œâ”€â”€ ğŸ“‚ docs/                     # ë¬¸ì„œ
â”œâ”€â”€ ğŸ“‚ config/                   # ì„¤ì • íŒŒì¼
â”œâ”€â”€ ğŸ“‚ notebooks/               # Jupyter ë…¸íŠ¸ë¶
â”œâ”€â”€ ğŸ“‚ outputs/                  # ì¶œë ¥ ê²°ê³¼
â”‚   â”œâ”€â”€ ğŸ“‚ figures/             # ê·¸ë˜í”„/ì°¨íŠ¸
â”‚   â””â”€â”€ ğŸ“‚ models/              # ì €ì¥ëœ ëª¨ë¸
â”œâ”€â”€ ğŸ“‚ logs/                     # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ ğŸ“‚ tests/                    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ ğŸ“„ main.py                   # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ ğŸ“„ test_*.py                 # ê°ì¢… í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ ğŸ“„ requirements.txt          # íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
```

---

## ğŸ› ï¸ ì‚¬ìš©ë²•ë³„ ê°€ì´ë“œ

### ğŸ”° ì´ˆê¸‰ ì‚¬ìš©ì: GUI ìŠ¤íƒ€ì¼ ì‹¤í–‰

#### ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
# ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰ (ëŒ€í™”í˜• ë©”ë‰´ í¬í•¨)
python main.py

# ë˜ëŠ” ê°„ë‹¨í•œ ë¶„ì„ ì‹¤í–‰
python quick_analysis.py

# ìƒ˜í”Œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰
python sample_analysis.py
```

#### ì„¤ì • íŒŒì¼ ì‚¬ìš©
```json
// config/user_settings.json
{
    "input_xtf_path": "datasets/Pohang_Eardo_1_*/simulation/xtf_input/*.xtf",
    "output_dir": "data/results/analysis_output",
    "feature_extractors": ["lbp", "gabor"],
    "use_synthetic_data": true,
    "enable_visualization": true,
    "patch_size": 64
}
```

```bash
# ì„¤ì • íŒŒì¼ë¡œ ì‹¤í–‰ (ì„¤ì • íŒŒì¼ì´ ìˆì„ ê²½ìš°)
python main.py --config config/user_settings.json

# ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python main.py
```

### ğŸ”§ ì¤‘ê¸‰ ì‚¬ìš©ì: ëª¨ë“ˆë³„ ì‹¤í–‰

#### 1. XTF ë°ì´í„° ì²˜ë¦¬ë§Œ ì‹¤í–‰
```python
from src.data_processing.xtf_intensity_extractor import XTFIntensityExtractor

# XTF ê°•ë„ ë°ì´í„° ì¶”ì¶œ
extractor = XTFIntensityExtractor()
intensity_data = extractor.extract_intensity_data(
    xtf_path="datasets/Pohang_Eardo_1_Klein3900_900_050_20241011171100_001_04/simulation/xtf_input/sample.xtf",
    output_dir="data/results/intensity"
)

print(f"ì¶”ì¶œëœ Ping ìˆ˜: {intensity_data['metadata'].ping_count}")
print(f"ì´ë¯¸ì§€ í¬ê¸°: {intensity_data['intensity_images']['combined'].shape}")
```

#### 2. íŠ¹ì§• ì¶”ì¶œë§Œ ì‹¤í–‰
```python
from src.feature_extraction.lbp_extractor import ComprehensiveLBPExtractor
from src.feature_extraction.gabor_extractor import GaborFeatureExtractor
import numpy as np

# ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
image = np.random.rand(128, 128)  # ë”ë¯¸ ì´ë¯¸ì§€ (ì‹¤ì œë¡œëŠ” XTFì—ì„œ ì¶”ì¶œ)

# LBP íŠ¹ì§• ì¶”ì¶œ
lbp_extractor = ComprehensiveLBPExtractor()
lbp_features = lbp_extractor.extract_comprehensive_features(image)
print(f"LBP íŠ¹ì§• ì°¨ì›: {len(lbp_features)}")

# Gabor íŠ¹ì§• ì¶”ì¶œ
gabor_extractor = GaborFeatureExtractor()
gabor_features = gabor_extractor.extract_comprehensive_features(image)
print(f"Gabor íŠ¹ì§• ì°¨ì›: {len(gabor_features)}")
```

#### 3. ëª¨ì˜ë°ì´í„°ë§Œ ìƒì„±
```python
from src.data_simulation.scenario_generator import ScenarioBasedGenerator

# ì‹œë‚˜ë¦¬ì˜¤ë³„ ëª¨ì˜ë°ì´í„° ìƒì„±
generator = ScenarioBasedGenerator()

scenarios = ['A_deep_ocean', 'B_shallow_coastal', 'C_medium_depth']
for scenario in scenarios:
    synthetic_data = generator.generate_scenario_data(scenario, num_samples=50)
    print(f"{scenario}: {len(synthetic_data)} ìƒ˜í”Œ ìƒì„±")
```

### âš¡ ê³ ê¸‰ ì‚¬ìš©ì: ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### 1. ìƒˆë¡œìš´ íŠ¹ì§• ì¶”ì¶œê¸° êµ¬í˜„
```python
from src.feature_extraction.base_extractor import BaseFeatureExtractor
import numpy as np

class CustomTextureExtractor(BaseFeatureExtractor):
    def __init__(self, param1=1.0, param2=2.0):
        super().__init__()
        self.param1 = param1
        self.param2 = param2
    
    def extract_features(self, image):
        """ì‚¬ìš©ì ì •ì˜ í…ìŠ¤ì²˜ íŠ¹ì§• ì¶”ì¶œ"""
        # ì—¬ê¸°ì— ì»¤ìŠ¤í…€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        features = self.my_custom_algorithm(image)
        return features
    
    def my_custom_algorithm(self, image):
        # ì˜ˆì‹œ: í†µê³„ì  íŠ¹ì§•
        features = [
            np.mean(image),
            np.std(image),
            np.skew(image.flatten()),
            np.kurtosis(image.flatten())
        ]
        return np.array(features)

# ì‚¬ìš©ë²•
custom_extractor = CustomTextureExtractor(param1=1.5)
custom_features = custom_extractor.extract_features(image)
```

#### 2. íŒŒì´í”„ë¼ì¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
from src.main_pipeline import MineDetectionPipeline, PipelineConfig

class CustomPipeline(MineDetectionPipeline):
    def __init__(self, config):
        super().__init__(config)
        # ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸ ì¶”ê°€
        self.custom_extractor = CustomTextureExtractor()
    
    def custom_step_extra_processing(self):
        """ì¶”ê°€ ì²˜ë¦¬ ë‹¨ê³„"""
        print("ì»¤ìŠ¤í…€ ì²˜ë¦¬ ë‹¨ê³„ ì‹¤í–‰ ì¤‘...")
        # ì»¤ìŠ¤í…€ ë¡œì§
        pass
    
    def run_custom_pipeline(self):
        """ì»¤ìŠ¤í„°ë§ˆì´ì§•ëœ íŒŒì´í”„ë¼ì¸"""
        # ê¸°ë³¸ ë‹¨ê³„ë“¤
        self.step1_extract_intensity_data()
        self.step2_preprocess_and_map()
        
        # ì»¤ìŠ¤í…€ ë‹¨ê³„
        self.custom_step_extra_processing()
        
        # ë‚˜ë¨¸ì§€ ë‹¨ê³„ë“¤
        self.step4_extract_and_validate_features()
        self.step5_evaluate_feature_performance()

# ì‚¬ìš©ë²•
custom_config = PipelineConfig(
    output_dir="data/results/custom_analysis",
    feature_extractors=['lbp', 'gabor', 'custom']
)

custom_pipeline = CustomPipeline(custom_config)
custom_pipeline.feature_extractors['custom'] = custom_pipeline.custom_extractor
results = custom_pipeline.run_custom_pipeline()
```

---

## ğŸ“Š ì‹¤í–‰ ëª¨ë“œë³„ ê°€ì´ë“œ

### 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ
```bash
# ì „ì²´ 7ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰ (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)
python main.py

# ë˜ëŠ” ì§ì ‘ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ì‹¤í–‰
python -c "
from src.main_pipeline import *
config = PipelineConfig()
pipeline = MineDetectionPipeline(config)
pipeline.run_full_pipeline()
"
```

**ì‹¤í–‰ ì‹œê°„**: ì•½ 10-30ë¶„ (ë°ì´í„° í¬ê¸°ì— ë”°ë¼)  
**ê²°ê³¼ë¬¼**: 
- ê°•ë„ ì´ë¯¸ì§€
- ì¶”ì¶œëœ íŠ¹ì§•
- í›ˆë ¨ëœ ëª¨ë¸
- ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸
- ì‹œê°í™” ì°¨íŠ¸

### 2. ê°œë³„ ë‹¨ê³„ ëª¨ë“œ
```bash
# 1ë‹¨ê³„ë§Œ ì‹¤í–‰: XTF ë°ì´í„° ì¶”ì¶œ
python test_pipeline_modules.py --mode step --step 1

# 4ë‹¨ê³„ë§Œ ì‹¤í–‰: íŠ¹ì§• ì¶”ì¶œ
python test_pipeline_modules.py --mode step --step 4

# 7ë‹¨ê³„ë§Œ ì‹¤í–‰: ì‹¤-ëª¨ì˜ ë°ì´í„° ë¹„êµ
python test_pipeline_modules.py --mode step --step 7
```

### 3. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
```bash
# í•µì‹¬ ê¸°ëŠ¥ë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
python test_pipeline_modules.py --mode quick

# ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: 2-5ë¶„
# ê²°ê³¼: ê° ëª¨ë“ˆì˜ ê¸°ë³¸ ë™ì‘ ê²€ì¦
```

### 4. ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
```python
# ì—¬ëŸ¬ XTF íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
import os
from pathlib import Path

def batch_process_xtf_files(input_dir, output_dir):
    xtf_files = list(Path(input_dir).glob("*.xtf"))
    
    for i, xtf_file in enumerate(xtf_files):
        print(f"ì²˜ë¦¬ ì¤‘ ({i+1}/{len(xtf_files)}): {xtf_file.name}")
        
        config = PipelineConfig(
            input_xtf_path=str(xtf_file),
            output_dir=f"{output_dir}/{xtf_file.stem}",
            enable_visualization=False  # ì†ë„ í–¥ìƒ
        )
        
        pipeline = MineDetectionPipeline(config)
        results = pipeline.run_full_pipeline()
        
        print(f"âœ… ì™„ë£Œ: {xtf_file.name}")

# ì‚¬ìš©ë²•
batch_process_xtf_files("datasets/", "data/results/batch_analysis/")
```

---

## ğŸ¨ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### ê²°ê³¼ êµ¬ì¡° ì´í•´
```
data/results/pipeline_output/
â”œâ”€â”€ 01_intensity_data/          # ì¶”ì¶œëœ ê°•ë„ ë°ì´í„°
â”‚   â”œâ”€â”€ combined_intensity.npy
â”‚   â”œâ”€â”€ port_intensity.npy
â”‚   â””â”€â”€ navigation_data.npz
â”œâ”€â”€ 02_preprocessed/            # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ 03_features/               # ì¶”ì¶œëœ íŠ¹ì§•
â”‚   â”œâ”€â”€ lbp_features.npy
â”‚   â””â”€â”€ gabor_features.npy
â”œâ”€â”€ 04_models/                 # í›ˆë ¨ëœ ëª¨ë¸
â”œâ”€â”€ 05_evaluation/             # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼
â”‚   â”œâ”€â”€ performance_metrics.json
â”‚   â””â”€â”€ confusion_matrix.png
â”œâ”€â”€ 06_comparison/             # ì‹¤-ëª¨ì˜ ë°ì´í„° ë¹„êµ
â””â”€â”€ 07_visualization/          # ì‹œê°í™” ê²°ê³¼
    â”œâ”€â”€ intensity_images/
    â”œâ”€â”€ feature_importance/
    â””â”€â”€ performance_charts/
```

### ê²°ê³¼ í•´ì„ ê°€ì´ë“œ

#### 1. ì„±ëŠ¥ ì§€í‘œ í•´ì„
```python
import json

# ì„±ëŠ¥ ê²°ê³¼ ë¡œë“œ
with open('data/results/pipeline_output/05_evaluation/performance_metrics.json', 'r') as f:
    performance = json.load(f)

# í•´ì„
for extractor, metrics in performance.items():
    print(f"\nğŸ“Š {extractor.upper()} ì„±ëŠ¥:")
    print(f"  ì •í™•ë„: {metrics.get('accuracy', 0):.1%}")
    print(f"  ì •ë°€ë„: {metrics.get('precision', 0):.1%}")
    print(f"  ì¬í˜„ìœ¨: {metrics.get('recall', 0):.1%}")
    print(f"  F1ì ìˆ˜: {metrics.get('f1_score', 0):.1%}")
    
    # í•´ì„ ê°€ì´ë“œ
    accuracy = metrics.get('accuracy', 0)
    if accuracy > 0.9:
        print("  âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥")
    elif accuracy > 0.8:
        print("  âœ… ì–‘í˜¸í•œ ì„±ëŠ¥")
    elif accuracy > 0.7:
        print("  âš ï¸  ê°œì„  í•„ìš”")
    else:
        print("  âŒ ì„±ëŠ¥ ë¶ˆëŸ‰ - ì¬ê²€í†  í•„ìš”")
```

#### 2. íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
```python
import numpy as np
import matplotlib.pyplot as plt

def analyze_feature_importance(feature_file):
    features = np.load(feature_file)
    
    # íŠ¹ì§• í†µê³„
    print(f"íŠ¹ì§• ì°¨ì›: {features.shape[1]}")
    print(f"ìƒ˜í”Œ ìˆ˜: {features.shape[0]}")
    print(f"í‰ê·  íŠ¹ì§•ê°’: {np.mean(features):.4f}")
    print(f"íŠ¹ì§• ë²”ìœ„: [{np.min(features):.4f}, {np.max(features):.4f}]")
    
    # íŠ¹ì§• ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    
    plt.subplot(131)
    plt.hist(features.flatten(), bins=50, alpha=0.7)
    plt.title('Feature Value Distribution')
    plt.xlabel('Feature Value')
    plt.ylabel('Frequency')
    
    plt.subplot(132)
    feature_means = np.mean(features, axis=0)
    plt.plot(feature_means)
    plt.title('Feature Means')
    plt.xlabel('Feature Index')
    plt.ylabel('Mean Value')
    
    plt.subplot(133)
    feature_stds = np.std(features, axis=0)
    plt.plot(feature_stds)
    plt.title('Feature Standard Deviations')
    plt.xlabel('Feature Index')
    plt.ylabel('Std Value')
    
    plt.tight_layout()
    plt.show()

# ì‚¬ìš©ë²•
analyze_feature_importance('data/results/pipeline_output/03_features/lbp_features.npy')
```

#### 3. ì‹¤-ëª¨ì˜ ë°ì´í„° ë¹„êµ ê²°ê³¼ í•´ì„
```python
def interpret_comparison_results(comparison_file):
    with open(comparison_file, 'r') as f:
        comparison = json.load(f)
    
    print("ğŸ” ì‹¤-ëª¨ì˜ ë°ì´í„° ë¹„êµ ë¶„ì„:")
    
    # íŠ¹ì§• ë¶„í¬ ìœ ì‚¬ë„
    feature_dist = comparison.get('feature_distributions', {})
    kl_div = feature_dist.get('kl_divergence', 0)
    similarity = feature_dist.get('distribution_similarity', 0)
    
    print(f"\nğŸ“ˆ ë¶„í¬ ìœ ì‚¬ë„:")
    print(f"  KL Divergence: {kl_div:.4f} (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)")
    print(f"  ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.3f} (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)")
    
    if similarity > 0.8:
        print("  âœ… ë§¤ìš° ìœ ì‚¬í•œ ë¶„í¬ - ëª¨ì˜ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜")
    elif similarity > 0.6:
        print("  âœ… ì–‘í˜¸í•œ ìœ ì‚¬ë„ - ëª¨ì˜ë°ì´í„° í™œìš© ê°€ëŠ¥")
    else:
        print("  âš ï¸  ìœ ì‚¬ë„ ë¶€ì¡± - ëª¨ì˜ë°ì´í„° ê°œì„  í•„ìš”")
    
    # êµì°¨ ë„ë©”ì¸ ì„±ëŠ¥
    cross_perf = comparison.get('cross_domain_performance', {})
    print(f"\nğŸ”„ êµì°¨ ë„ë©”ì¸ ì„±ëŠ¥:")
    for test_type, result in cross_perf.items():
        accuracy = result.get('accuracy', 0)
        print(f"  {test_type}: {accuracy:.1%}")

# ì‚¬ìš©ë²•  
interpret_comparison_results('data/results/pipeline_output/06_comparison/comparison_results.json')
```

---

## âš ï¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

#### 1. ëª¨ë“ˆ import ì˜¤ë¥˜
```
ModuleNotFoundError: No module named 'src.xxx'
```
**í•´ê²°ì±…**:
```bash
# Python ê²½ë¡œ ì„¤ì •
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì— ì¶”ê°€
import sys
sys.path.append('/path/to/Analysis_MD')
```

#### 2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```
MemoryError: Unable to allocate xxx GB
```
**í•´ê²°ì±…**:
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
config = PipelineConfig(
    patch_size=32,  # 64ì—ì„œ 32ë¡œ ì¤„ì„
    use_synthetic_data=False,  # ëª¨ì˜ë°ì´í„° ë¹„í™œì„±í™”
)

# ë˜ëŠ” ì²˜ë¦¬ ë‹¨ìœ„ ì¤„ì´ê¸°
def process_in_chunks(data, chunk_size=100):
    for i in range(0, len(data), chunk_size):
        chunk = data[i:i+chunk_size]
        yield process_chunk(chunk)
```

#### 3. OpenCV ì„¤ì¹˜ ì˜¤ë¥˜
```
ImportError: No module named 'cv2'
```
**í•´ê²°ì±…**:
```bash
# OpenCV ì„¤ì¹˜ ì‹œë„ (ì—¬ëŸ¬ ì˜µì…˜)
pip install opencv-python
# ë˜ëŠ”
pip install opencv-python-headless
# ë˜ëŠ” conda
conda install opencv

# ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ ì‹œìŠ¤í…œì€ ëŒ€ì•ˆ êµ¬í˜„ ìë™ ì‚¬ìš©
```

#### 4. XTF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜
```
FileNotFoundError: XTF file not found
```
**í•´ê²°ì±…**:
```python
# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
config = PipelineConfig(
    input_xtf_path=None,  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
    use_synthetic_data=True
)

# ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ í™•ì¸
import os
from pathlib import Path
dataset_dirs = [d for d in os.listdir('datasets/') if d.startswith('Pohang_')]
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {dataset_dirs}")

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
sample_files = list(Path('[ìƒ˜í”Œ]ë°ì´í„°/').glob('*'))
print(f"ìƒ˜í”Œ íŒŒì¼ë“¤: {[f.name for f in sample_files]}")
```

### ì„±ëŠ¥ ìµœì í™” íŒ

#### 1. ì²˜ë¦¬ ì†ë„ í–¥ìƒ
```python
# ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
from multiprocessing import Pool
import numpy as np

def extract_features_parallel(images):
    with Pool(processes=4) as pool:
        results = pool.map(extract_single_feature, images)
    return results

# íŠ¹ì§• ìºì‹±
import pickle
import os

def cached_feature_extraction(image, cache_dir='cache/features/'):
    os.makedirs(cache_dir, exist_ok=True)
    
    # ì´ë¯¸ì§€ í•´ì‹œë¡œ ìºì‹œ í‚¤ ìƒì„±
    image_hash = str(hash(image.tobytes()))
    cache_file = os.path.join(cache_dir, f'features_{image_hash}.pkl')
    
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    
    # íŠ¹ì§• ì¶”ì¶œ
    features = extractor.extract_features(image)
    
    # ìºì‹œ ì €ì¥
    with open(cache_file, 'wb') as f:
        pickle.dump(features, f)
    
    return features
```

#### 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
```python
# ì œë„ˆë ˆì´í„° ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
def data_generator(file_paths):
    for file_path in file_paths:
        data = load_data(file_path)
        yield process_data(data)
        del data  # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    memory_usage = process.memory_info().rss / 1024 / 1024  # MB
    print(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f} MB")

# ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ
monitor_memory()
```

---

## ğŸš€ ê³ ê¸‰ í™œìš© ì‚¬ë¡€

### 1. ì‹¤ì‹œê°„ ê¸°ë¢° íƒì§€ ì‹œìŠ¤í…œ
```python
import time
from threading import Thread
import queue

class RealTimeDetector:
    def __init__(self):
        self.data_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_running = False
        
    def start_real_time_processing(self):
        self.is_running = True
        
        # ë°ì´í„° ìˆ˜ì‹  ìŠ¤ë ˆë“œ
        data_thread = Thread(target=self.data_receiver)
        data_thread.start()
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        processing_thread = Thread(target=self.data_processor)
        processing_thread.start()
        
        print("ğŸš€ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œì‘")
        
    def data_receiver(self):
        # ì†Œë‚˜ ë°ì´í„° ìˆ˜ì‹  ì‹œë®¬ë ˆì´ì…˜
        while self.is_running:
            # ì‹¤ì œë¡œëŠ” ì†Œë‚˜ ì¥ë¹„ì—ì„œ ë°ì´í„° ìˆ˜ì‹ 
            simulated_data = generate_test_data()
            self.data_queue.put(simulated_data)
            time.sleep(0.1)  # 100ms ê°„ê²©
    
    def data_processor(self):
        while self.is_running:
            if not self.data_queue.empty():
                data = self.data_queue.get()
                
                # ë¹ ë¥¸ íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ë¥˜
                features = self.fast_feature_extraction(data)
                prediction = self.quick_classify(features)
                
                self.result_queue.put({
                    'timestamp': time.time(),
                    'prediction': prediction,
                    'confidence': 0.85  # ì„ì‹œê°’
                })
                
                if prediction == 'mine':
                    print("âš ï¸  ê¸°ë¢° íƒì§€!")

# ì‚¬ìš©ë²•
detector = RealTimeDetector()
detector.start_real_time_processing()
```

### 2. ìë™ ë³´ê³ ì„œ ìƒì„±
```python
from datetime import datetime
import matplotlib.pyplot as plt
import json

class AutoReportGenerator:
    def __init__(self, analysis_results_dir):
        self.results_dir = analysis_results_dir
        
    def generate_comprehensive_report(self):
        report_html = f"""
        <html>
        <head>
            <title>ê¸°ë¢°íƒì§€ ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œ</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ text-align: center; color: #2c3e50; }}
                .section {{ margin: 20px 0; padding: 20px; border: 1px solid #ddd; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; 
                         background: #f8f9fa; border-radius: 5px; }}
                .good {{ color: #27ae60; }}
                .warning {{ color: #f39c12; }}
                .error {{ color: #e74c3c; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸŒŠ ê¸°ë¢°íƒì§€ ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œ</h1>
                <p>ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            {self.generate_performance_section()}
            {self.generate_feature_analysis_section()}
            {self.generate_recommendations_section()}
        </body>
        </html>
        """
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = f"{self.results_dir}/auto_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_html)
        
        print(f"ğŸ“„ ìë™ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
        return report_path
    
    def generate_performance_section(self):
        # ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ ë° HTML ìƒì„±
        perf_data = self.load_performance_data()
        
        html = "<div class='section'><h2>ğŸ“Š ì„±ëŠ¥ ë¶„ì„</h2>"
        for extractor, metrics in perf_data.items():
            accuracy = metrics.get('accuracy', 0)
            status_class = 'good' if accuracy > 0.8 else 'warning' if accuracy > 0.7 else 'error'
            
            html += f"""
            <div class='metric {status_class}'>
                <strong>{extractor.upper()}</strong><br>
                ì •í™•ë„: {accuracy:.1%}
            </div>
            """
        html += "</div>"
        return html

# ì‚¬ìš©ë²•
report_gen = AutoReportGenerator('data/results/pipeline_output/')
report_gen.generate_comprehensive_report()
```

### 3. ì›¹ ê¸°ë°˜ ë¶„ì„ ì¸í„°í˜ì´ìŠ¤
```python
# Flask ì›¹ ì¸í„°í˜ì´ìŠ¤ ì˜ˆì‹œ
from flask import Flask, request, jsonify, render_template
import os

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('analysis_interface.html')

@app.route('/upload_xtf', methods=['POST'])
def upload_xtf():
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    if file.filename.endswith('.xtf'):
        # XTF íŒŒì¼ ì €ì¥ ë° ì²˜ë¦¬
        filepath = os.path.join('uploads', file.filename)
        file.save(filepath)
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹œì‘
        analysis_id = start_background_analysis(filepath)
        
        return jsonify({
            'status': 'success',
            'analysis_id': analysis_id,
            'message': 'ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
    
    return jsonify({'error': 'Invalid file type'}), 400

@app.route('/analysis_status/<analysis_id>')
def get_analysis_status(analysis_id):
    status = check_analysis_status(analysis_id)
    return jsonify(status)

# ì‹¤í–‰
# python web_interface.py
# http://localhost:5000 ì ‘ì†
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œì‘ ì „ í™•ì¸ì‚¬í•­
- [ ] Python 3.9+ ì„¤ì¹˜ë¨
- [ ] í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨ (`pip install -r requirements.txt`)
- [ ] PYTHONPATH ì„¤ì •ë¨
- [ ] ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬ (`data/` í´ë”)
- [ ] ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ (ìµœì†Œ 2GB)

### ì‹¤í–‰ ì¤‘ í™•ì¸ì‚¬í•­
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (8GB ì´í•˜ ê¶Œì¥)
- [ ] ë¡œê·¸ íŒŒì¼ ìƒì„± í™•ì¸
- [ ] ì¤‘ê°„ ê²°ê³¼ íŒŒì¼ ìƒì„± í™•ì¸
- [ ] ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ

### ì™„ë£Œ í›„ í™•ì¸ì‚¬í•­
- [ ] ëª¨ë“  ê²°ê³¼ íŒŒì¼ ìƒì„±ë¨
- [ ] ì„±ëŠ¥ ì§€í‘œê°€ í•©ë¦¬ì  ë²”ìœ„ (50% ì´ìƒ)
- [ ] ì‹œê°í™” ì°¨íŠ¸ ìƒì„±ë¨
- [ ] ìµœì¢… ë³´ê³ ì„œ ì™„ì„±ë¨

---

## ğŸ“ ì§€ì› ë° ë¬¸ì˜

### ë¬¸ì œ í•´ê²° ìˆœì„œ
1. **ë¡œê·¸ íŒŒì¼ í™•ì¸**: `data/results/pipeline_output/logs/`
2. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: `python test_pipeline_modules.py --mode quick`
3. **ì„¤ì • í™•ì¸**: íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë°”ë¥¸ì§€ ì ê²€
4. **ë¬¸ì„œ ì°¸ì¡°**: `docs/analysis_methodology.md`
5. **GitHub ì´ìŠˆ**: [Issues í˜ì´ì§€](https://github.com/your-repo/issues)

### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

**Q: XTF íŒŒì¼ ì—†ì´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‚˜ìš”?**  
A: ë„¤, `input_xtf_path=None`ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Q: ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.**  
A: `patch_size`ë¥¼ 32ë¡œ ì¤„ì´ê³  `use_synthetic_data=False`ë¡œ ì„¤ì •í•´ë³´ì„¸ìš”.

**Q: OpenCV ì„¤ì¹˜ê°€ ì•ˆë©ë‹ˆë‹¤.**  
A: ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ëŒ€ì•ˆ êµ¬í˜„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ì…”ë„ ë©ë‹ˆë‹¤.

**Q: ê²°ê³¼ í•´ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.**  
A: `docs/analysis_methodology.md`ì˜ "ì„±ëŠ¥ í‰ê°€ ì²´ê³„" ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

**Q: ì»¤ìŠ¤í…€ íŠ¹ì§•ì„ ì¶”ê°€í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.**  
A: ê³ ê¸‰ ì‚¬ìš©ì ê°€ì´ë“œì˜ "ìƒˆë¡œìš´ íŠ¹ì§• ì¶”ì¶œê¸° êµ¬í˜„" ë¶€ë¶„ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

**ë²„ì „ íˆìŠ¤í† ë¦¬**:
- v2.0 (2025-09-09): ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë° ë¬¸ì„œí™”
- v1.5 (2025-09-08): ëª¨ì˜ë°ì´í„° ë° ë¹„êµ ë¶„ì„ ì¶”ê°€
- v1.0 (2025-09-07): ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ë¥˜ êµ¬í˜„

**ë¼ì´ì„ ìŠ¤**: Research & Educational Use Only  
**ê°œë°œíŒ€**: ECMiner ê¸°ë¢°íƒì§€ì‹œìŠ¤í…œ ê°œë°œíŒ€