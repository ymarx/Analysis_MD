# ğŸš€ GPU/í´ë¼ìš°ë“œ ë°°í¬ ì „ëµ ê³„íšì„œ

**ì‘ì„±ì¼**: 2025-09-09  
**ëª©í‘œ**: ë¡œì»¬ ê¸°ëŠ¥ ì†ìƒ ì—†ì´ GPU ë° í´ë¼ìš°ë“œ í™˜ê²½ ì§€ì›  

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

### 1. **ê¸°ì¡´ ì½”ë“œ ë³´ì¡´** 
- í˜„ì¬ CPU ê¸°ë°˜ ë¡œì»¬ ì‹¤í–‰ ê¸°ëŠ¥ 100% ìœ ì§€
- í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥ (ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ ê·¸ëŒ€ë¡œ ì‘ë™)
- ì˜µì…˜ ë°©ì‹ìœ¼ë¡œ GPU/í´ë¼ìš°ë“œ ê¸°ëŠ¥ ì¶”ê°€

### 2. **ì ì§„ì  í™•ì¥**
- ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì  í™˜ê²½ ì„ íƒ
- í™˜ê²½ë³„ ì„¤ì • ìë™ ì¡°ì •
- í´ë°± ë©”ì»¤ë‹ˆì¦˜ (GPU ì—†ìœ¼ë©´ CPUë¡œ ìë™ ì „í™˜)

### 3. **í™•ì¥ì„±**
- ë¡œì»¬ â†’ GPU â†’ ë¶„ì‚° â†’ í´ë¼ìš°ë“œ ë‹¨ê³„ì  ì§€ì›
- ëª¨ë“ˆë³„ ë…ë¦½ì  GPU í™œìš©
- í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë³„ ìµœì í™”

---

## ğŸ“Š êµ¬í˜„ ë‹¨ê³„

### Phase 1: ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ (1-2ì¼)

#### 1.1 ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
```python
# src/utils/device_manager.py
class DeviceManager:
    def __init__(self):
        self.device = self.detect_optimal_device()
        self.capabilities = self.analyze_capabilities()
    
    def detect_optimal_device(self):
        # CUDA â†’ MPS â†’ CPU ìˆœì„œë¡œ ê°ì§€
        if torch.cuda.is_available():
            return torch.device('cuda')
        elif torch.backends.mps.is_available():  # Apple Silicon
            return torch.device('mps') 
        else:
            return torch.device('cpu')
```

#### 1.2 ì„¤ì • ìë™ ì¡°ì •
```python
# config/device_config.py
DEVICE_CONFIGS = {
    'cpu': {
        'batch_size': 8,
        'num_workers': 2,
        'memory_limit': '4GB'
    },
    'cuda': {
        'batch_size': 32,
        'num_workers': 4,
        'memory_limit': '8GB'
    },
    'mps': {
        'batch_size': 16,
        'num_workers': 3,
        'memory_limit': '6GB'
    }
}
```

### Phase 2: GPU ìµœì í™” ëª¨ë“ˆ (2-3ì¼)

#### 2.1 CNN ëª¨ë¸ GPU ê°€ì†
```python
# src/models/gpu_detector.py
class GPUOptimizedDetector(SidescanTargetDetector):
    def __init__(self, config=None, device='auto'):
        super().__init__(config)
        self.device_manager = DeviceManager()
        self.device = device if device != 'auto' else self.device_manager.device
        self.to(self.device)
        
        # GPUë³„ ìµœì í™”
        if self.device.type == 'cuda':
            self.enable_cuda_optimizations()
        elif self.device.type == 'mps':
            self.enable_mps_optimizations()
```

#### 2.2 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
# src/training/gpu_pipeline.py
class GPUTrainingPipeline:
    def __init__(self, device_manager):
        self.device = device_manager.device
        self.batch_size = device_manager.get_optimal_batch_size()
        self.dataloader = self.create_optimized_dataloader()
    
    def create_optimized_dataloader(self):
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            num_workers=self.get_num_workers(),
            pin_memory=self.device.type == 'cuda'
        )
```

### Phase 3: í´ë¼ìš°ë“œ ë°°í¬ ì§€ì› (3-4ì¼)

#### 3.1 Docker ì»¨í…Œì´ë„ˆí™”
```dockerfile
# docker/Dockerfile.gpu
FROM nvidia/cuda:11.8-runtime-ubuntu20.04

# ê¸°ë³¸ í™˜ê²½ ì„¤ì •
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.9 python3-pip \
    libopencv-dev \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements_gpu.txt .
RUN pip install -r requirements_gpu.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY . /app
WORKDIR /app

# ì‹¤í–‰ ê¶Œí•œ ì„¤ì •
RUN chmod +x scripts/run_gpu.sh

CMD ["python", "main.py", "--device", "auto"]
```

#### 3.2 í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë³„ ë°°í¬
```python
# cloud/aws_deployment.py
class AWSGPUDeployment:
    def __init__(self):
        self.instance_types = {
            'p3.2xlarge': {'gpu': 'V100', 'memory': '61GB'},
            'g4dn.xlarge': {'gpu': 'T4', 'memory': '16GB'},
            'p4d.24xlarge': {'gpu': 'A100', 'memory': '1152GB'}
        }
    
    def deploy_model(self, model_path, instance_type='g4dn.xlarge'):
        # EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ëª¨ë¸ ë°°í¬
        pass
```

### Phase 4: ë¶„ì‚° ì²˜ë¦¬ ì§€ì› (ì„ íƒì‚¬í•­)

#### 4.1 ë©€í‹° GPU ì§€ì›
```python
# src/training/distributed.py
class DistributedTrainer:
    def __init__(self, model, device_manager):
        if device_manager.gpu_count > 1:
            self.model = nn.DataParallel(model)
        else:
            self.model = model
```

---

## ğŸ”§ êµ¬í˜„ ìƒì„¸

### 1. ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ

**íŒŒì¼**: `src/utils/device_manager.py`

**í•µì‹¬ ê¸°ëŠ¥**:
- ìë™ í•˜ë“œì›¨ì–´ ê°ì§€ (CUDA, MPS, CPU)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

**ê¸°ì¡´ ì½”ë“œì™€ì˜ í†µí•©**:
```python
# ê¸°ì¡´ ì½”ë“œ (ë³€ê²½ ì—†ìŒ)
pipeline = MineDetectionPipeline(config)

# ìƒˆë¡œìš´ GPU ì§€ì› (ì˜µì…˜)
pipeline = MineDetectionPipeline(config, device='auto')
```

### 2. ì„¤ì • ê³„ì¸µí™”

**íŒŒì¼ êµ¬ì¡°**:
```
config/
â”œâ”€â”€ settings.py           # ê¸°ë³¸ ì„¤ì • (ë³€ê²½ ì—†ìŒ)
â”œâ”€â”€ device_configs.py     # ë””ë°”ì´ìŠ¤ë³„ ì„¤ì • (ì‹ ê·œ)
â”œâ”€â”€ cloud_configs.py      # í´ë¼ìš°ë“œ ì„¤ì • (ì‹ ê·œ)
â””â”€â”€ environments/         # í™˜ê²½ë³„ ì„¤ì • (ì‹ ê·œ)
    â”œâ”€â”€ local.yaml
    â”œâ”€â”€ gpu.yaml
    â”œâ”€â”€ aws.yaml
    â””â”€â”€ azure.yaml
```

### 3. ë°±ì›Œë“œ í˜¸í™˜ì„± ë³´ì¥

**ì›ì¹™**:
- ëª¨ë“  ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ì‘ë™
- ìƒˆ ê¸°ëŠ¥ì€ opt-in ë°©ì‹
- í™˜ê²½ ê°ì§€ í›„ ìë™ ìµœì í™”

**ì˜ˆì‹œ**:
```bash
# ê¸°ì¡´ ë°©ì‹ (ë³€ê²½ ì—†ìŒ)
python main.py

# ìƒˆë¡œìš´ ì˜µì…˜ (GPU ê°•ì œ ì‚¬ìš©)
python main.py --device cuda

# ìë™ ìµœì í™” (ê¶Œì¥)
python main.py --device auto
```

---

## ğŸŒ í´ë¼ìš°ë“œ í”Œë«í¼ë³„ ì§€ì›

### 1. Runpod (ì£¼ìš” ì§€ì› í”Œë«í¼)

**ì§€ì› GPU**:
- RTX 3090 (24GB VRAM) - $0.44/ì‹œê°„
- RTX 4090 (24GB VRAM) - $0.69/ì‹œê°„
- A40 (48GB VRAM) - $1.29/ì‹œê°„
- A100-40GB - $1.89/ì‹œê°„
- A100-80GB - $2.99/ì‹œê°„

**ì£¼ìš” ê¸°ëŠ¥**:
- Spot ì¸ìŠ¤í„´ìŠ¤ë¡œ ìµœëŒ€ 70% ë¹„ìš© ì ˆì•½
- SSH, Jupyter Lab, TensorBoard í†µí•© í™˜ê²½
- ìë™ ë°ì´í„° ë™ê¸°í™” ë° í”„ë¡œì íŠ¸ ì—…ë¡œë“œ
- GPU ì‚¬ìš©ë¥  ë° ë¹„ìš© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

**ë°°í¬ ë°©ë²•**:
```bash
# Runpod ìë™ ë°°í¬
export RUNPOD_API_KEY="your-api-key"
python scripts/deploy_runpod.py --gpu-type "RTX 4090" --bid-price 0.5
```

### 2. ë¡œì»¬ GPU í™˜ê²½

**ì§€ì› GPU**:
- NVIDIA CUDA (GTX 1060 ì´ìƒ)
- Apple Silicon M1/M2 (MPS)
- Intel GPU (ì‹¤í—˜ì  ì§€ì›)

**ìµœì í™” ê¸°ëŠ¥**:
- ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬
- Mixed Precision í›ˆë ¨
- ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •

### 3. ê¸°íƒ€ í´ë¼ìš°ë“œ í”Œë«í¼

**í™•ì¥ ê°€ëŠ¥í•œ í”Œë«í¼**:
- Paperspace Gradient
- Lambda Labs
- Vast.ai
- Google Colab Pro
- ê°œì¸ GPU ì„œë²„

---

## ğŸ“¦ íŒ¨í‚¤ì§• ì „ëµ

### 1. ì˜ì¡´ì„± ê´€ë¦¬

**CPU ì „ìš©** (í˜„ì¬):
```txt
# requirements.txt (ë³€ê²½ ì—†ìŒ)
numpy>=1.21,<2.0
opencv-python>=4.5,<5.0
# ... ê¸°ì¡´ íŒ¨í‚¤ì§€ë“¤
```

**GPU ì§€ì›**:
```txt
# requirements_gpu.txt (ì‹ ê·œ)
torch>=2.0.0+cu118
torchvision>=0.15.0+cu118
# ... GPU ìµœì í™” íŒ¨í‚¤ì§€ë“¤
```

**í´ë¼ìš°ë“œ ìµœì í™”**:
```txt
# requirements_cloud.txt (ì‹ ê·œ)  
boto3>=1.26.0  # AWS
google-cloud-storage>=2.7.0  # GCP
azure-storage-blob>=12.14.0  # Azure
```

### 2. Docker ì´ë¯¸ì§€ ê³„ì¸µí™”

**ê¸°ë³¸ ì´ë¯¸ì§€**:
```dockerfile
# Dockerfile (CPU, ë³€ê²½ ì—†ìŒ)
FROM python:3.9-slim
```

**GPU ì´ë¯¸ì§€**:
```dockerfile
# Dockerfile.gpu (ì‹ ê·œ)
FROM nvidia/cuda:11.8-runtime-ubuntu20.04
```

**í´ë¼ìš°ë“œ ì´ë¯¸ì§€**:
```dockerfile
# Dockerfile.cloud (ì‹ ê·œ)
FROM nvidia/cuda:11.8-runtime-ubuntu20.04
# í´ë¼ìš°ë“œ SDK í¬í•¨
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### 1. í™˜ê²½ë³„ í…ŒìŠ¤íŠ¸

**ë¡œì»¬ CPU** (ê¸°ì¡´):
```bash
python test_pipeline_modules.py --device cpu
```

**ë¡œì»¬ GPU**:
```bash
python test_pipeline_modules.py --device cuda
```

**í´ë¼ìš°ë“œ**:
```bash
python test_pipeline_modules.py --cloud aws --device auto
```

### 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

**ì§€í‘œ**:
- í›ˆë ¨ ì‹œê°„ (epochë‹¹)
- ì¶”ë¡  ì†ë„ (ì´ë¯¸ì§€ë‹¹)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ë¹„ìš© íš¨ìœ¨ì„±

**ìë™ ë²¤ì¹˜ë§ˆí‚¹**:
```python
# benchmarks/performance_test.py
class MultiEnvironmentBenchmark:
    def run_all_tests(self):
        results = {}
        for device in ['cpu', 'cuda', 'mps']:
            if self.is_available(device):
                results[device] = self.benchmark_device(device)
        return results
```

---

## ğŸ”’ ë³´ì•ˆ ë° ëª¨ë‹ˆí„°ë§

### 1. ë³´ì•ˆ

**í´ë¼ìš°ë“œ ë³´ì•ˆ**:
- IAM ì—­í•  ê¸°ë°˜ ì•¡ì„¸ìŠ¤
- VPC ë‚´ ë°°í¬
- ì•”í˜¸í™”ëœ ìŠ¤í† ë¦¬ì§€

**API í‚¤ ê´€ë¦¬**:
```python
# src/utils/secrets_manager.py
class SecretsManager:
    def get_cloud_credentials(self, provider):
        # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” í´ë¼ìš°ë“œ í‚¤ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì‚¬ìš©
        pass
```

### 2. ëª¨ë‹ˆí„°ë§

**ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**:
- GPU ì‚¬ìš©ë¥ 
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„
- ì˜¤ë¥˜ìœ¨

**ë¡œê¹…**:
```python
# src/utils/cloud_logger.py
class CloudLogger:
    def __init__(self, provider='local'):
        if provider == 'aws':
            self.setup_cloudwatch()
        elif provider == 'gcp':
            self.setup_cloud_logging()
```

---

## ğŸ“ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œë“œë§µ

### ë‹¨ê³„ 1: ì¤€ë¹„ (1ì¼)
- [ ] í˜„ì¬ ì½”ë“œ ë°±ì—…
- [ ] ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ê¸°ë³¸ GPU ì§€ì› ì¶”ê°€

### ë‹¨ê³„ 2: GPU ìµœì í™” (2ì¼)  
- [ ] CNN ëª¨ë¸ GPU ê°€ì†
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- [ ] ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ 

### ë‹¨ê³„ 3: í´ë¼ìš°ë“œ ì¤€ë¹„ (2ì¼)
- [ ] Docker ì»¨í…Œì´ë„ˆí™”
- [ ] í´ë¼ìš°ë“œ ì„¤ì • ì¶”ê°€
- [ ] ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

### ë‹¨ê³„ 4: í…ŒìŠ¤íŠ¸ ë° ë¬¸ì„œí™” (1ì¼)
- [ ] ë‹¤ì¤‘ í™˜ê²½ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

---

## ğŸ’° ë¹„ìš© ë¶„ì„

### ë¡œì»¬ vs í´ë¼ìš°ë“œ ë¹„êµ

| í™˜ê²½ | ì´ˆê¸° ë¹„ìš© | ìš´ì˜ ë¹„ìš© | ì„±ëŠ¥ | í™•ì¥ì„± | ì„¤ì • ì‹œê°„ |
|------|-----------|-----------|------|--------|----------|
| **ë¡œì»¬ CPU** | $0 | ì „ë ¥ë¹„ | ê¸°ì¤€ (1x) | ì œí•œì  | 0ë¶„ |
| **ë¡œì»¬ GPU** | $500-2000 | ì „ë ¥ë¹„ | 5-15ë°° | ì œí•œì  | 30ë¶„ |
| **Runpod RTX 4090** | $0 | $0.35-0.69/ì‹œê°„ | 15-25ë°° | ë¬´ì œí•œ | 5ë¶„ |
| **Runpod A100** | $0 | $1.5-3/ì‹œê°„ | 20-50ë°° | ë¬´ì œí•œ | 5ë¶„ |

### ê¶Œì¥ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

**ë¡œì»¬ CPU**: ê°œë°œ, í…ŒìŠ¤íŠ¸, ì†Œê·œëª¨ ë¶„ì„ (< 1000 ì´ë¯¸ì§€)  
**ë¡œì»¬ GPU**: ì¤‘ê·œëª¨ ì—°êµ¬, í”„ë¡œí† íƒ€ì´í•‘ (< 10,000 ì´ë¯¸ì§€)  
**Runpod RTX 4090**: ëŒ€ê·œëª¨ í›ˆë ¨, ì‹¤í—˜ (< 100,000 ì´ë¯¸ì§€)  
**Runpod A100**: í”„ë¡œë•ì…˜ ë°°í¬, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ (ë¬´ì œí•œ)

### ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„

**1ì‹œê°„ í›ˆë ¨ ì‘ì—… ê¸°ì¤€**:
- ë¡œì»¬ CPU (8ì½”ì–´): 24ì‹œê°„ ì†Œìš” â†’ ì „ë ¥ë¹„ $2-5
- ë¡œì»¬ GPU (RTX 3080): 2ì‹œê°„ ì†Œìš” â†’ ì „ë ¥ë¹„ $0.3
- Runpod RTX 4090: 1ì‹œê°„ ì†Œìš” â†’ $0.69
- Runpod A100-80GB: 30ë¶„ ì†Œìš” â†’ $1.5

**ê²°ë¡ **: ì¤‘ê·œëª¨ ì´ìƒ ì‘ì—…ì—ì„œëŠ” Runpodì´ ê°€ì¥ ë¹„ìš© íš¨ìœ¨ì 

---

ì´ ê³„íšì„ í†µí•´ í˜„ì¬ ë¡œì»¬ ê¸°ëŠ¥ì„ ì™„ì „íˆ ë³´ì¡´í•˜ë©´ì„œ GPU ë° í´ë¼ìš°ë“œ í™˜ê²½ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë³€ê²½ì‚¬í•­ì€ opt-in ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ê¸°ì¡´ ì‚¬ìš©ìì—ê²Œ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.