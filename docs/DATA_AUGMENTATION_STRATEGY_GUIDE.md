# ê¸°ë¢° íƒì§€ ë°ì´í„° ë¶„í•  ë° ì¦ê°• ì „ëµ ì¢…í•© ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 20ì¼
**ë²„ì „**: 1.0
**ëª©ì **: ê·¹ë‹¨ì  í´ë˜ìŠ¤ ë¶ˆê· í˜• ë° ì˜¤ë²„í”¼íŒ… í•´ê²°ì„ ìœ„í•œ ê³¼í•™ì  ë°ì´í„° ì²˜ë¦¬ ì „ëµ

---

## ğŸ“‘ ëª©ì°¨

1. [ì´ë¡ ì  ë°°ê²½](#1-ì´ë¡ ì -ë°°ê²½)
2. [í”„ë¡œì íŠ¸ ë°ì´í„° í˜•ì‹ ì´í•´](#2-í”„ë¡œì íŠ¸-ë°ì´í„°-í˜•ì‹-ì´í•´)
3. [NPY ê¸°ë°˜ ì „ëµ (ê¶Œì¥)](#3-npy-ê¸°ë°˜-ì „ëµ-ê¶Œì¥)
4. [BMP ê¸°ë°˜ ì „ëµ (ì–´ë…¸í…Œì´ì…˜ í™œìš©)](#4-bmp-ê¸°ë°˜-ì „ëµ-ì–´ë…¸í…Œì´ì…˜-í™œìš©)
5. [Cross-Validation ì „ëµ](#5-cross-validation-ì „ëµ)
6. [ì„±ëŠ¥ í‰ê°€ ë° ê²€ì¦](#6-ì„±ëŠ¥-í‰ê°€-ë°-ê²€ì¦)
7. [ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê¶Œì¥ì‚¬í•­](#7-ì²´í¬ë¦¬ìŠ¤íŠ¸-ë°-ê¶Œì¥ì‚¬í•­)

---

## 1. ì´ë¡ ì  ë°°ê²½

### 1.1 í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ

**í”„ë¡œì íŠ¸ í˜„í™©:**
- ê¸°ë¢°: 25ê°œ ìƒ˜í”Œ (0.22%)
- ë°°ê²½: ì†Œë‚˜ ì´ë¯¸ì§€ ì „ì²´ (99.78%)
- ìì—° ë¹„ìœ¨: **1:463**

**1:1 ê· í˜•ì˜ ë¬¸ì œì :**
```
ìì—° ë¶„í¬: ê¸°ë¢° 0.22%, ë°°ê²½ 99.78% (1:463)
1:1 í›ˆë ¨:  ê¸°ë¢° 50%, ë°°ê²½ 50%

â†’ ê¸°ë¢° 227ë°° ê³¼ëŒ€í‘œí˜„
â†’ ì˜¤ë²„í”¼íŒ… + False Positive í­ì¦
â†’ ì‹¤ì „ ì‚¬ìš© ë¶ˆê°€ëŠ¥ (ê²½ë³´ í­ì£¼)
```

### 1.2 Data Leakage ë°©ì§€ ì›ì¹™

**ìµœì‹  ì—°êµ¬ í•©ì˜ (2025):**

> "Performing class balancing techniques (SMOTE, augmentation) **before splitting** causes information from the test set to bleed into the training set, **inflating metrics**. You should apply resampling **only to the training subset after splitting**."
>
> â€” Source: Cross Validated, Imbalanced-Learn Documentation

**ì˜ëª»ëœ ìˆœì„œ:**
```python
# âŒ ì˜ëª»ëœ ë°©ë²•
ì›ë³¸ 25ê°œ â†’ 275ê°œ ì¦ê°• â†’ Train(220)/Val(55) ë¶„í• 
â†’ ê²€ì¦ì…‹ì— í›ˆë ¨ ë°ì´í„°ì˜ ì¦ê°•ë³¸ í¬í•¨
â†’ ê³¼ëŒ€í‰ê°€! (ê°™ì€ ê¸°ë¢°ì˜ ë‹¤ë¥¸ ê°ë„)
```

**ì˜¬ë°”ë¥¸ ìˆœì„œ:**
```python
# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
ì›ë³¸ 25ê°œ â†’ Train(20)/Val(5) ë¶„í•  â†’ ê°ê° ë…ë¦½ ì¦ê°•
â†’ ì™„ì „íˆ ë‹¤ë¥¸ ê¸°ë¢°ë¡œ ê²€ì¦
â†’ ì •í™•í•œ í‰ê°€!
```

### 1.3 Stratified Samplingì˜ ì¤‘ìš”ì„±

**ì—°êµ¬ ê²°ê³¼:**

> "On highly imbalanced data, vanilla random sampling can lead to a test set that contains **zero examples** of the minority class, making metrics such as recall or AUC **meaningless**."
>
> â€” Source: AWS Prescriptive Guidance, Machine Learning Operations

**Stratified Split í•„ìˆ˜ ì´ìœ :**
- ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤(ê¸°ë¢° 25ê°œ)ì—ì„œëŠ” ë¬´ì‘ìœ„ ë¶„í•  ì‹œ ê²€ì¦ì…‹ì— ê¸°ë¢° 0ê°œ ê°€ëŠ¥
- í´ë˜ìŠ¤ ë¹„ìœ¨ ë³´ì¡´ìœ¼ë¡œ **ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€** ë³´ì¥
- Cross-Validationì—ì„œ ê° foldê°€ ê· í˜•ìˆëŠ” í‘œí˜„ í™•ë³´

### 1.4 Hard Negative Mining

**ì†Œë‚˜ ê°ì²´ íƒì§€ ì—°êµ¬:**

> "Hard negative mining focuses on negative examples that are **currently rated as positive** or ambiguous by the detector, which can **strongly influence parameters** when the network is trained to correct them."
>
> â€” Source: ECCV 2018, Unsupervised Hard Example Mining

**ë°°ê²½ ìƒ˜í”Œ ì „ëµ:**
```
Hard Negatives (70%): ê¸°ë¢°ì™€ í˜¼ë™ ê°€ëŠ¥
    - ê¸°ë¢° ì£¼ë³€ 50-100m
    - ì•”ì„, ì¹¨ì „ë¬¼, ê°•í•œ ë°˜ì‚¬ì²´

Medium (20%): ì¤‘ê°„ ë³µì¡ë„
    - ê¸°ë¢°ì—ì„œ 100-200m ê±°ë¦¬
    - í•´ì´ˆ, ëª¨ë˜ íŒŒë„

Easy (10%): ë‚®ì€ ë³µì¡ë„
    - í‰í‰í•œ í•´ì €
    - ì •ë³´ ê°€ì¹˜ ë‚®ìŒ (baseline)
```

### 1.5 ì†Œë‚˜ ì´ë¯¸ì§€ ì¦ê°•ì˜ íŠ¹ìˆ˜ì„±

**ìµœì‹  ì—°êµ¬ ê²½ê³  (2025):**

> "Traditional augmentation methods designed for **camera images have limitations** when applied to sonar imaging because sonar images work on fundamentally different principles, using **sound waves rather than light**, resulting in high noise and low resolution."
>
> â€” Source: Frontiers in Marine Science, arXiv 2412.11840v1

**ì•ˆì „í•œ ì¦ê°• vs ìœ„í—˜í•œ ì¦ê°•:**

| ì¦ê°• ê¸°ë²• | ì†Œë‚˜ ì í•©ì„± | ì´ìœ  |
|----------|----------|------|
| **íšŒì „ (Rotation)** | âœ… ì•ˆì „ | ìŒí–¥ ë¬¼ë¦¬í•™ ë³´ì¡´ |
| **í‰í–‰ì´ë™ (Translation)** | âœ… ì•ˆì „ | ìœ„ì¹˜ ë³€í™”ë§Œ, ê°•ë„ ë¶ˆë³€ |
| **ìŠ¤ì¼€ì¼ë§ (Scaling)** | âœ… ë³´ìˆ˜ì  ì‚¬ìš© | ê±°ë¦¬ ë³€í™” ëª¨ì‚¬ (Â±10%) |
| **ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ** | âœ… ì•ˆì „ | ìŒí–¥ ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜ |
| **ìŒí–¥ ê·¸ë¦¼ì** | âœ… ì†Œë‚˜ íŠ¹í™” | ì†Œë‚˜ ë¬¼ë¦¬ í˜„ìƒ |
| **Mixup** | âŒ ìœ„í—˜ | SNR ë„ˆë¬´ ë‚®ì•„ í’ˆì§ˆ ì €í•˜ |
| **ë°ê¸° ì¡°ì •** | âš ï¸ ì£¼ì˜ | ìŒí–¥ ê°•ë„ ì›ë¦¬ ìœ„ë°˜ ê°€ëŠ¥ (Â±5%ë§Œ) |
| **ìƒ‰ìƒ ë³€í™˜** | âŒ ë¶ˆê°€ëŠ¥ | ì†Œë‚˜ëŠ” grayscale |

### 1.6 ì ì • ì¦ê°• ë°°ìˆ˜

**ì—°êµ¬ ê¸°ì¤€:**

> "Data augmentation factors between **5-20x** have been shown effective for imbalanced datasets, with 10x being optimal for most scenarios."
>
> â€” Source: Scientific Reports (Nature), Journal of Big Data

**ì¦ê°• ë°°ìˆ˜ ê°€ì´ë“œë¼ì¸:**

| ì¦ê°• ë°°ìˆ˜ | íš¨ê³¼ | ìœ„í—˜ | ê¶Œì¥ ëŒ€ìƒ |
|----------|------|------|-----------|
| **5ë°°** | ìµœì†Œ ë‹¤ì–‘ì„± | ë°ì´í„° ë¶€ì¡± | ì´ˆê¸° ì‹¤í—˜ |
| **10ë°°** | ìµœì  ê· í˜• | ë‚®ìŒ | **ê¶Œì¥ (í‘œì¤€)** |
| **15ë°°** | ë†’ì€ ë‹¤ì–‘ì„± | ì¸ê³µ íŒ¨í„´ ì¦ê°€ | ë³µì¡í•œ ë„ë©”ì¸ |
| **20ë°°** | ìµœëŒ€ ë‹¤ì–‘ì„± | ë†’ìŒ | ê·¹ë‹¨ì  ë¶ˆê· í˜• |
| **>20ë°°** | ê³¼ë„í•œ ì¦ê°• | ë§¤ìš° ë†’ìŒ | ë¹„ê¶Œì¥ |

---

## 2. í”„ë¡œì íŠ¸ ë°ì´í„° í˜•ì‹ ì´í•´

### 2.1 NPY íŒŒì¼ (NumPy ë°°ì—´)

**íŠ¹ì§•:**
```python
# íŒŒì¼ ì •ë³´
ê²½ë¡œ: data/processed/xtf_extracted/*.npy
í˜•ì‹: NumPy binary format (.npy)
ë°ì´í„° íƒ€ì…: float32
ê°’ ë²”ìœ„: 0.0 - 1.0 (ì •ê·œí™”ëœ ìŒí–¥ ê°•ë„)
Shape: (7974, 6832) - (pings, samples)
íŒŒì¼ í¬ê¸°: ~215MB (Git LFS ì‚¬ìš©)
```

**ë¡œë“œ ì˜ˆì‹œ:**
```python
import numpy as np

# NPY ë¡œë“œ
intensity_matrix = np.load(
    'data/processed/xtf_extracted/Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04_combined_intensity.npy'
)

print(f"Shape: {intensity_matrix.shape}")  # (7974, 6832)
print(f"Dtype: {intensity_matrix.dtype}")  # float32
print(f"Range: [{intensity_matrix.min():.4f}, {intensity_matrix.max():.4f}]")  # [0.0000, 1.0000]

# íŠ¹ì • ì˜ì—­ í™•ì¸
print(f"Sample value: {intensity_matrix[1000, 3000]}")  # 0.4523 (ì˜ˆì‹œ)
```

**ìƒì„± ê³¼ì •:**
```
XTF íŒŒì¼ (binary, 107MB)
    â†“ pyxtf.xtf_read()
ê° pingì˜ intensity íŒ¨í‚· ì¶”ì¶œ
    â†“ ì •ê·œí™” (min-max scaling to 0-1)
NumPy ë°°ì—´ ìŠ¤íƒ
    â†“ np.save()
NPY íŒŒì¼ (float32, 215MB)
```

**ì¥ì :**
- âœ… **ë†’ì€ ì •ë°€ë„**: 32-bit floating point
- âœ… **ì •ë³´ ì†ì‹¤ ì—†ìŒ**: ì›ë³¸ ê°•ë„ ê°’ ë³´ì¡´
- âœ… **ê¸°ê³„í•™ìŠµ ìµœì í™”**: ì •ê·œí™”ëœ ì…ë ¥
- âœ… **ë¹ ë¥¸ ì²˜ë¦¬**: NumPy native ì—°ì‚°

**ë‹¨ì :**
- âš ï¸ **ëŒ€ìš©ëŸ‰**: Git LFS í•„ìš”
- âš ï¸ **ì‹œê°í™” í•„ìš”**: ì§ì ‘ ë³¼ ìˆ˜ ì—†ìŒ

### 2.2 BMP íŒŒì¼ (Bitmap ì´ë¯¸ì§€)

**íŠ¹ì§•:**
```python
# ì›ë³¸ ì†Œë‚˜ ì´ë¯¸ì§€
ê²½ë¡œ: datasets/.../original/*.BMP
í˜•ì‹: Windows Bitmap (24-bit)
ë°ì´í„° íƒ€ì…: uint8
ê°’ ë²”ìœ„: 0 - 255 (í”½ì…€ ê°•ë„)
Shape: (7974, 1024) - grayscale
íŒŒì¼ í¬ê¸°: ~23MB

# ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€
ê²½ë¡œ: datasets/PH_annotation.bmp
í˜•ì‹: Windows Bitmap (24-bit RGB)
Shape: (3862, 1024, 3) - RGB
íŒŒì¼ í¬ê¸°: ~11MB
```

**ë¡œë“œ ì˜ˆì‹œ:**
```python
import cv2

# ì›ë³¸ ì†Œë‚˜ BMP
original_bmp = cv2.imread(
    'datasets/Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04/original/Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04_IMG_00.BMP',
    cv2.IMREAD_GRAYSCALE
)

print(f"Shape: {original_bmp.shape}")  # (7974, 1024)
print(f"Dtype: {original_bmp.dtype}")  # uint8
print(f"Range: [{original_bmp.min()}, {original_bmp.max()}]")  # [0, 255]

# ì–´ë…¸í…Œì´ì…˜ BMP (ê¸°ë¢° ìœ„ì¹˜ í‘œì‹œ)
annotation_bmp = cv2.imread(
    'datasets/PH_annotation.bmp',
    cv2.IMREAD_COLOR  # RGB
)

print(f"Shape: {annotation_bmp.shape}")  # (3862, 1024, 3)
print(f"Channels: {annotation_bmp.shape[2]}")  # 3 (B, G, R)
```

**ìƒì„± ê³¼ì •:**
```
XTF íŒŒì¼
    â†“ ì†Œë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ (SonarWiz, Hypack ë“±)
BMP ì´ë¯¸ì§€ (ì‹œê°í™”, uint8, 0-255)
    â†“ ìˆ˜ë™ ì–´ë…¸í…Œì´ì…˜ ë„êµ¬
Annotation BMP (ê¸°ë¢° ìœ„ì¹˜ í‘œì‹œ, RGB)
```

**ì¥ì :**
- âœ… **ì‹œê°í™” ìš©ì´**: ì§ì ‘ í™•ì¸ ê°€ëŠ¥
- âœ… **ì–´ë…¸í…Œì´ì…˜ í†µí•©**: ë¼ë²¨ ì •ë³´ í¬í•¨
- âœ… **í‘œì¤€ í˜•ì‹**: ëŒ€ë¶€ë¶„ì˜ ë„êµ¬ ì§€ì›
- âœ… **ì¤‘ê°„ í¬ê¸°**: Gitì—ì„œ ì§ì ‘ ê´€ë¦¬

**ë‹¨ì :**
- âš ï¸ **ë‚®ì€ ì •ë°€ë„**: 8-bit ì–‘ìí™”
- âš ï¸ **ì •ë³´ ì†ì‹¤**: 0-1 â†’ 0-255 ë³€í™˜ ì‹œ ì†ì‹¤
- âš ï¸ **í•´ìƒë„ ê°ì†Œ**: 6832 â†’ 1024 samples

### 2.3 í˜•ì‹ ë¹„êµí‘œ

| íŠ¹ì„± | NPY (float32) | BMP (uint8) |
|------|---------------|-------------|
| **ì •ë°€ë„** | 32-bit (ë†’ìŒ) | 8-bit (ë‚®ìŒ) |
| **ê°’ ë²”ìœ„** | 0.0 - 1.0 | 0 - 255 |
| **í•´ìƒë„** | (7974, 6832) | (7974, 1024) |
| **íŒŒì¼ í¬ê¸°** | ~215MB | ~23MB |
| **ì •ë³´ ë³´ì¡´** | 100% | ~93% (ì–‘ìí™” ì†ì‹¤) |
| **ML ì…ë ¥** | ì§ì ‘ ì‚¬ìš© | ë³€í™˜ í•„ìš” |
| **ì‹œê°í™”** | ë³€í™˜ í•„ìš” | ì§ì ‘ í‘œì‹œ |
| **ì–´ë…¸í…Œì´ì…˜** | ë³„ë„ ì²˜ë¦¬ | í†µí•© ê°€ëŠ¥ |
| **ê¶Œì¥ ìš©ë„** | **ì¦ê°•, íŠ¹ì§• ì¶”ì¶œ, í•™ìŠµ** | ì‹œê°í™”, ë¼ë²¨ë§ |

---

## 3. NPY ê¸°ë°˜ ì „ëµ (ê¶Œì¥)

### 3.1 ì „ì²´ ì›Œí¬í”Œë¡œìš° ê°œìš”

```
Step 1: ì›ë³¸ ë¶„í•  (ì¦ê°• ì „!)
    25ê°œ ì›ë³¸ ê¸°ë¢° â†’ Train(15) / Val(5) / Test(5)

Step 2: ë°°ê²½ ìƒ˜í”Œ ì¶”ì¶œ
    ê° splitë³„ ë…ë¦½ì ìœ¼ë¡œ Hard Negative Mining

Step 3: Train ì¦ê°• (10ë°°)
    15ê°œ â†’ 150ê°œ (ë‹¤ì–‘í•œ ë³€í™˜ ì¡°í•©)

Step 4: ë°ì´í„°ì…‹ êµ¬ì„±
    Train: 150 ê¸°ë¢° + 75 ë°°ê²½ (2:1)
    Val:     5 ê¸°ë¢° + 25 ë°°ê²½ (1:5, í˜„ì‹¤ ë°˜ì˜)
    Test:    5 ê¸°ë¢° + 25 ë°°ê²½ (1:5, í˜„ì‹¤ ë°˜ì˜)

Step 5: íŠ¹ì§• ì¶”ì¶œ ë° í•™ìŠµ
    Class weight='balanced' ì ìš©
```

### 3.2 Step 1: ì›ë³¸ ë°ì´í„° ë¶„í• 

#### 3.2.1 ê¸°ë¢° ì¢Œí‘œ ë¡œë“œ

```python
import numpy as np
from pathlib import Path
import json

# GPS ì¢Œí‘œ ë¡œë“œ
gps_coords_path = Path('data/processed/coordinate_mappings/pohang_mine_coordinates.json')
with open(gps_coords_path, 'r') as f:
    gps_data = json.load(f)

mine_gps_coords = gps_data['mine_coordinates']  # 25ê°œ
print(f"Total mines: {len(mine_gps_coords)}")

# ì˜ˆì‹œ êµ¬ì¡°
# mine_gps_coords = [
#     {"id": 1, "lat": 36.034500, "lon": 129.387667},
#     {"id": 2, "lat": 36.034517, "lon": 129.387683},
#     ...
# ]
```

#### 3.2.2 NPY ê°•ë„ ë°ì´í„° ë¡œë“œ

```python
# NPY ê°•ë„ ë§¤íŠ¸ë¦­ìŠ¤ ë¡œë“œ
intensity_npy_path = Path(
    'data/processed/xtf_extracted/'
    'Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04_combined_intensity.npy'
)

intensity_matrix = np.load(intensity_npy_path)  # (7974, 6832) float32

print(f"Intensity matrix shape: {intensity_matrix.shape}")
print(f"Data type: {intensity_matrix.dtype}")
print(f"Value range: [{intensity_matrix.min():.4f}, {intensity_matrix.max():.4f}]")

# Output:
# Intensity matrix shape: (7974, 6832)
# Data type: float32
# Value range: [0.0000, 1.0000]
```

#### 3.2.3 GPS â†’ í”½ì…€ ì¢Œí‘œ ë³€í™˜

```python
from src.data_processing.coordinate_mapper import GPSToPixelMapper

# ì¢Œí‘œ ë³€í™˜ê¸° ì´ˆê¸°í™”
mapper = GPSToPixelMapper(
    xtf_metadata_path='data/processed/xtf_extracted/Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04_metadata.json'
)

# GPS â†’ í”½ì…€ ë³€í™˜
mine_pixel_coords = []
for mine in mine_gps_coords:
    pixel_coord = mapper.gps_to_pixel(
        lat=mine['lat'],
        lon=mine['lon']
    )
    mine_pixel_coords.append({
        'id': mine['id'],
        'ping_idx': pixel_coord['ping_idx'],
        'sample_idx': pixel_coord['sample_idx']
    })

print(f"Converted {len(mine_pixel_coords)} mine coordinates")

# ì˜ˆì‹œ ì¶œë ¥
# mine_pixel_coords = [
#     {'id': 1, 'ping_idx': 1234, 'sample_idx': 3456},
#     {'id': 2, 'ping_idx': 1245, 'sample_idx': 3467},
#     ...
# ]
```

#### 3.2.4 íŒ¨ì¹˜ ì¶”ì¶œ (64Ã—64)

```python
def extract_mine_patches(
    intensity_matrix: np.ndarray,
    mine_pixel_coords: list,
    patch_size: int = 64
) -> tuple:
    """
    ê¸°ë¢° ìœ„ì¹˜ì—ì„œ íŒ¨ì¹˜ ì¶”ì¶œ

    Args:
        intensity_matrix: ê°•ë„ ë°ì´í„° (H, W) float32
        mine_pixel_coords: í”½ì…€ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
        patch_size: íŒ¨ì¹˜ í¬ê¸° (ê¸°ë³¸ 64x64)

    Returns:
        (patches, valid_indices): ìœ íš¨í•œ íŒ¨ì¹˜ì™€ ì¸ë±ìŠ¤
    """
    patches = []
    valid_indices = []
    half_size = patch_size // 2

    h, w = intensity_matrix.shape

    for idx, coord in enumerate(mine_pixel_coords):
        ping_idx = coord['ping_idx']
        sample_idx = coord['sample_idx']

        # ê²½ê³„ ì²´í¬
        if (ping_idx - half_size < 0 or ping_idx + half_size > h or
            sample_idx - half_size < 0 or sample_idx + half_size > w):
            print(f"Warning: Mine {coord['id']} at ({ping_idx}, {sample_idx}) is too close to boundary, skipping")
            continue

        # íŒ¨ì¹˜ ì¶”ì¶œ
        patch = intensity_matrix[
            ping_idx - half_size : ping_idx + half_size,
            sample_idx - half_size : sample_idx + half_size
        ]

        # í¬ê¸° ê²€ì¦
        if patch.shape == (patch_size, patch_size):
            patches.append(patch)
            valid_indices.append(idx)
        else:
            print(f"Warning: Patch for mine {coord['id']} has invalid shape {patch.shape}, skipping")

    return np.array(patches), valid_indices

# íŒ¨ì¹˜ ì¶”ì¶œ
mine_patches, valid_indices = extract_mine_patches(
    intensity_matrix,
    mine_pixel_coords,
    patch_size=64
)

print(f"Extracted patches: {mine_patches.shape}")
print(f"Data type: {mine_patches.dtype}")

# Output:
# Extracted patches: (25, 64, 64)
# Data type: float32
```

#### 3.2.5 Train-Val-Test Split (Stratified)

```python
from sklearn.model_selection import train_test_split

# ì›ë³¸ ì¸ë±ìŠ¤
original_indices = np.arange(len(mine_patches))  # [0, 1, 2, ..., 24]

# 1ì°¨ ë¶„í• : Train+Val (80%) vs Test (20%)
train_val_idx, test_idx = train_test_split(
    original_indices,
    test_size=0.2,      # 5ê°œ í…ŒìŠ¤íŠ¸
    random_state=42,
    shuffle=True
)

print(f"Train+Val: {len(train_val_idx)} samples")  # 20ê°œ
print(f"Test: {len(test_idx)} samples")            # 5ê°œ

# 2ì°¨ ë¶„í• : Train (75%) vs Val (25%) from Train+Val
train_idx, val_idx = train_test_split(
    train_val_idx,
    test_size=0.25,     # 20ê°œì˜ 25% = 5ê°œ
    random_state=42,
    shuffle=True
)

print(f"\n=== Final Split ===")
print(f"Train: {len(train_idx)} samples")  # 15ê°œ
print(f"Val:   {len(val_idx)} samples")    # 5ê°œ
print(f"Test:  {len(test_idx)} samples")   # 5ê°œ

# Split ì €ì¥ (ì¬í˜„ì„±)
split_info = {
    'train_indices': train_idx.tolist(),
    'val_indices': val_idx.tolist(),
    'test_indices': test_idx.tolist(),
    'random_state': 42,
    'split_date': '2025-10-20'
}

split_save_path = Path('data/processed/splits/mine_split_info.json')
split_save_path.parent.mkdir(parents=True, exist_ok=True)
with open(split_save_path, 'w') as f:
    json.dump(split_info, f, indent=2)

print(f"\nSplit info saved to: {split_save_path}")
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âœ… **ì¦ê°• ì „ ë¶„í• **: ì›ë³¸ 25ê°œë¥¼ ë¨¼ì € ì™„ì „ ë¶„ë¦¬
- âœ… **3-way split**: Train/Val/Test ì™„ì „ ë…ë¦½
- âœ… **Test ê³ ì •**: Test 5ê°œëŠ” ì ˆëŒ€ ì¦ê°•í•˜ì§€ ì•ŠìŒ
- âœ… **ì¬í˜„ì„± ë³´ì¥**: random_state=42 ê³ ì •, ì¸ë±ìŠ¤ ì €ì¥

### 3.3 Step 2: ë°°ê²½ ìƒ˜í”Œ ì¶”ì¶œ (Hard Negative Mining)

#### 3.3.1 Hard Negative Mining êµ¬í˜„

```python
import random
from typing import List, Tuple

def sample_background_patches_hard_negative(
    intensity_matrix: np.ndarray,
    mine_pixel_coords: List[dict],
    mine_indices: np.ndarray,
    num_samples: int,
    hard_negative_ratio: float = 0.7,
    medium_ratio: float = 0.2,
    patch_size: int = 64
) -> List[np.ndarray]:
    """
    Hard Negative Miningìœ¼ë¡œ ë°°ê²½ íŒ¨ì¹˜ ìƒ˜í”Œë§

    Args:
        intensity_matrix: ê°•ë„ ë°ì´í„° (H, W) float32
        mine_pixel_coords: ì „ì²´ ê¸°ë¢° í”½ì…€ ì¢Œí‘œ
        mine_indices: í˜„ì¬ splitì— í•´ë‹¹í•˜ëŠ” ê¸°ë¢° ì¸ë±ìŠ¤
        num_samples: ì¶”ì¶œí•  ë°°ê²½ ìƒ˜í”Œ ìˆ˜
        hard_negative_ratio: Hard Negative ë¹„ìœ¨ (ê¸°ë³¸ 70%)
        medium_ratio: Medium Negative ë¹„ìœ¨ (ê¸°ë³¸ 20%)
        patch_size: íŒ¨ì¹˜ í¬ê¸°

    Returns:
        ë°°ê²½ íŒ¨ì¹˜ ë¦¬ìŠ¤íŠ¸
    """
    background_patches = []
    half_size = patch_size // 2
    h, w = intensity_matrix.shape

    # í˜„ì¬ splitì˜ ê¸°ë¢° ì¢Œí‘œë§Œ ì‚¬ìš©
    current_mine_coords = [mine_pixel_coords[i] for i in mine_indices]

    # Hard Negative: ê¸°ë¢° ì£¼ë³€ 50-100m
    n_hard = int(num_samples * hard_negative_ratio)
    hard_count = 0

    while hard_count < n_hard:
        # ëœë¤ ê¸°ë¢° ì„ íƒ
        mine_coord = random.choice(current_mine_coords)
        ping_center = mine_coord['ping_idx']
        sample_center = mine_coord['sample_idx']

        # 50-100m ë²”ìœ„ (í”½ì…€ ë‹¨ìœ„ë¡œ ë³€í™˜, ì˜ˆ: 1m = 2 pixels)
        offset_ping = random.randint(100, 200) * random.choice([-1, 1])  # 50-100m
        offset_sample = random.randint(100, 200) * random.choice([-1, 1])

        ping_idx = ping_center + offset_ping
        sample_idx = sample_center + offset_sample

        # ê²½ê³„ ë° ê¸°ë¢° ì¤‘ë³µ ì²´í¬
        if not is_valid_background_patch(
            ping_idx, sample_idx, h, w, half_size, current_mine_coords
        ):
            continue

        # íŒ¨ì¹˜ ì¶”ì¶œ
        patch = intensity_matrix[
            ping_idx - half_size : ping_idx + half_size,
            sample_idx - half_size : sample_idx + half_size
        ]

        if patch.shape == (patch_size, patch_size):
            background_patches.append(patch)
            hard_count += 1

    # Medium Negative: ê¸°ë¢°ì—ì„œ 100-200m
    n_medium = int(num_samples * medium_ratio)
    medium_count = 0

    while medium_count < n_medium:
        mine_coord = random.choice(current_mine_coords)
        ping_center = mine_coord['ping_idx']
        sample_center = mine_coord['sample_idx']

        # 100-200m ë²”ìœ„
        offset_ping = random.randint(200, 400) * random.choice([-1, 1])
        offset_sample = random.randint(200, 400) * random.choice([-1, 1])

        ping_idx = ping_center + offset_ping
        sample_idx = sample_center + offset_sample

        if not is_valid_background_patch(
            ping_idx, sample_idx, h, w, half_size, current_mine_coords
        ):
            continue

        patch = intensity_matrix[
            ping_idx - half_size : ping_idx + half_size,
            sample_idx - half_size : sample_idx + half_size
        ]

        if patch.shape == (patch_size, patch_size):
            background_patches.append(patch)
            medium_count += 1

    # Easy Negative: ë¬´ì‘ìœ„ ìœ„ì¹˜
    easy_ratio = 1.0 - hard_negative_ratio - medium_ratio
    n_easy = num_samples - n_hard - n_medium
    easy_count = 0

    max_attempts = n_easy * 10  # ë¬´í•œ ë£¨í”„ ë°©ì§€
    attempts = 0

    while easy_count < n_easy and attempts < max_attempts:
        ping_idx = random.randint(half_size, h - half_size)
        sample_idx = random.randint(half_size, w - half_size)

        if not is_valid_background_patch(
            ping_idx, sample_idx, h, w, half_size, current_mine_coords
        ):
            attempts += 1
            continue

        patch = intensity_matrix[
            ping_idx - half_size : ping_idx + half_size,
            sample_idx - half_size : sample_idx + half_size
        ]

        if patch.shape == (patch_size, patch_size):
            background_patches.append(patch)
            easy_count += 1

        attempts += 1

    print(f"Background sampling: Hard={n_hard}, Medium={n_medium}, Easy={easy_count}")

    return background_patches


def is_valid_background_patch(
    ping_idx: int,
    sample_idx: int,
    h: int,
    w: int,
    half_size: int,
    mine_coords: List[dict],
    min_distance: int = 64
) -> bool:
    """
    ë°°ê²½ íŒ¨ì¹˜ ìœ íš¨ì„± ê²€ì‚¬

    Args:
        ping_idx, sample_idx: íŒ¨ì¹˜ ì¤‘ì‹¬ ì¢Œí‘œ
        h, w: ì´ë¯¸ì§€ í¬ê¸°
        half_size: íŒ¨ì¹˜ ë°˜ í¬ê¸°
        mine_coords: ê¸°ë¢° ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
        min_distance: ê¸°ë¢°ë¡œë¶€í„° ìµœì†Œ ê±°ë¦¬ (í”½ì…€)

    Returns:
        ìœ íš¨ ì—¬ë¶€
    """
    # ê²½ê³„ ì²´í¬
    if (ping_idx - half_size < 0 or ping_idx + half_size > h or
        sample_idx - half_size < 0 or sample_idx + half_size > w):
        return False

    # ê¸°ë¢°ì™€ì˜ ê±°ë¦¬ ì²´í¬ (ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ì œì™¸)
    for mine_coord in mine_coords:
        mine_ping = mine_coord['ping_idx']
        mine_sample = mine_coord['sample_idx']

        distance = np.sqrt(
            (ping_idx - mine_ping)**2 + (sample_idx - mine_sample)**2
        )

        if distance < min_distance:
            return False

    return True
```

#### 3.3.2 ê° Splitë³„ ë°°ê²½ ìƒ˜í”Œë§

```python
# Train ë°°ê²½ ìƒ˜í”Œë§ (1:5 ë¹„ìœ¨)
train_bg_patches = sample_background_patches_hard_negative(
    intensity_matrix=intensity_matrix,
    mine_pixel_coords=mine_pixel_coords,
    mine_indices=train_idx,  # 15ê°œ ê¸°ë¢°ë§Œ ì‚¬ìš©
    num_samples=len(train_idx) * 5,  # 15 Ã— 5 = 75ê°œ
    hard_negative_ratio=0.7,
    medium_ratio=0.2,
    patch_size=64
)

print(f"Train background patches: {len(train_bg_patches)}")
# Output: Train background patches: 75

# Val ë°°ê²½ ìƒ˜í”Œë§ (1:5 ë¹„ìœ¨)
val_bg_patches = sample_background_patches_hard_negative(
    intensity_matrix=intensity_matrix,
    mine_pixel_coords=mine_pixel_coords,
    mine_indices=val_idx,  # 5ê°œ ê¸°ë¢°ë§Œ ì‚¬ìš©
    num_samples=len(val_idx) * 5,  # 5 Ã— 5 = 25ê°œ
    hard_negative_ratio=0.7,
    medium_ratio=0.2,
    patch_size=64
)

print(f"Val background patches: {len(val_bg_patches)}")
# Output: Val background patches: 25

# Test ë°°ê²½ ìƒ˜í”Œë§ (1:5 ë¹„ìœ¨)
test_bg_patches = sample_background_patches_hard_negative(
    intensity_matrix=intensity_matrix,
    mine_pixel_coords=mine_pixel_coords,
    mine_indices=test_idx,  # 5ê°œ ê¸°ë¢°ë§Œ ì‚¬ìš©
    num_samples=len(test_idx) * 5,  # 5 Ã— 5 = 25ê°œ
    hard_negative_ratio=0.7,
    medium_ratio=0.2,
    patch_size=64
)

print(f"Test background patches: {len(test_bg_patches)}")
# Output: Test background patches: 25
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âœ… **ë…ë¦½ ìƒ˜í”Œë§**: ê° splitì˜ ê¸°ë¢° ì£¼ë³€ì—ì„œë§Œ ìƒ˜í”Œë§
- âœ… **Hard Negative 70%**: ê¸°ë¢°ì™€ í˜¼ë™ ê°€ëŠ¥í•œ ì–´ë ¤ìš´ ìƒ˜í”Œ ìš°ì„ 
- âœ… **ì¤‘ë³µ ë°©ì§€**: ê¸°ë¢° ìœ„ì¹˜ì—ì„œ ìµœì†Œ ê±°ë¦¬ ìœ ì§€
- âœ… **í˜„ì‹¤ ë°˜ì˜**: Val/TestëŠ” 1:5 ë¹„ìœ¨ (ì‹¤ì œ ìš´ìš© í™˜ê²½)

### 3.4 Step 3: Train Set ì¦ê°• (10ë°°)

#### 3.4.1 ì†Œë‚˜ ì•ˆì „ ì¦ê°• ì„¤ì •

```python
from src.data_augmentation.augmentation_engine import (
    AdvancedAugmentationEngine,
    AugmentationConfig
)

# ì†Œë‚˜ ì „ìš© ì•ˆì „ ì¦ê°• ì„¤ì •
safe_sonar_config = AugmentationConfig(
    # === ê¸°í•˜í•™ì  ë³€í™˜ (ì•ˆì „) ===
    rotation_range=(-180, 180),      # ëª¨ë“  ê°ë„ ê°€ëŠ¥ (ì†Œë‚˜ëŠ” ë°©í–¥ ë¬´ê´€)
    rotation_probability=0.8,        # ë†’ì€ í™•ë¥ 

    scale_range=(0.9, 1.1),          # Â±10% (ë³´ìˆ˜ì )
    scale_probability=0.5,

    translation_range=(-0.05, 0.05), # Â±5% ì´ë™
    translation_probability=0.4,

    # === ê´‘ë„ ë³€í™˜ (ì£¼ì˜) ===
    noise_std_range=(0.01, 0.03),    # ë‚®ì€ SNR ê³ ë ¤
    noise_probability=0.6,

    brightness_range=(0.95, 1.05),   # Â±5% (ë§¤ìš° ë³´ìˆ˜ì )
    brightness_probability=0.3,      # ë‚®ì€ í™•ë¥ 

    contrast_range=(0.95, 1.05),
    contrast_probability=0.3,

    # === ì†Œë‚˜ ì „ìš© íš¨ê³¼ ===
    acoustic_shadow_probability=0.2,  # ìŒí–¥ ê·¸ë¦¼ì
    beam_angle_variation=3.0,         # ë¹” ê°ë„ ë³€í™” (ë„)
    range_distortion=0.03,            # ê±°ë¦¬ ì™œê³¡ (3%)

    # === ì „ì²´ ê°•ë„ ===
    augmentation_strength=0.7         # 70% ì ìš© í™•ë¥ 
)

# ì¦ê°• ì—”ì§„ ì´ˆê¸°í™”
augmenter = AdvancedAugmentationEngine(config=safe_sonar_config)

print("ì†Œë‚˜ ì•ˆì „ ì¦ê°• ì—”ì§„ ì´ˆê¸°ï¿½ï¿½ï¿½ ì™„ë£Œ")
```

#### 3.4.2 ë‹¤ì–‘í•œ ì¦ê°• ì¡°í•© ìƒì„±

```python
def augment_mine_samples_diverse(
    mine_patches: np.ndarray,
    augmenter: AdvancedAugmentationEngine,
    augmentation_factor: int = 10
) -> np.ndarray:
    """
    ë‹¤ì–‘í•œ ë³€í™˜ ì¡°í•©ìœ¼ë¡œ ê¸°ë¢° ìƒ˜í”Œ ì¦ê°•

    Args:
        mine_patches: ì›ë³¸ ê¸°ë¢° íŒ¨ì¹˜ (N, H, W) float32
        augmenter: ì¦ê°• ì—”ì§„
        augmentation_factor: ì¦ê°• ë°°ìˆ˜ (10ë°° ê¶Œì¥)

    Returns:
        ì¦ê°•ëœ íŒ¨ì¹˜ ë°°ì—´ (N * factor, H, W)
    """
    augmented_patches = []

    # ë³€í™˜ ì¡°í•© í…œí”Œë¦¿ ì •ì˜
    augmentation_templates = [
        # 1. ì›ë³¸
        {'types': [], 'label': 'original'},

        # 2. ë‹¨ì¼ ë³€í™˜
        {'types': ['geometric'], 'label': 'rotation_only'},
        {'types': ['photometric'], 'label': 'noise_only'},
        {'types': ['sonar'], 'label': 'sonar_effects_only'},

        # 3. 2ê°€ì§€ ì¡°í•©
        {'types': ['geometric', 'photometric'], 'label': 'rotation+noise'},
        {'types': ['geometric', 'sonar'], 'label': 'rotation+sonar'},
        {'types': ['photometric', 'sonar'], 'label': 'noise+sonar'},

        # 4. 3ê°€ì§€ ì¡°í•©
        {'types': ['geometric', 'photometric', 'sonar'], 'label': 'all_light'},
        {'types': ['geometric', 'photometric', 'sonar'], 'label': 'all_medium'},
        {'types': ['geometric', 'photometric', 'sonar'], 'label': 'all_heavy'},
    ]

    # ê° ì›ë³¸ íŒ¨ì¹˜ì— ëŒ€í•´
    for patch_idx, original_patch in enumerate(mine_patches):
        # 1. ì›ë³¸ ì¶”ê°€
        augmented_patches.append(original_patch.copy())

        # 2. ì¦ê°• ìƒ˜í”Œ ìƒì„± (augmentation_factor - 1ê°œ)
        for aug_idx in range(augmentation_factor - 1):
            # í…œí”Œë¦¿ ì„ íƒ (ìˆœí™˜)
            template = augmentation_templates[(aug_idx % (len(augmentation_templates) - 1)) + 1]

            if len(template['types']) == 0:
                # ì›ë³¸ì€ ì´ë¯¸ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                continue

            # ì¦ê°• ì ìš©
            aug_patch, _ = augmenter.augment_single(
                original_patch,
                mask=None,
                augmentation_types=template['types']
            )

            augmented_patches.append(aug_patch)

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10%ë§ˆë‹¤)
            total_progress = (patch_idx * (augmentation_factor - 1) + aug_idx + 1)
            total_expected = len(mine_patches) * (augmentation_factor - 1)

            if total_progress % max(1, total_expected // 10) == 0:
                progress_pct = (total_progress / total_expected) * 100
                print(f"Augmentation progress: {progress_pct:.1f}% ({total_progress}/{total_expected})")

    augmented_array = np.array(augmented_patches)

    print(f"\nì¦ê°• ì™„ë£Œ: {len(mine_patches)} â†’ {len(augmented_array)} íŒ¨ì¹˜")

    return augmented_array
```

#### 3.4.3 Train ê¸°ë¢° ì¦ê°• ì‹¤í–‰

```python
# Train ê¸°ë¢° íŒ¨ì¹˜ ì¶”ì¶œ
train_mine_patches = mine_patches[train_idx]  # (15, 64, 64) float32

print(f"Original train mine patches: {train_mine_patches.shape}")
print(f"Data type: {train_mine_patches.dtype}")

# ì¦ê°• ì‹¤í–‰ (15 â†’ 150ê°œ)
train_mine_augmented = augment_mine_samples_diverse(
    mine_patches=train_mine_patches,
    augmenter=augmenter,
    augmentation_factor=10
)

print(f"\n=== Augmentation Results ===")
print(f"Original: {train_mine_patches.shape}")
print(f"Augmented: {train_mine_augmented.shape}")
print(f"Augmentation factor: {train_mine_augmented.shape[0] / train_mine_patches.shape[0]:.1f}x")

# Output:
# Original train mine patches: (15, 64, 64)
# Data type: float32
# Augmentation progress: 10.0% (14/135)
# Augmentation progress: 20.0% (27/135)
# ...
# Augmentation progress: 100.0% (135/135)
#
# ì¦ê°• ì™„ë£Œ: 15 â†’ 150 íŒ¨ì¹˜
#
# === Augmentation Results ===
# Original: (15, 64, 64)
# Augmented: (150, 64, 64)
# Augmentation factor: 10.0x
```

#### 3.4.4 ì¦ê°• í’ˆì§ˆ ê²€ì¦

```python
from src.data_augmentation.augmentation_engine import AugmentationValidator

# ê²€ì¦ê¸° ì´ˆê¸°í™”
validator = AugmentationValidator()

# ìƒ˜í”Œ ì¦ê°• í’ˆì§ˆ í‰ê°€
sample_idx = 0
original_sample = train_mine_patches[sample_idx]
augmented_sample = train_mine_augmented[sample_idx + 10]  # 10ë²ˆì§¸ ì¦ê°•ë³¸

quality_metrics = validator.validate_augmentation_quality(
    original_image=original_sample,
    augmented_image=augmented_sample
)

print("=== ì¦ê°• í’ˆì§ˆ í‰ê°€ ===")
for metric, value in quality_metrics.items():
    print(f"{metric}: {value:.4f}")

# Output:
# === ì¦ê°• í’ˆì§ˆ í‰ê°€ ===
# structural_similarity: 0.8234
# histogram_similarity: 0.9123
# energy_preservation: 0.9567
# gradient_preservation: 0.8891

# ë°ì´í„°ì…‹ ë‹¤ì–‘ì„± í‰ê°€
diversity_metrics = validator.assess_dataset_diversity(
    images=list(train_mine_augmented[:50])  # ìƒ˜í”Œë§
)

print("\n=== ë°ì´í„°ì…‹ ë‹¤ì–‘ì„± í‰ê°€ ===")
for metric, value in diversity_metrics.items():
    print(f"{metric}: {value:.4f}")

# Output:
# === ë°ì´í„°ì…‹ ë‹¤ì–‘ì„± í‰ê°€ ===
# diversity_score: 0.6234 (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
# similarity_std: 0.1523
# histogram_diversity: 45.2341
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âœ… **ë‹¤ì–‘í•œ ì¡°í•©**: 9ê°€ì§€ ë³€í™˜ í…œí”Œë¦¿ìœ¼ë¡œ ì¸ê³µ íŒ¨í„´ ë°©ì§€
- âœ… **ë³´ìˆ˜ì  íŒŒë¼ë¯¸í„°**: ì†Œë‚˜ ë¬¼ë¦¬í•™ íŠ¹ì„± ë³´ì¡´
- âœ… **í’ˆì§ˆ ê²€ì¦**: SSIM, íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„ë¡œ ê²€ì¦
- âœ… **Trainë§Œ ì¦ê°•**: Val/TestëŠ” ì¼ë°˜í™” ëŠ¥ë ¥ í‰ê°€

### 3.5 Step 4: ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±

#### 3.5.1 Train Dataset

```python
# Train ë°ì´í„°ì…‹ êµ¬ì„±
X_train_mines = train_mine_augmented  # (150, 64, 64) float32
X_train_bg = np.array(train_bg_patches)  # (75, 64, 64) float32

X_train = np.vstack([X_train_mines, X_train_bg])  # (225, 64, 64)
y_train = np.hstack([
    np.ones(len(X_train_mines)),   # 150ê°œ ê¸°ë¢° (label=1)
    np.zeros(len(X_train_bg))      # 75ê°œ ë°°ê²½ (label=0)
])

# Shuffle
np.random.seed(42)
shuffle_idx = np.random.permutation(len(X_train))
X_train = X_train[shuffle_idx]
y_train = y_train[shuffle_idx]

print("=== Train Dataset ===")
print(f"Shape: {X_train.shape}")
print(f"Labels: {y_train.shape}")
print(f"ê¸°ë¢°: {np.sum(y_train == 1)} samples")
print(f"ë°°ê²½: {np.sum(y_train == 0)} samples")
print(f"ë¹„ìœ¨: {np.sum(y_train == 1)}:{np.sum(y_train == 0)} (2:1)")

# Output:
# === Train Dataset ===
# Shape: (225, 64, 64)
# Labels: (225,)
# ê¸°ë¢°: 150 samples
# ë°°ê²½: 75 samples
# ë¹„ìœ¨: 150:75 (2:1)
```

#### 3.5.2 Validation Dataset

```python
# Val ë°ì´í„°ì…‹ êµ¬ì„± (ì¦ê°• ì•ˆ í•¨!)
X_val_mines = mine_patches[val_idx]  # (5, 64, 64) float32 ì›ë³¸ë§Œ
X_val_bg = np.array(val_bg_patches)  # (25, 64, 64) float32

X_val = np.vstack([X_val_mines, X_val_bg])  # (30, 64, 64)
y_val = np.hstack([
    np.ones(len(X_val_mines)),    # 5ê°œ ê¸°ë¢° (label=1)
    np.zeros(len(X_val_bg))       # 25ê°œ ë°°ê²½ (label=0)
])

# Shuffle
np.random.seed(42)
shuffle_idx = np.random.permutation(len(X_val))
X_val = X_val[shuffle_idx]
y_val = y_val[shuffle_idx]

print("\n=== Validation Dataset ===")
print(f"Shape: {X_val.shape}")
print(f"Labels: {y_val.shape}")
print(f"ê¸°ë¢°: {np.sum(y_val == 1)} samples (ì›ë³¸ë§Œ)")
print(f"ë°°ê²½: {np.sum(y_val == 0)} samples")
print(f"ë¹„ìœ¨: {np.sum(y_val == 1)}:{np.sum(y_val == 0)} (1:5, í˜„ì‹¤ ë°˜ì˜)")

# Output:
# === Validation Dataset ===
# Shape: (30, 64, 64)
# Labels: (30,)
# ê¸°ë¢°: 5 samples (ì›ë³¸ë§Œ)
# ë°°ê²½: 25 samples
# ë¹„ìœ¨: 5:25 (1:5, í˜„ì‹¤ ë°˜ì˜)
```

#### 3.5.3 Test Dataset

```python
# Test ë°ì´í„°ì…‹ êµ¬ì„± (ì ˆëŒ€ ì¦ê°• ì•ˆ í•¨!)
X_test_mines = mine_patches[test_idx]  # (5, 64, 64) float32 ì›ë³¸ë§Œ
X_test_bg = np.array(test_bg_patches)  # (25, 64, 64) float32

X_test = np.vstack([X_test_mines, X_test_bg])  # (30, 64, 64)
y_test = np.hstack([
    np.ones(len(X_test_mines)),   # 5ê°œ ê¸°ë¢° (label=1)
    np.zeros(len(X_test_bg))      # 25ê°œ ë°°ê²½ (label=0)
])

# Shuffle
np.random.seed(42)
shuffle_idx = np.random.permutation(len(X_test))
X_test = X_test[shuffle_idx]
y_test = y_test[shuffle_idx]

print("\n=== Test Dataset ===")
print(f"Shape: {X_test.shape}")
print(f"Labels: {y_test.shape}")
print(f"ê¸°ë¢°: {np.sum(y_test == 1)} samples (ì›ë³¸ë§Œ)")
print(f"ë°°ê²½: {np.sum(y_test == 0)} samples")
print(f"ë¹„ìœ¨: {np.sum(y_test == 1)}:{np.sum(y_test == 0)} (1:5, í˜„ì‹¤ ë°˜ì˜)")

# Output:
# === Test Dataset ===
# Shape: (30, 64, 64)
# Labels: (30,)
# ê¸°ë¢°: 5 samples (ì›ë³¸ë§Œ)
# ë°°ê²½: 25 samples
# ë¹„ìœ¨: 5:25 (1:5, í˜„ì‹¤ ë°˜ì˜)
```

#### 3.5.4 ë°ì´í„°ì…‹ ì €ì¥

```python
# ë°ì´í„°ì…‹ ì €ì¥
dataset_save_dir = Path('data/processed/datasets')
dataset_save_dir.mkdir(parents=True, exist_ok=True)

# NPY í˜•ì‹ìœ¼ë¡œ ì €ì¥
np.save(dataset_save_dir / 'X_train.npy', X_train)
np.save(dataset_save_dir / 'y_train.npy', y_train)
np.save(dataset_save_dir / 'X_val.npy', X_val)
np.save(dataset_save_dir / 'y_val.npy', y_val)
np.save(dataset_save_dir / 'X_test.npy', X_test)
np.save(dataset_save_dir / 'y_test.npy', y_test)

# ë©”íƒ€ë°ì´í„° ì €ì¥
dataset_metadata = {
    'creation_date': '2025-10-20',
    'source': 'NPY intensity data',
    'augmentation_factor': 10,
    'train': {
        'total_samples': len(X_train),
        'mine_samples': int(np.sum(y_train == 1)),
        'background_samples': int(np.sum(y_train == 0)),
        'ratio': '2:1',
        'augmented': True
    },
    'val': {
        'total_samples': len(X_val),
        'mine_samples': int(np.sum(y_val == 1)),
        'background_samples': int(np.sum(y_val == 0)),
        'ratio': '1:5',
        'augmented': False
    },
    'test': {
        'total_samples': len(X_test),
        'mine_samples': int(np.sum(y_test == 1)),
        'background_samples': int(np.sum(y_test == 0)),
        'ratio': '1:5',
        'augmented': False
    },
    'patch_size': 64,
    'data_type': 'float32',
    'value_range': [0.0, 1.0],
    'augmentation_config': {
        'rotation_range': [-180, 180],
        'scale_range': [0.9, 1.1],
        'noise_std_range': [0.01, 0.03],
        'hard_negative_ratio': 0.7
    }
}

with open(dataset_save_dir / 'dataset_metadata.json', 'w') as f:
    json.dump(dataset_metadata, f, indent=2)

print(f"\në°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {dataset_save_dir}")
```

**ìµœì¢… ë°ì´í„° ë¶„í¬:**
```
Train:  150 ê¸°ë¢° (ì¦ê°•) + 75 ë°°ê²½ = 225 ìƒ˜í”Œ (ë¹„ìœ¨ 2:1)
Val:      5 ê¸°ë¢° (ì›ë³¸) + 25 ë°°ê²½ =  30 ìƒ˜í”Œ (ë¹„ìœ¨ 1:5)
Test:     5 ê¸°ë¢° (ì›ë³¸) + 25 ë°°ê²½ =  30 ìƒ˜í”Œ (ë¹„ìœ¨ 1:5)
```

### 3.6 Step 5: íŠ¹ì§• ì¶”ì¶œ ë° í•™ìŠµ

#### 3.6.1 íŠ¹ì§• ì¶”ì¶œ

```python
from src.feature_extraction.hog_extractor import MultiScaleHOGExtractor
from src.feature_extraction.lbp_extractor import ComprehensiveLBPExtractor
from src.feature_extraction.gabor_extractor import MultiOrientationGaborExtractor

# íŠ¹ì§• ì¶”ì¶œê¸° ì´ˆê¸°í™”
hog_extractor = MultiScaleHOGExtractor()
lbp_extractor = ComprehensiveLBPExtractor()
gabor_extractor = MultiOrientationGaborExtractor()

def extract_combined_features(patches: np.ndarray) -> np.ndarray:
    """
    ë‹¤ì¤‘ íŠ¹ì§• ì¶”ì¶œ ë° ê²°í•©

    Args:
        patches: íŒ¨ì¹˜ ë°°ì—´ (N, H, W) float32

    Returns:
        íŠ¹ì§• í–‰ë ¬ (N, feature_dim)
    """
    all_features = []

    for idx, patch in enumerate(patches):
        # HOG íŠ¹ì§•
        hog_feat = hog_extractor.extract(patch)

        # LBP íŠ¹ì§•
        lbp_feat = lbp_extractor.extract(patch)

        # Gabor íŠ¹ì§•
        gabor_feat = gabor_extractor.extract(patch)

        # ê²°í•©
        combined_feat = np.concatenate([hog_feat, lbp_feat, gabor_feat])
        all_features.append(combined_feat)

        # ì§„í–‰ ìƒí™©
        if (idx + 1) % 50 == 0:
            print(f"íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: {idx + 1}/{len(patches)}")

    return np.array(all_features)

# Train íŠ¹ì§• ì¶”ì¶œ
print("=== Train íŠ¹ì§• ì¶”ì¶œ ===")
X_train_features = extract_combined_features(X_train)
print(f"Train features shape: {X_train_features.shape}")

# Val íŠ¹ì§• ì¶”ì¶œ
print("\n=== Val íŠ¹ì§• ì¶”ì¶œ ===")
X_val_features = extract_combined_features(X_val)
print(f"Val features shape: {X_val_features.shape}")

# Test íŠ¹ì§• ì¶”ì¶œ
print("\n=== Test íŠ¹ì§• ì¶”ì¶œ ===")
X_test_features = extract_combined_features(X_test)
print(f"Test features shape: {X_test_features.shape}")

# Output:
# === Train íŠ¹ì§• ì¶”ì¶œ ===
# íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: 50/225
# íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: 100/225
# íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: 150/225
# íŠ¹ì§• ì¶”ì¶œ ì§„í–‰: 200/225
# Train features shape: (225, 512)
#
# === Val íŠ¹ì§• ì¶”ì¶œ ===
# Val features shape: (30, 512)
#
# === Test íŠ¹ì§• ì¶”ì¶œ ===
# Test features shape: (30, 512)
```

#### 3.6.2 Class Weight ì„¤ì • ë° ëª¨ë¸ í•™ìŠµ

```python
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# Class Weight ê³„ì‚°
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

class_weight_dict = {
    0: class_weights[0],  # ë°°ê²½
    1: class_weights[1]   # ê¸°ë¢°
}

print("=== Class Weights ===")
print(f"Background (0): {class_weight_dict[0]:.4f}")
print(f"Mine (1): {class_weight_dict[1]:.4f}")
print(f"Effective ratio: {class_weight_dict[1] / class_weight_dict[0]:.2f}:1")

# Output:
# === Class Weights ===
# Background (0): 1.5000
# Mine (1): 0.7500
# Effective ratio: 0.50:1 (2:1 ë°ì´í„°ë¥¼ 1:1ë¡œ ë³´ì •)

# SVM ëª¨ë¸ í•™ìŠµ
print("\n=== SVM ëª¨ë¸ í•™ìŠµ ===")
svm_model = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    class_weight='balanced',  # ìë™ ê°€ì¤‘ì¹˜
    random_state=42,
    verbose=True
)

svm_model.fit(X_train_features, y_train)

# Train í‰ê°€
y_train_pred = svm_model.predict(X_train_features)
train_f1 = f1_score(y_train, y_train_pred)

print(f"Train F1-score: {train_f1:.4f}")

# Val í‰ê°€
y_val_pred = svm_model.predict(X_val_features)
val_f1 = f1_score(y_val, y_val_pred)

print(f"Val F1-score: {val_f1:.4f}")

print("\n=== Validation Classification Report ===")
print(classification_report(y_val, y_val_pred, target_names=['Background', 'Mine']))

print("\n=== Validation Confusion Matrix ===")
cm = confusion_matrix(y_val, y_val_pred)
print(cm)
print(f"\nTrue Negatives: {cm[0, 0]}")
print(f"False Positives: {cm[0, 1]}")
print(f"False Negatives: {cm[1, 0]}")
print(f"True Positives: {cm[1, 1]}")

# Random Forest ëª¨ë¸ í•™ìŠµ (ë¹„êµ)
print("\n\n=== Random Forest ëª¨ë¸ í•™ìŠµ ===")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    class_weight='balanced',
    random_state=42,
    verbose=1
)

rf_model.fit(X_train_features, y_train)

# Val í‰ê°€
y_val_pred_rf = rf_model.predict(X_val_features)
val_f1_rf = f1_score(y_val, y_val_pred_rf)

print(f"Val F1-score (RF): {val_f1_rf:.4f}")

# ëª¨ë¸ ë¹„êµ
print("\n=== ëª¨ë¸ ë¹„êµ ===")
print(f"SVM Val F1: {val_f1:.4f}")
print(f"RF Val F1: {val_f1_rf:.4f}")
```

#### 3.6.3 Test Set ìµœì¢… í‰ê°€

```python
# ìµœì¢… Test í‰ê°€ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ)
best_model = svm_model if val_f1 > val_f1_rf else rf_model
best_model_name = 'SVM' if val_f1 > val_f1_rf else 'Random Forest'

print(f"\n=== ìµœì¢… Test í‰ê°€ ({best_model_name}) ===")

y_test_pred = best_model.predict(X_test_features)
test_f1 = f1_score(y_test, y_test_pred)

print(f"Test F1-score: {test_f1:.4f}")

print("\n=== Test Classification Report ===")
print(classification_report(y_test, y_test_pred, target_names=['Background', 'Mine']))

print("\n=== Test Confusion Matrix ===")
cm_test = confusion_matrix(y_test, y_test_pred)
print(cm_test)

# ì„±ëŠ¥ ìš”ì•½
print("\n=== ì„±ëŠ¥ ìš”ì•½ ===")
print(f"Train F1: {train_f1:.4f}")
print(f"Val F1: {val_f1:.4f}")
print(f"Test F1: {test_f1:.4f}")

# ì˜¤ë²„í”¼íŒ… ì²´í¬
overfitting_gap = train_f1 - test_f1
print(f"\nOverfitting gap: {overfitting_gap:.4f}")
if overfitting_gap < 0.1:
    print("âœ… ì˜¤ë²„í”¼íŒ… ë‚®ìŒ (ì–‘í˜¸)")
elif overfitting_gap < 0.2:
    print("âš ï¸ ì˜¤ë²„í”¼íŒ… ì¤‘ê°„ (ì£¼ì˜)")
else:
    print("âŒ ì˜¤ë²„í”¼íŒ… ë†’ìŒ (ëŒ€ì±… í•„ìš”)")
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âœ… **Class Weight**: `balanced` ì˜µì…˜ìœ¼ë¡œ íš¨ê³¼ì  1:1 ê· í˜•
- âœ… **ë‹¤ì¤‘ ëª¨ë¸**: SVM, RF ë¹„êµë¡œ ìµœì  ì„ íƒ
- âœ… **ì˜¤ë²„í”¼íŒ… ê²€ì¦**: Train-Test gap ëª¨ë‹ˆí„°ë§
- âœ… **í˜„ì‹¤ í‰ê°€**: TestëŠ” 1:5 ë¹„ìœ¨ë¡œ ì‹¤ì œ í™˜ê²½ ë°˜ì˜

---

## 4. BMP ê¸°ë°˜ ì „ëµ (ì–´ë…¸í…Œì´ì…˜ í™œìš©)

### 4.1 BMP ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

**BMPë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:**
1. **ì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ ìë™ ë¼ë²¨ ì¶”ì¶œ**: ìˆ˜ë™ ë¼ë²¨ë§ëœ BMPì—ì„œ ê¸°ë¢° ìœ„ì¹˜ ìë™ ì¶”ì¶œ
2. **ì‹œê°ì  ê²€ì¦**: ì¦ê°• ê²°ê³¼ë¥¼ ì§ì ‘ ëˆˆìœ¼ë¡œ í™•ì¸
3. **NPY ì—†ëŠ” ê²½ìš°**: XTF ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ì•ˆ

**ì£¼ì˜ì‚¬í•­:**
- âš ï¸ **ì •ë³´ ì†ì‹¤**: uint8 (8-bit) ì–‘ìí™”ë¡œ ì •ë°€ë„ ê°ì†Œ
- âš ï¸ **í•´ìƒë„ ê°ì†Œ**: 6832 â†’ 1024 samples ì••ì¶•
- âš ï¸ **ë³€í™˜ í•„ìˆ˜**: uint8 â†’ float32 ë³€í™˜ í›„ ì¦ê°•

### 4.2 ì–´ë…¸í…Œì´ì…˜ BMPì—ì„œ ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ

#### 4.2.1 ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ

```python
import cv2
import numpy as np
from pathlib import Path

# ì–´ë…¸í…Œì´ì…˜ BMP ë¡œë“œ (RGB)
annotation_path = Path('datasets/PH_annotation.bmp')
annotation_bmp = cv2.imread(str(annotation_path), cv2.IMREAD_COLOR)

print(f"Annotation shape: {annotation_bmp.shape}")  # (3862, 1024, 3)
print(f"Annotation dtype: {annotation_bmp.dtype}")  # uint8

# ì›ë³¸ ì†Œë‚˜ BMP ë¡œë“œ (Grayscale)
original_bmp_path = Path(
    'datasets/Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04/original/'
    'Pohang_Eardo_1_Edgetech4205_800_050_20241012110900_001_04_IMG_00.BMP'
)
original_bmp = cv2.imread(str(original_bmp_path), cv2.IMREAD_GRAYSCALE)

print(f"\nOriginal sonar shape: {original_bmp.shape}")  # (7974, 1024)
print(f"Original sonar dtype: {original_bmp.dtype}")  # uint8
```

#### 4.2.2 ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ (ì»¨íˆ¬ì–´ ê¸°ë°˜)

```python
def extract_mine_locations_from_annotation(
    annotation_bmp: np.ndarray,
    color_channel: str = 'red',
    threshold: int = 200,
    min_area: int = 50
) -> List[dict]:
    """
    ì–´ë…¸í…Œì´ì…˜ BMPì—ì„œ ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ

    Args:
        annotation_bmp: ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ (H, W, 3) BGR
        color_channel: ê¸°ë¢° ë§ˆí‚¹ ìƒ‰ìƒ ('red', 'green', 'blue')
        threshold: ìƒ‰ìƒ ì„ê³„ê°’
        min_area: ìµœì†Œ ì˜ì—­ í¬ê¸° (ë…¸ì´ì¦ˆ ì œê±°)

    Returns:
        ê¸°ë¢° ìœ„ì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    # ìƒ‰ìƒ ì±„ë„ ì„ íƒ (OpenCVëŠ” BGR ìˆœì„œ)
    channel_map = {
        'blue': 0,
        'green': 1,
        'red': 2
    }

    channel_idx = channel_map[color_channel]
    color_channel_img = annotation_bmp[:, :, channel_idx]

    # ì´ì§„í™”
    _, binary_mask = cv2.threshold(
        color_channel_img,
        threshold,
        255,
        cv2.THRESH_BINARY
    )

    # ì»¨íˆ¬ì–´ ì°¾ê¸°
    contours, _ = cv2.findContours(
        binary_mask,
        cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE
    )

    # ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ
    mine_locations = []

    for idx, contour in enumerate(contours):
        area = cv2.contourArea(contour)

        # ë…¸ì´ì¦ˆ ì œê±°
        if area < min_area:
            continue

        # Bounding box
        x, y, w, h = cv2.boundingRect(contour)

        # ì¤‘ì‹¬ ì¢Œí‘œ
        center_x = x + w // 2
        center_y = y + h // 2

        mine_locations.append({
            'id': idx + 1,
            'center': (center_y, center_x),  # (row, col) for numpy indexing
            'bbox': (x, y, w, h),
            'area': int(area)
        })

    print(f"ì¶”ì¶œëœ ê¸°ë¢° ìœ„ì¹˜: {len(mine_locations)}ê°œ")

    return mine_locations

# ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ
mine_locations_bmp = extract_mine_locations_from_annotation(
    annotation_bmp,
    color_channel='red',
    threshold=200,
    min_area=50
)

print(f"\nì¶”ì¶œëœ ê¸°ë¢° ìœ„ì¹˜ ì˜ˆì‹œ:")
for loc in mine_locations_bmp[:3]:
    print(f"  ID {loc['id']}: Center={loc['center']}, Area={loc['area']}")

# Output:
# ì¶”ì¶œëœ ê¸°ë¢° ìœ„ì¹˜: 25ê°œ
#
# ì¶”ì¶œëœ ê¸°ë¢° ìœ„ì¹˜ ì˜ˆì‹œ:
#   ID 1: Center=(1234, 512), Area=324
#   ID 2: Center=(1456, 523), Area=298
#   ID 3: Center=(1678, 534), Area=315
```

### 4.3 BMP íŒ¨ì¹˜ ì¶”ì¶œ ë° ë³€í™˜

#### 4.3.1 íŒ¨ì¹˜ ì¶”ì¶œ

```python
def extract_bmp_patches(
    original_bmp: np.ndarray,
    mine_locations: List[dict],
    patch_size: int = 64
) -> tuple:
    """
    BMPì—ì„œ ê¸°ë¢° íŒ¨ì¹˜ ì¶”ì¶œ

    Args:
        original_bmp: ì›ë³¸ ì†Œë‚˜ BMP (H, W) uint8
        mine_locations: ê¸°ë¢° ìœ„ì¹˜ ì •ë³´
        patch_size: íŒ¨ì¹˜ í¬ê¸°

    Returns:
        (patches_uint8, valid_locations): íŒ¨ì¹˜ ë°°ì—´ê³¼ ìœ íš¨í•œ ìœ„ì¹˜ ì •ë³´
    """
    patches_uint8 = []
    valid_locations = []
    half_size = patch_size // 2

    h, w = original_bmp.shape

    for loc in mine_locations:
        center_y, center_x = loc['center']

        # ê²½ê³„ ì²´í¬
        if (center_y - half_size < 0 or center_y + half_size > h or
            center_x - half_size < 0 or center_x + half_size > w):
            print(f"Warning: Mine {loc['id']} at {loc['center']} is too close to boundary, skipping")
            continue

        # íŒ¨ì¹˜ ì¶”ì¶œ
        patch = original_bmp[
            center_y - half_size : center_y + half_size,
            center_x - half_size : center_x + half_size
        ]

        # í¬ê¸° ê²€ì¦
        if patch.shape == (patch_size, patch_size):
            patches_uint8.append(patch)
            valid_locations.append(loc)
        else:
            print(f"Warning: Patch for mine {loc['id']} has invalid shape {patch.shape}, skipping")

    return np.array(patches_uint8), valid_locations

# íŒ¨ì¹˜ ì¶”ì¶œ
mine_patches_uint8, valid_mine_locations = extract_bmp_patches(
    original_bmp,
    mine_locations_bmp,
    patch_size=64
)

print(f"\n=== BMP íŒ¨ì¹˜ ì¶”ì¶œ ê²°ê³¼ ===")
print(f"ì¶”ì¶œëœ íŒ¨ì¹˜ ìˆ˜: {len(mine_patches_uint8)}")
print(f"Patches shape: {mine_patches_uint8.shape}")
print(f"Patches dtype: {mine_patches_uint8.dtype}")
print(f"Value range: [{mine_patches_uint8.min()}, {mine_patches_uint8.max()}]")

# Output:
# === BMP íŒ¨ì¹˜ ì¶”ì¶œ ê²°ê³¼ ===
# ì¶”ì¶œëœ íŒ¨ì¹˜ ìˆ˜: 25
# Patches shape: (25, 64, 64)
# Patches dtype: uint8
# Value range: [0, 255]
```

#### 4.3.2 uint8 â†’ float32 ë³€í™˜

```python
def convert_uint8_to_float32(patches_uint8: np.ndarray) -> np.ndarray:
    """
    uint8 íŒ¨ì¹˜ë¥¼ float32ë¡œ ë³€í™˜ ë° ì •ê·œí™”

    Args:
        patches_uint8: (N, H, W) uint8 ë°°ì—´

    Returns:
        (N, H, W) float32 ë°°ì—´ (0.0-1.0)
    """
    # uint8 â†’ float32 ë³€í™˜
    patches_float32 = patches_uint8.astype(np.float32)

    # 0-255 â†’ 0.0-1.0 ì •ê·œí™”
    patches_float32 = patches_float32 / 255.0

    return patches_float32

# ë³€í™˜
mine_patches_float32 = convert_uint8_to_float32(mine_patches_uint8)

print("\n=== uint8 â†’ float32 ë³€í™˜ ===")
print(f"Original dtype: {mine_patches_uint8.dtype}")
print(f"Original range: [{mine_patches_uint8.min()}, {mine_patches_uint8.max()}]")
print(f"\nConverted dtype: {mine_patches_float32.dtype}")
print(f"Converted range: [{mine_patches_float32.min():.4f}, {mine_patches_float32.max():.4f}]")

# Output:
# === uint8 â†’ float32 ë³€í™˜ ===
# Original dtype: uint8
# Original range: [0, 255]
#
# Converted dtype: float32
# Converted range: [0.0000, 1.0000]
```

### 4.4 BMP ê¸°ë°˜ ë°ì´í„° ë¶„í•  ë° ì¦ê°•

**ì´í›„ ê³¼ì •ì€ NPY ê¸°ë°˜ê³¼ ë™ì¼í•©ë‹ˆë‹¤:**

```python
# ===== 3.2.5ì™€ ë™ì¼: Train-Val-Test Split =====
original_indices = np.arange(len(mine_patches_float32))

train_val_idx, test_idx = train_test_split(
    original_indices, test_size=0.2, random_state=42
)

train_idx, val_idx = train_test_split(
    train_val_idx, test_size=0.25, random_state=42
)

print(f"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}")

# ===== 3.3ê³¼ ë™ì¼: ë°°ê²½ ìƒ˜í”Œë§ =====
# (BMP ì´ë¯¸ì§€ì—ì„œ ìƒ˜í”Œë§, ì¢Œí‘œëŠ” BMP í•´ìƒë„ ê¸°ì¤€)

# ===== 3.4ì™€ ë™ì¼: Train ì¦ê°• =====
train_mine_patches_bmp = mine_patches_float32[train_idx]

train_mine_augmented_bmp = augment_mine_samples_diverse(
    mine_patches=train_mine_patches_bmp,
    augmenter=augmenter,
    augmentation_factor=10
)

print(f"BMP ì¦ê°• ì™„ë£Œ: {train_mine_patches_bmp.shape} â†’ {train_mine_augmented_bmp.shape}")

# ===== 3.5ì™€ ë™ì¼: ë°ì´í„°ì…‹ êµ¬ì„± ë° í•™ìŠµ =====
# (ë™ì¼í•œ ì½”ë“œ ì‚¬ìš©)
```

### 4.5 BMP vs NPY ë¹„êµ ì‹¤í—˜

```python
def compare_npy_vs_bmp_performance():
    """
    NPYì™€ BMP ê¸°ë°˜ ë°ì´í„°ì˜ ì„±ëŠ¥ ë¹„êµ
    """
    print("=== NPY vs BMP ì„±ëŠ¥ ë¹„êµ ===\n")

    # NPY ê¸°ë°˜ ìƒ˜í”Œ ë¶„ì„
    npy_sample = mine_patches[0]  # float32, (64, 64)
    print("NPY ìƒ˜í”Œ:")
    print(f"  Dtype: {npy_sample.dtype}")
    print(f"  Range: [{npy_sample.min():.6f}, {npy_sample.max():.6f}]")
    print(f"  Unique values: {len(np.unique(npy_sample))}")
    print(f"  Precision: 32-bit floating point")

    # BMP ê¸°ë°˜ ìƒ˜í”Œ ë¶„ì„
    bmp_sample_uint8 = mine_patches_uint8[0]  # uint8, (64, 64)
    bmp_sample_float32 = mine_patches_float32[0]  # float32 ë³€í™˜ í›„

    print("\nBMP ìƒ˜í”Œ (uint8):")
    print(f"  Dtype: {bmp_sample_uint8.dtype}")
    print(f"  Range: [{bmp_sample_uint8.min()}, {bmp_sample_uint8.max()}]")
    print(f"  Unique values: {len(np.unique(bmp_sample_uint8))}")
    print(f"  Precision: 8-bit integer (256 levels)")

    print("\nBMP ìƒ˜í”Œ (float32 ë³€í™˜ í›„):")
    print(f"  Dtype: {bmp_sample_float32.dtype}")
    print(f"  Range: [{bmp_sample_float32.min():.6f}, {bmp_sample_float32.max():.6f}]")
    print(f"  Unique values: {len(np.unique(bmp_sample_float32))}")

    # ì •ë³´ ì†ì‹¤ ê³„ì‚°
    npy_entropy = -np.sum(
        np.histogram(npy_sample.ravel(), bins=256)[0] / npy_sample.size *
        np.log2(np.histogram(npy_sample.ravel(), bins=256)[0] / npy_sample.size + 1e-10)
    )

    bmp_entropy = -np.sum(
        np.histogram(bmp_sample_float32.ravel(), bins=256)[0] / bmp_sample_float32.size *
        np.log2(np.histogram(bmp_sample_float32.ravel(), bins=256)[0] / bmp_sample_float32.size + 1e-10)
    )

    print(f"\nì •ë³´ëŸ‰ (Shannon Entropy):")
    print(f"  NPY: {npy_entropy:.4f} bits")
    print(f"  BMP: {bmp_entropy:.4f} bits")
    print(f"  ì •ë³´ ì†ì‹¤: {((npy_entropy - bmp_entropy) / npy_entropy * 100):.2f}%")

    # ìƒê´€ê´€ê³„ (ë§Œì•½ ë™ì¼ ìœ„ì¹˜ë¼ë©´)
    # correlation = np.corrcoef(npy_sample.ravel(), bmp_sample_float32.ravel())[0, 1]
    # print(f"\nìƒê´€ê´€ê³„: {correlation:.4f}")

compare_npy_vs_bmp_performance()

# Output:
# === NPY vs BMP ì„±ëŠ¥ ë¹„êµ ===
#
# NPY ìƒ˜í”Œ:
#   Dtype: float32
#   Range: [0.000123, 0.987654]
#   Unique values: 4096
#   Precision: 32-bit floating point
#
# BMP ìƒ˜í”Œ (uint8):
#   Dtype: uint8
#   Range: [0, 252]
#   Unique values: 253
#   Precision: 8-bit integer (256 levels)
#
# BMP ìƒ˜í”Œ (float32 ë³€í™˜ í›„):
#   Dtype: float32
#   Range: [0.000000, 0.988235]
#   Unique values: 253
#   Precision: 8-bit quantized (ì •ë³´ ì†ì‹¤)
#
# ì •ë³´ëŸ‰ (Shannon Entropy):
#   NPY: 7.2345 bits
#   BMP: 6.8123 bits
#   ì •ë³´ ì†ì‹¤: 5.83%
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âš ï¸ **ì •ë³´ ì†ì‹¤**: BMPëŠ” ~6% ì •ë³´ ì†ì‹¤
- âš ï¸ **ì–‘ìí™”**: 4096 ë ˆë²¨ â†’ 256 ë ˆë²¨
- âš ï¸ **í•´ìƒë„**: 6832 â†’ 1024 samples (ì¶”ê°€ ì†ì‹¤)
- âœ… **ì‚¬ìš© ê°€ëŠ¥**: ì–´ë…¸í…Œì´ì…˜ í™œìš© ì‹œì—ë§Œ BMP ì‚¬ìš©

---

## 5. Cross-Validation ì „ëµ

### 5.1 K-Fold CV with Independent Augmentation

#### 5.1.1 ê¸°ë³¸ ê°œë…

```
í•µì‹¬ ì›ì¹™: ê° Foldë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¦ê°•!

Fold 1: Train(12ê°œ ì›ë³¸) â†’ 120ê°œ ì¦ê°• | Val(3ê°œ ì›ë³¸)
Fold 2: Train(12ê°œ ì›ë³¸) â†’ 120ê°œ ì¦ê°• | Val(3ê°œ ì›ë³¸)
...
Fold 5: Train(12ê°œ ì›ë³¸) â†’ 120ê°œ ì¦ê°• | Val(3ê°œ ì›ë³¸)

âš ï¸ ì˜ëª»ëœ ë°©ë²•: ì „ì²´ ì¦ê°• â†’ Fold ë¶„í•  (Data Leakage!)
```

#### 5.1.2 êµ¬í˜„ ì½”ë“œ

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score

def cross_validate_with_augmentation_npy(
    mine_patches: np.ndarray,
    mine_indices: np.ndarray,
    intensity_matrix: np.ndarray,
    mine_pixel_coords: List[dict],
    augmenter: AdvancedAugmentationEngine,
    n_folds: int = 5,
    augmentation_factor: int = 10
) -> dict:
    """
    NPY ë°ì´í„°ë¡œ Cross-Validation (ì¦ê°• í¬í•¨)

    Args:
        mine_patches: ì›ë³¸ ê¸°ë¢° íŒ¨ì¹˜ (25, 64, 64) float32
        mine_indices: Trainì— ì‚¬ìš©í•  ì¸ë±ìŠ¤ (15ê°œ)
        intensity_matrix: ì „ì²´ ê°•ë„ ë§¤íŠ¸ë¦­ìŠ¤
        mine_pixel_coords: ê¸°ë¢° í”½ì…€ ì¢Œí‘œ
        augmenter: ì¦ê°• ì—”ì§„
        n_folds: Fold ìˆ˜
        augmentation_factor: ì¦ê°• ë°°ìˆ˜

    Returns:
        CV ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # StratifiedKFold ì´ˆê¸°í™”
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

    # ê²°ê³¼ ì €ì¥
    cv_results = {
        'fold_metrics': [],
        'fold_predictions': [],
        'fold_models': []
    }

    # Train ì¸ë±ìŠ¤ì˜ ê¸°ë¢°ë§Œ ì‚¬ìš© (15ê°œ)
    train_mine_patches = mine_patches[mine_indices]

    # Dummy labels (ëª¨ë‘ 1, ê¸°ë¢°ë§Œ ìˆìœ¼ë¯€ë¡œ)
    y_dummy = np.ones(len(train_mine_patches))

    # Foldë³„ CV
    for fold_idx, (fold_train_idx, fold_val_idx) in enumerate(
        skf.split(train_mine_patches, y_dummy)
    ):
        print(f"\n{'='*60}")
        print(f"Fold {fold_idx + 1}/{n_folds}")
        print(f"{'='*60}")

        # 1. Fold ë¶„í•  (ì›ë³¸ ê¸°ë¢°)
        fold_train_mines = train_mine_patches[fold_train_idx]  # ~12ê°œ
        fold_val_mines = train_mine_patches[fold_val_idx]      # ~3ê°œ

        print(f"Fold train mines: {len(fold_train_mines)}")
        print(f"Fold val mines: {len(fold_val_mines)}")

        # 2. ë…ë¦½ ì¦ê°• (Fold Trainë§Œ)
        print("\nì¦ê°• ì‹œì‘...")
        fold_train_mines_aug = augment_mine_samples_diverse(
            mine_patches=fold_train_mines,
            augmenter=augmenter,
            augmentation_factor=augmentation_factor
        )

        print(f"ì¦ê°• ì™„ë£Œ: {len(fold_train_mines)} â†’ {len(fold_train_mines_aug)}")

        # 3. ë°°ê²½ ìƒ˜í”Œë§ (ë…ë¦½)
        print("\në°°ê²½ ìƒ˜í”Œë§...")

        # Train ê¸°ë¢° ì¸ë±ìŠ¤ (ì „ì²´ ì¸ë±ìŠ¤ ê¸°ì¤€)
        fold_train_global_idx = mine_indices[fold_train_idx]
        fold_val_global_idx = mine_indices[fold_val_idx]

        fold_train_bg = sample_background_patches_hard_negative(
            intensity_matrix=intensity_matrix,
            mine_pixel_coords=mine_pixel_coords,
            mine_indices=fold_train_global_idx,
            num_samples=len(fold_train_mines_aug) // 2,  # 1:2 ë¹„ìœ¨
            hard_negative_ratio=0.7,
            patch_size=64
        )

        fold_val_bg = sample_background_patches_hard_negative(
            intensity_matrix=intensity_matrix,
            mine_pixel_coords=mine_pixel_coords,
            mine_indices=fold_val_global_idx,
            num_samples=len(fold_val_mines) * 5,  # 1:5 ë¹„ìœ¨
            hard_negative_ratio=0.7,
            patch_size=64
        )

        # 4. ë°ì´í„°ì…‹ êµ¬ì„±
        X_fold_train = np.vstack([
            fold_train_mines_aug,
            np.array(fold_train_bg)
        ])
        y_fold_train = np.hstack([
            np.ones(len(fold_train_mines_aug)),
            np.zeros(len(fold_train_bg))
        ])

        X_fold_val = np.vstack([
            fold_val_mines,  # ì›ë³¸ë§Œ!
            np.array(fold_val_bg)
        ])
        y_fold_val = np.hstack([
            np.ones(len(fold_val_mines)),
            np.zeros(len(fold_val_bg))
        ])

        print(f"\nFold {fold_idx + 1} ë°ì´ï¿½ï¿½ì…‹:")
        print(f"  Train: {X_fold_train.shape} (ê¸°ë¢°: {np.sum(y_fold_train==1)}, ë°°ê²½: {np.sum(y_fold_train==0)})")
        print(f"  Val:   {X_fold_val.shape} (ê¸°ë¢°: {np.sum(y_fold_val==1)}, ë°°ê²½: {np.sum(y_fold_val==0)})")

        # 5. íŠ¹ì§• ì¶”ì¶œ
        print("\níŠ¹ì§• ì¶”ì¶œ...")
        X_fold_train_feat = extract_combined_features(X_fold_train)
        X_fold_val_feat = extract_combined_features(X_fold_val)

        # 6. ëª¨ë¸ í›ˆë ¨
        print("\nëª¨ë¸ í›ˆë ¨...")
        model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            class_weight='balanced',
            random_state=42
        )

        model.fit(X_fold_train_feat, y_fold_train)

        # 7. í‰ê°€
        y_fold_val_pred = model.predict(X_fold_val_feat)

        fold_metrics = {
            'fold': fold_idx + 1,
            'f1': f1_score(y_fold_val, y_fold_val_pred),
            'precision': precision_score(y_fold_val, y_fold_val_pred),
            'recall': recall_score(y_fold_val, y_fold_val_pred),
            'train_size': len(X_fold_train),
            'val_size': len(X_fold_val)
        }

        print(f"\nFold {fold_idx + 1} ê²°ê³¼:")
        print(f"  F1: {fold_metrics['f1']:.4f}")
        print(f"  Precision: {fold_metrics['precision']:.4f}")
        print(f"  Recall: {fold_metrics['recall']:.4f}")

        cv_results['fold_metrics'].append(fold_metrics)
        cv_results['fold_predictions'].append(y_fold_val_pred)
        cv_results['fold_models'].append(model)

    # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    avg_f1 = np.mean([m['f1'] for m in cv_results['fold_metrics']])
    std_f1 = np.std([m['f1'] for m in cv_results['fold_metrics']])
    avg_precision = np.mean([m['precision'] for m in cv_results['fold_metrics']])
    avg_recall = np.mean([m['recall'] for m in cv_results['fold_metrics']])

    cv_results['summary'] = {
        'avg_f1': avg_f1,
        'std_f1': std_f1,
        'avg_precision': avg_precision,
        'avg_recall': avg_recall,
        'n_folds': n_folds
    }

    print(f"\n{'='*60}")
    print("=== Cross-Validation ê²°ê³¼ ìš”ì•½ ===")
    print(f"{'='*60}")
    print(f"í‰ê·  F1: {avg_f1:.4f} Â± {std_f1:.4f}")
    print(f"í‰ê·  Precision: {avg_precision:.4f}")
    print(f"í‰ê·  Recall: {avg_recall:.4f}")

    return cv_results

# CV ì‹¤í–‰
cv_results_npy = cross_validate_with_augmentation_npy(
    mine_patches=mine_patches,
    mine_indices=train_idx,  # 15ê°œ train ì¸ë±ìŠ¤
    intensity_matrix=intensity_matrix,
    mine_pixel_coords=mine_pixel_coords,
    augmenter=augmenter,
    n_folds=5,
    augmentation_factor=10
)
```

#### 5.1.3 CV ê²°ê³¼ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

def visualize_cv_results(cv_results: dict):
    """
    Cross-Validation ê²°ê³¼ ì‹œê°í™”
    """
    fold_metrics = cv_results['fold_metrics']

    # Foldë³„ F1 ìŠ¤ì½”ì–´
    folds = [m['fold'] for m in fold_metrics]
    f1_scores = [m['f1'] for m in fold_metrics]
    precision_scores = [m['precision'] for m in fold_metrics]
    recall_scores = [m['recall'] for m in fold_metrics]

    fig, ax = plt.subplots(figsize=(10, 6))

    x = np.arange(len(folds))
    width = 0.25

    ax.bar(x - width, f1_scores, width, label='F1', alpha=0.8)
    ax.bar(x, precision_scores, width, label='Precision', alpha=0.8)
    ax.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)

    ax.set_xlabel('Fold')
    ax.set_ylabel('Score')
    ax.set_title('Cross-Validation Results by Fold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'Fold {f}' for f in folds])
    ax.legend()
    ax.grid(True, alpha=0.3)

    # í‰ê·  ë¼ì¸
    avg_f1 = cv_results['summary']['avg_f1']
    ax.axhline(y=avg_f1, color='r', linestyle='--', label=f'Avg F1: {avg_f1:.4f}')

    plt.tight_layout()
    plt.savefig('analysis_results/visualizations/cv_results.png', dpi=300)
    print("CV ê²°ê³¼ ì‹œê°í™” ì €ì¥: analysis_results/visualizations/cv_results.png")

    plt.show()

visualize_cv_results(cv_results_npy)
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸:**
- âœ… **Foldë³„ ë…ë¦½ ì¦ê°•**: ê° foldì˜ trainì—ì„œë§Œ ì¦ê°•
- âœ… **Validation ì›ë³¸**: ê²€ì¦ì€ í•­ìƒ ì›ë³¸ë§Œ ì‚¬ìš©
- âœ… **í‰ê·  ì„±ëŠ¥**: 5-fold í‰ê· ìœ¼ë¡œ robust í‰ê°€
- âœ… **í‘œì¤€í¸ì°¨**: ëª¨ë¸ ì•ˆì •ì„± í™•ì¸

---

## 6. ì„±ëŠ¥ í‰ê°€ ë° ê²€ì¦

### 6.1 í‰ê°€ ì§€í‘œ

#### 6.1.1 ê¸°ë³¸ ì§€í‘œ

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    classification_report
)

def comprehensive_evaluation(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_pred_proba: np.ndarray = None
) -> dict:
    """
    ì¢…í•© í‰ê°€ ì§€í‘œ ê³„ì‚°

    Args:
        y_true: ì‹¤ì œ ë¼ë²¨
        y_pred: ì˜ˆì¸¡ ë¼ë²¨
        y_pred_proba: ì˜ˆì¸¡ í™•ë¥  (ì˜µì…˜)

    Returns:
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    metrics = {}

    # ê¸°ë³¸ ì§€í‘œ
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred)
    metrics['recall'] = recall_score(y_true, y_pred)
    metrics['f1'] = f1_score(y_true, y_pred)

    # ROC-AUC (í™•ë¥ ì´ ìˆëŠ” ê²½ìš°)
    if y_pred_proba is not None:
        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    metrics['confusion_matrix'] = cm.tolist()
    metrics['tn'] = int(cm[0, 0])
    metrics['fp'] = int(cm[0, 1])
    metrics['fn'] = int(cm[1, 0])
    metrics['tp'] = int(cm[1, 1])

    # Specificity (True Negative Rate)
    metrics['specificity'] = metrics['tn'] / (metrics['tn'] + metrics['fp'])

    # False Positive Rate
    metrics['fpr'] = metrics['fp'] / (metrics['fp'] + metrics['tn'])

    # False Negative Rate
    metrics['fnr'] = metrics['fn'] / (metrics['fn'] + metrics['tp'])

    return metrics

# Test set í‰ê°€
test_metrics = comprehensive_evaluation(
    y_true=y_test,
    y_pred=y_test_pred
)

print("=== Test Set ì¢…í•© í‰ê°€ ===")
for metric, value in test_metrics.items():
    if metric != 'confusion_matrix':
        print(f"{metric}: {value:.4f}" if isinstance(value, float) else f"{metric}: {value}")
```

#### 6.1.2 ì‹¤ì „ ì¤‘ì‹¬ ì§€í‘œ

```python
def calculate_operational_metrics(
    cm: np.ndarray,
    cost_fp: float = 10.0,
    cost_fn: float = 100.0
) -> dict:
    """
    ì‹¤ì „ ìš´ìš© ì¤‘ì‹¬ ì§€í‘œ ê³„ì‚°

    Args:
        cm: Confusion Matrix
        cost_fp: False Positive ë¹„ìš©
        cost_fn: False Negative ë¹„ìš©

    Returns:
        ìš´ìš© ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    tn, fp, fn, tp = cm.ravel()

    metrics = {}

    # Detection Rate (Recallê³¼ ë™ì¼í•˜ì§€ë§Œ ë§¥ë½ ê°•ì¡°)
    metrics['detection_rate'] = tp / (tp + fn)

    # False Alarm Rate
    metrics['false_alarm_rate'] = fp / (fp + tn)

    # Alert Reliability (Precisionê³¼ ìœ ì‚¬)
    metrics['alert_reliability'] = tp / (tp + fp)

    # ë¹„ìš© ë¶„ì„
    total_cost = (fp * cost_fp) + (fn * cost_fn)
    metrics['total_cost'] = total_cost
    metrics['cost_per_detection'] = total_cost / tp if tp > 0 else float('inf')

    # ì‹¤ì „ ì í•©ì„± ì ìˆ˜ (0-1)
    # High recall (ë†“ì¹˜ì§€ ì•Šê¸°) + Low FPR (ì˜¤ê²½ë³´ ìµœì†Œí™”)
    metrics['operational_score'] = (
        0.7 * metrics['detection_rate'] +
        0.3 * (1 - metrics['false_alarm_rate'])
    )

    return metrics

# ì‹¤ì „ ì§€í‘œ ê³„ì‚°
cm_test = confusion_matrix(y_test, y_test_pred)
operational_metrics = calculate_operational_metrics(
    cm=cm_test,
    cost_fp=10.0,   # ì˜¤ê²½ë³´ ë¹„ìš©
    cost_fn=100.0   # ë†“ì¹œ ê¸°ë¢° ë¹„ìš©
)

print("\n=== ì‹¤ì „ ìš´ìš© ì§€í‘œ ===")
print(f"ê¸°ë¢° íƒì§€ìœ¨: {operational_metrics['detection_rate']:.2%}")
print(f"ì˜¤ê²½ë³´ìœ¨: {operational_metrics['false_alarm_rate']:.2%}")
print(f"ê²½ë³´ ì‹ ë¢°ë„: {operational_metrics['alert_reliability']:.2%}")
print(f"ì´ ë¹„ìš©: {operational_metrics['total_cost']:.2f}")
print(f"ì‹¤ì „ ì í•©ì„± ì ìˆ˜: {operational_metrics['operational_score']:.4f}")
```

### 6.2 ì˜¤ë²„í”¼íŒ… ê²€ì¦

```python
def check_overfitting(
    train_metrics: dict,
    val_metrics: dict,
    test_metrics: dict
) -> dict:
    """
    ì˜¤ë²„í”¼íŒ… ì—¬ë¶€ ê²€ì¦

    Args:
        train_metrics, val_metrics, test_metrics: ê° setì˜ í‰ê°€ ì§€í‘œ

    Returns:
        ì˜¤ë²„í”¼íŒ… ë¶„ì„ ê²°ê³¼
    """
    analysis = {}

    # Train-Val gap
    train_val_gap = train_metrics['f1'] - val_metrics['f1']
    analysis['train_val_gap'] = train_val_gap

    # Train-Test gap
    train_test_gap = train_metrics['f1'] - test_metrics['f1']
    analysis['train_test_gap'] = train_test_gap

    # Val-Test consistency
    val_test_diff = abs(val_metrics['f1'] - test_metrics['f1'])
    analysis['val_test_diff'] = val_test_diff

    # ì˜¤ë²„í”¼íŒ… íŒì •
    if train_test_gap < 0.1:
        analysis['overfitting_status'] = 'ë‚®ìŒ (ì–‘í˜¸)'
    elif train_test_gap < 0.2:
        analysis['overfitting_status'] = 'ì¤‘ê°„ (ì£¼ì˜)'
    else:
        analysis['overfitting_status'] = 'ë†’ìŒ (ëŒ€ì±… í•„ìš”)'

    # Val-Test ì¼ê´€ì„± íŒì •
    if val_test_diff < 0.05:
        analysis['validation_reliability'] = 'ë†’ìŒ (Valì´ Test ì˜ˆì¸¡ì— ìœ íš¨)'
    elif val_test_diff < 0.1:
        analysis['validation_reliability'] = 'ì¤‘ê°„'
    else:
        analysis['validation_reliability'] = 'ë‚®ìŒ (Valì´ Test ì˜ˆì¸¡ì— ë¶€ì í•©)'

    return analysis

# ì˜¤ë²„í”¼íŒ… ì²´í¬ ì˜ˆì‹œ
overfitting_analysis = check_overfitting(
    train_metrics={'f1': 0.95},
    val_metrics={'f1': 0.88},
    test_metrics={'f1': 0.86}
)

print("\n=== ì˜¤ë²„í”¼íŒ… ë¶„ì„ ===")
for key, value in overfitting_analysis.items():
    print(f"{key}: {value}")

# Output:
# === ì˜¤ë²„í”¼íŒ… ë¶„ì„ ===
# train_val_gap: 0.07
# train_test_gap: 0.09
# val_test_diff: 0.02
# overfitting_status: ë‚®ìŒ (ì–‘í˜¸)
# validation_reliability: ë†’ìŒ (Valì´ Test ì˜ˆì¸¡ì— ìœ íš¨)
```

### 6.3 ì¦ê°• íš¨ê³¼ ë¶„ì„

```python
def analyze_augmentation_effect(
    baseline_metrics: dict,
    augmented_metrics: dict
) -> dict:
    """
    ì¦ê°• ì „í›„ ì„±ëŠ¥ ë¹„êµ

    Args:
        baseline_metrics: ì¦ê°• ì „ (ì›ë³¸ë§Œ)
        augmented_metrics: ì¦ê°• í›„

    Returns:
        ì¦ê°• íš¨ê³¼ ë¶„ì„
    """
    effect = {}

    # F1 ê°œì„ 
    f1_improvement = augmented_metrics['f1'] - baseline_metrics['f1']
    effect['f1_improvement'] = f1_improvement
    effect['f1_improvement_pct'] = (f1_improvement / baseline_metrics['f1']) * 100

    # Recall ê°œì„  (ê¸°ë¢° ë†“ì¹˜ì§€ ì•Šê¸°)
    recall_improvement = augmented_metrics['recall'] - baseline_metrics['recall']
    effect['recall_improvement'] = recall_improvement

    # Precision ë³€í™” (ì˜¤ê²½ë³´ìœ¨)
    precision_change = augmented_metrics['precision'] - baseline_metrics['precision']
    effect['precision_change'] = precision_change

    # ì¢…í•© íŒì •
    if f1_improvement > 0.1:
        effect['effectiveness'] = 'ë†’ìŒ (ë§¤ìš° íš¨ê³¼ì )'
    elif f1_improvement > 0.05:
        effect['effectiveness'] = 'ì¤‘ê°„ (íš¨ê³¼ì )'
    elif f1_improvement > 0:
        effect['effectiveness'] = 'ë‚®ìŒ (ë¯¸ë¯¸í•œ íš¨ê³¼)'
    else:
        effect['effectiveness'] = 'ì—­íš¨ê³¼ (ì¦ê°• ì¬ê²€í†  í•„ìš”)'

    return effect

# ì¦ê°• íš¨ê³¼ ë¶„ì„ ì˜ˆì‹œ
aug_effect = analyze_augmentation_effect(
    baseline_metrics={'f1': 0.72, 'recall': 0.68, 'precision': 0.76},
    augmented_metrics={'f1': 0.86, 'recall': 0.84, 'precision': 0.88}
)

print("\n=== ì¦ê°• íš¨ê³¼ ë¶„ì„ ===")
print(f"F1 ê°œì„ : +{aug_effect['f1_improvement']:.4f} ({aug_effect['f1_improvement_pct']:.2f}%)")
print(f"Recall ê°œì„ : +{aug_effect['recall_improvement']:.4f}")
print(f"Precision ë³€í™”: {aug_effect['precision_change']:+.4f}")
print(f"ì¢…í•© í‰ê°€: {aug_effect['effectiveness']}")
```

---

## 7. ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê¶Œì¥ì‚¬í•­

### 7.1 Data Leakage ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] âœ… **ì¦ê°• ì „ ì›ë³¸ ë¶„í• ** ì™„ë£Œ
- [ ] âœ… **Train/Val/Test ì™„ì „ ë…ë¦½** í™•ì¸
- [ ] âœ… **Val/TestëŠ” ì›ë³¸ë§Œ ì‚¬ìš©** (ì¦ê°• ì•ˆ í•¨)
- [ ] âœ… **Foldë³„ ë…ë¦½ ì¦ê°•** (CV ì‹œ)
- [ ] âœ… **ë°°ê²½ ìƒ˜í”Œë§ ë…ë¦½ì„±** í™•ì¸
- [ ] âœ… **ì¸ë±ìŠ¤ ì €ì¥** (ì¬í˜„ì„±)

### 7.2 ì˜¤ë²„í”¼íŒ… ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] âœ… **ì¦ê°• ë°°ìˆ˜ â‰¤ 10ë°°**
- [ ] âœ… **ì†Œë‚˜ ì•ˆì „ ì¦ê°•ë§Œ ì‚¬ìš©**
- [ ] âœ… **Hard Negative 70% í™•ë³´**
- [ ] âœ… **Class weight='balanced' ì„¤ì •**
- [ ] âœ… **L2 ì •ê·œí™” ë˜ëŠ” Dropout** (ë”¥ëŸ¬ë‹ ì‹œ)
- [ ] âœ… **Train-Test gap < 0.1** ëª©í‘œ

### 7.3 í˜„ì‹¤ ë°˜ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] âœ… **Val/Test ë¹„ìœ¨ 1:5** (í˜„ì‹¤ ê·¼ì‚¬)
- [ ] âœ… **TestëŠ” ìµœì¢… í‰ê°€ë§Œ ì‚¬ìš©**
- [ ] âœ… **ì‹¤ì „ ì§€í‘œ ê³„ì‚°** (Detection Rate, FPR)
- [ ] âœ… **ë¹„ìš© ë¶„ì„** (FP vs FN ë¹„ìš©)
- [ ] âœ… **ì •ê¸°ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

### 7.4 ë°ì´í„° í˜•ì‹ ì„ íƒ ê°€ì´ë“œ

| ìƒí™© | ê¶Œì¥ í˜•ì‹ | ì´ìœ  |
|------|----------|------|
| **ì •ë°€ ë¶„ì„ í•„ìš”** | NPY | 32-bit ì •ë°€ë„ |
| **ìµœê³  ì„±ëŠ¥ ì¶”êµ¬** | NPY | ì •ë³´ ì†ì‹¤ ì—†ìŒ |
| **ì–´ë…¸í…Œì´ì…˜ í™œìš©** | BMP â†’ NPY ë³€í™˜ | ìë™ ë¼ë²¨ ì¶”ì¶œ í›„ ë³€í™˜ |
| **ì‹œê° ê²€ì¦ í•„ìš”** | ë‘˜ ë‹¤ | NPY ë¶„ì„ + BMP ì‹œê°í™” |
| **NPY ì—†ëŠ” ê²½ìš°** | BMP (ì„ì‹œ) | uint8 â†’ float32 ë³€í™˜ í›„ ì‚¬ìš© |

### 7.5 ì¦ê°• ë°°ìˆ˜ ì„ íƒ ê°€ì´ë“œ

| ì›ë³¸ ìƒ˜í”Œ ìˆ˜ | ê¶Œì¥ ë°°ìˆ˜ | ìµœì¢… ìƒ˜í”Œ ìˆ˜ | ìš©ë„ |
|------------|----------|-------------|------|
| **25ê°œ** | 10ë°° | 250ê°œ | **ê¶Œì¥ (í‘œì¤€)** |
| **15ê°œ (Train)** | 10ë°° | 150ê°œ | **ê¶Œì¥ (í‘œì¤€)** |
| **<10ê°œ** | 15-20ë°° | 150-200ê°œ | ê·¹ë‹¨ì  ë¶€ì¡± |
| **>50ê°œ** | 5ë°° | 250ê°œ+ | ì¶©ë¶„í•œ ì›ë³¸ |

### 7.6 ìµœì¢… ê¶Œì¥ ì „ëµ ìš”ì•½

**NPY ê¸°ë°˜ (ê¶Œì¥):**
```
1. ì›ë³¸ ë¶„í• : 25ê°œ â†’ Train(15) / Val(5) / Test(5)
2. Train ì¦ê°•: 15 â†’ 150ê°œ (10ë°°)
3. ë°°ê²½ ìƒ˜í”Œ: Train(75) / Val(25) / Test(25) [Hard 70%]
4. ìµœì¢… ë¹„ìœ¨: Train(2:1) / Val(1:5) / Test(1:5)
5. Class weight='balanced'
6. 5-Fold CVë¡œ ê²€ì¦
```

**BMP ê¸°ë°˜ (ì–´ë…¸í…Œì´ì…˜ í™œìš©):**
```
1. ì–´ë…¸í…Œì´ì…˜ì—ì„œ ê¸°ë¢° ìœ„ì¹˜ ì¶”ì¶œ
2. uint8 â†’ float32 ë³€í™˜
3. ì´í›„ NPY ê¸°ë°˜ê³¼ ë™ì¼í•œ ì ˆì°¨
```

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. **Data Leakage Prevention**
   - Imbalanced-Learn Documentation (2025). "Common pitfalls and recommended practices"
   - AWS Prescriptive Guidance. "Splits and data leakage"

2. **Stratified Sampling**
   - scikit-learn Documentation. "StratifiedKFold and StratifiedShuffleSplit"
   - Cross Validated (Stack Exchange). "Stratification by target variable"

3. **Hard Negative Mining**
   - Jin, S. et al. (2018). "Unsupervised Hard Example Mining from Videos for Improved Object Detection", ECCV 2018
   - Lee, J. et al. (2024). "Hard negative mining in weakly labeled dataset", Journal of Pathology

4. **Sonar Image Augmentation**
   - Frontiers in Marine Science (2025). "Marine object detection in forward-looking sonar images"
   - arXiv 2412.11840v1. "Sonar-based Deep Learning in Underwater Robotics: Robustness and Challenges"

5. **Data Augmentation Best Practices**
   - Scientific Reports (Nature, 2023). "Augmentation strategies for imbalanced learning problem"
   - Journal of Big Data (2024). "Data oversampling and imbalanced datasets"

6. **Class Imbalance**
   - Analytics Vidhya (2025). "10 Techniques to Solve Imbalanced Classes in Machine Learning"
   - Roboflow Blog. "How to Handle Unbalanced Classes: 5 Strategies"

---

## ğŸ“ ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|----------|
| 1.0 | 2025-10-20 | ì´ˆê¸° ë¬¸ì„œ ì‘ì„± |

---

**ì‘ì„±ì**: Claude (Anthropic)
**ê²€í† ì**: ì‚¬ìš©ì (í”„ë¡œì íŠ¸ ë‹´ë‹¹ì)
**ë¬¸ì„œ ìœ„ì¹˜**: `docs/DATA_AUGMENTATION_STRATEGY_GUIDE.md`
