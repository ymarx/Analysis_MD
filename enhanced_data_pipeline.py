#!/usr/bin/env python3
"""
í–¥ìƒëœ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

datasets í´ë”ì˜ ì‹¤ì œ/ëª¨ì˜ ë°ì´í„°ì—ì„œ intensity dataë¥¼ ì¶”ì¶œí•˜ê³ 
ê¸°ë¬¼ ìœ„ì¹˜ ë§¤í•‘ ë° íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import sys
import numpy as np
import pandas as pd
import cv2
from pathlib import Path
import json
import logging
from datetime import datetime
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from typing import List, Tuple, Dict, Optional, Union
import warnings
import pyxtf
from openpyxl import load_workbook

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
from config.paths import path_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EnhancedDataPipeline:
    """í–¥ìƒëœ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.datasets_path = path_manager.datasets
        self.output_path = path_manager.processed_data
        self.figures_path = path_manager.figures
        
        # ë°ì´í„° êµ¬ì¡°ì²´
        self.gps_data = None
        self.annotation_image = None
        self.object_locations = []
        self.intensity_data = {}
        
        logger.info("í–¥ìƒëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_location_data(self) -> bool:
        """Location_MDGPS.xlsx íŒŒì¼ì—ì„œ ê¸°ë¬¼ ìœ„ì¹˜ ì •ë³´ ë¡œë“œ"""
        gps_file = self.datasets_path / 'Location_MDGPS.xlsx'
        
        try:
            # ì—‘ì…€ íŒŒì¼ ë¡œë“œ (openpyxl ì‚¬ìš©)
            workbook = load_workbook(gps_file)
            sheet = workbook.active
            
            # ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
            data = []
            headers = [cell.value for cell in sheet[1]]  # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ
            
            for row in sheet.iter_rows(min_row=2, values_only=True):
                if any(row):  # ë¹ˆ í–‰ ì œì™¸
                    data.append(list(row))
            
            # DataFrame ìƒì„±
            self.gps_data = pd.DataFrame(data, columns=headers)
            
            logger.info(f"GPS ìœ„ì¹˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.gps_data)}ê°œ ìœ„ì¹˜")
            logger.info(f"ì»¬ëŸ¼: {list(self.gps_data.columns)}")
            
            return True
            
        except Exception as e:
            logger.error(f"GPS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def load_annotation_image(self) -> bool:
        """PH_annotation.png íŒŒì¼ì—ì„œ ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ"""
        annotation_file = self.datasets_path / 'PH_annotation.png'
        
        try:
            # OpenCVë¡œ ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(str(annotation_file))
            if image is not None:
                self.annotation_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                logger.info(f"ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {self.annotation_image.shape}")
                return True
            else:
                logger.error("ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def extract_object_locations_from_annotation(self) -> List[Dict]:
        """ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ì—ì„œ ë¹¨ê°„ ë°•ìŠ¤ ìœ„ì¹˜ ì¶”ì¶œ"""
        if self.annotation_image is None:
            logger.warning("ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # BGRë¡œ ë³€í™˜ (OpenCVì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
            image_bgr = cv2.cvtColor(self.annotation_image, cv2.COLOR_RGB2BGR)
            
            # HSV ë³€í™˜í•˜ì—¬ ë¹¨ê°„ìƒ‰ ì˜ì—­ ì°¾ê¸°
            hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)
            
            # ë¹¨ê°„ìƒ‰ ë²”ìœ„ ì •ì˜ (HSV)
            lower_red1 = np.array([0, 50, 50])
            upper_red1 = np.array([10, 255, 255])
            lower_red2 = np.array([170, 50, 50])
            upper_red2 = np.array([180, 255, 255])
            
            # ë¹¨ê°„ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
            mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
            mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
            mask = mask1 + mask2
            
            # ì»¨íˆ¬ì–´ ì°¾ê¸°
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            objects = []
            for i, contour in enumerate(contours):
                # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
                x, y, w, h = cv2.boundingRect(contour)
                
                # ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸
                if w > 5 and h > 5:
                    objects.append({
                        'id': i + 1,
                        'x': int(x),
                        'y': int(y),
                        'width': int(w),
                        'height': int(h),
                        'center_x': int(x + w // 2),
                        'center_y': int(y + h // 2),
                        'area': int(w * h)
                    })
            
            # Y ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
            objects.sort(key=lambda obj: obj['center_y'])
            
            # ID ì¬ë¶€ì—¬
            for i, obj in enumerate(objects):
                obj['id'] = i + 1
            
            self.object_locations = objects
            logger.info(f"ì–´ë…¸í…Œì´ì…˜ì—ì„œ {len(objects)}ê°œì˜ ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ ì™„ë£Œ")
            
            return objects
            
        except Exception as e:
            logger.error(f"ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def list_available_datasets(self) -> Dict[str, Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°ì‚¬"""
        datasets = {}
        
        for dataset_dir in self.datasets_path.iterdir():
            if dataset_dir.is_dir() and 'Pohang' in dataset_dir.name:
                dataset_info = {
                    'name': dataset_dir.name,
                    'path': dataset_dir,
                    'original_files': [],
                    'simulation_files': []
                }
                
                # Original ë°ì´í„° ì°¾ê¸°
                original_path = dataset_dir / 'original'
                if original_path.exists():
                    for xtf_file in original_path.glob('*.xtf'):
                        dataset_info['original_files'].append(xtf_file)
                
                # Simulation ë°ì´í„° ì°¾ê¸°
                simulation_path = dataset_dir / 'simulation'
                if simulation_path.exists():
                    for sim_dir in simulation_path.iterdir():
                        if sim_dir.is_dir():
                            for xtf_file in sim_dir.glob('*.xtf'):
                                dataset_info['simulation_files'].append(xtf_file)
                
                datasets[dataset_dir.name] = dataset_info
                
        logger.info(f"ë°œê²¬ëœ ë°ì´í„°ì…‹: {len(datasets)}ê°œ")
        for name, info in datasets.items():
            logger.info(f"- {name}: Original {len(info['original_files'])}ê°œ, Simulation {len(info['simulation_files'])}ê°œ")
        
        return datasets

    def extract_intensity_from_xtf(self, xtf_path: Path, max_pings: int = 1000) -> Optional[Dict]:
        """XTF íŒŒì¼ì—ì„œ intensity ë°ì´í„° ì¶”ì¶œ"""
        try:
            logger.info(f"XTF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {xtf_path.name}")
            
            # pyxtfë¡œ íŒŒì¼ ì½ê¸° (verbose ì¸ì ì œê±°)
            try:
                file_header, packets = pyxtf.xtf_read(str(xtf_path))
            except TypeError:
                # ì´ì „ ë²„ì „ì˜ pyxtfì—ì„œ verbose ì¸ìê°€ ìˆëŠ” ê²½ìš°
                file_header, packets = pyxtf.xtf_read(str(xtf_path), verbose=False)
            
            if not packets:
                logger.warning(f"íŒ¨í‚·ì´ ì—†ëŠ” íŒŒì¼: {xtf_path.name}")
                return None
            
            # ì†Œë‚˜ íŒ¨í‚· í•„í„°ë§
            sonar_packets = [p for p in packets if hasattr(p, 'data')]
            
            if not sonar_packets:
                logger.warning(f"ì†Œë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” íŒŒì¼: {xtf_path.name}")
                return None
            
            # ìµœëŒ€ ping ìˆ˜ ì œí•œ
            if len(sonar_packets) > max_pings:
                sonar_packets = sonar_packets[:max_pings]
                logger.info(f"íŒ¨í‚· ìˆ˜ ì œí•œ: {max_pings}ê°œë¡œ ì œí•œë¨")
            
            # intensity ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
            intensity_data = []
            ping_info = []
            
            for i, packet in enumerate(sonar_packets):
                if hasattr(packet, 'data') and packet.data is not None:
                    # ë°ì´í„°ë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(packet.data, np.ndarray):
                        intensity_row = packet.data.flatten()
                    else:
                        intensity_row = np.array(packet.data).flatten()
                    
                    intensity_data.append(intensity_row)
                    
                    # ping ì •ë³´ ìˆ˜ì§‘
                    ping_info.append({
                        'ping_number': i,
                        'timestamp': getattr(packet, 'timestamp', None),
                        'latitude': getattr(packet, 'SensorYcoordinate', 0),
                        'longitude': getattr(packet, 'SensorXcoordinate', 0),
                        'samples': len(intensity_row)
                    })
            
            if not intensity_data:
                logger.warning(f"ìœ íš¨í•œ intensity ë°ì´í„°ê°€ ì—†ëŠ” íŒŒì¼: {xtf_path.name}")
                return None
            
            # ëª¨ë“  í–‰ì˜ ê¸¸ì´ë¥¼ í†µì¼ (ìµœëŒ€ ê¸¸ì´ë¡œ ë§ì¶¤)
            max_samples = max(len(row) for row in intensity_data)
            normalized_data = []
            
            for row in intensity_data:
                if len(row) < max_samples:
                    # ë¶€ì¡±í•œ ë¶€ë¶„ì„ 0ìœ¼ë¡œ íŒ¨ë”©
                    padded_row = np.zeros(max_samples)
                    padded_row[:len(row)] = row
                    normalized_data.append(padded_row)
                else:
                    normalized_data.append(row[:max_samples])
            
            intensity_matrix = np.array(normalized_data)
            
            result = {
                'filename': xtf_path.name,
                'filepath': str(xtf_path),
                'intensity_matrix': intensity_matrix,
                'ping_info': ping_info,
                'shape': intensity_matrix.shape,
                'data_type': intensity_matrix.dtype,
                'processed_time': datetime.now()
            }
            
            logger.info(f"Intensity ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {intensity_matrix.shape}")
            return result
            
        except Exception as e:
            logger.error(f"XTF ì²˜ë¦¬ ì‹¤íŒ¨ {xtf_path.name}: {e}")
            return None

    def process_all_datasets(self) -> bool:
        """ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ intensity ë°ì´í„° ì¶”ì¶œ"""
        datasets = self.list_available_datasets()
        
        if not datasets:
            logger.error("ì²˜ë¦¬í•  ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        all_results = {}
        
        for dataset_name, dataset_info in datasets.items():
            logger.info(f"ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘: {dataset_name}")
            
            dataset_results = {
                'name': dataset_name,
                'original_data': {},
                'simulation_data': {},
                'processing_time': datetime.now()
            }
            
            # Original ë°ì´í„° ì²˜ë¦¬
            for xtf_file in dataset_info['original_files']:
                result = self.extract_intensity_from_xtf(xtf_file)
                if result:
                    key = xtf_file.name
                    dataset_results['original_data'][key] = result
            
            # Simulation ë°ì´í„° ì²˜ë¦¬
            for xtf_file in dataset_info['simulation_files']:
                result = self.extract_intensity_from_xtf(xtf_file)
                if result:
                    key = xtf_file.name
                    dataset_results['simulation_data'][key] = result
            
            all_results[dataset_name] = dataset_results
            
            logger.info(f"ë°ì´í„°ì…‹ {dataset_name} ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"- Original: {len(dataset_results['original_data'])}ê°œ íŒŒì¼")
            logger.info(f"- Simulation: {len(dataset_results['simulation_data'])}ê°œ íŒŒì¼")
        
        self.intensity_data = all_results
        return True

    def save_intensity_data(self) -> bool:
        """ì¶”ì¶œëœ intensity ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            save_dir = self.output_path / 'intensity_data'
            save_dir.mkdir(exist_ok=True)
            
            for dataset_name, dataset_data in self.intensity_data.items():
                dataset_dir = save_dir / dataset_name
                dataset_dir.mkdir(exist_ok=True)
                
                # Original ë°ì´í„° ì €ì¥
                original_dir = dataset_dir / 'original'
                original_dir.mkdir(exist_ok=True)
                
                for filename, data in dataset_data['original_data'].items():
                    save_path = original_dir / f"{Path(filename).stem}_intensity.npz"
                    np.savez_compressed(
                        save_path,
                        intensity_matrix=data['intensity_matrix'],
                        ping_info=data['ping_info'],
                        metadata={
                            'filename': data['filename'],
                            'shape': data['shape'],
                            'processed_time': str(data['processed_time'])
                        }
                    )
                
                # Simulation ë°ì´í„° ì €ì¥
                simulation_dir = dataset_dir / 'simulation'
                simulation_dir.mkdir(exist_ok=True)
                
                for filename, data in dataset_data['simulation_data'].items():
                    save_path = simulation_dir / f"{Path(filename).stem}_intensity.npz"
                    np.savez_compressed(
                        save_path,
                        intensity_matrix=data['intensity_matrix'],
                        ping_info=data['ping_info'],
                        metadata={
                            'filename': data['filename'],
                            'shape': data['shape'],
                            'processed_time': str(data['processed_time'])
                        }
                    )
            
            logger.info(f"Intensity ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def generate_intensity_images(self, max_files_per_dataset: int = 2) -> bool:
        """Intensity ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
        try:
            images_dir = self.figures_path / 'intensity_images'
            images_dir.mkdir(exist_ok=True)
            
            for dataset_name, dataset_data in self.intensity_data.items():
                dataset_images_dir = images_dir / dataset_name
                dataset_images_dir.mkdir(exist_ok=True)
                
                # Original ë°ì´í„° ì´ë¯¸ì§€í™”
                original_files = list(dataset_data['original_data'].items())[:max_files_per_dataset]
                for filename, data in original_files:
                    self._create_intensity_image(
                        data['intensity_matrix'], 
                        dataset_images_dir / f"original_{Path(filename).stem}.png",
                        f"Original - {filename}"
                    )
                
                # Simulation ë°ì´í„° ì´ë¯¸ì§€í™”
                simulation_files = list(dataset_data['simulation_data'].items())[:max_files_per_dataset]
                for filename, data in simulation_files:
                    self._create_intensity_image(
                        data['intensity_matrix'], 
                        dataset_images_dir / f"simulation_{Path(filename).stem}.png",
                        f"Simulation - {filename}"
                    )
            
            logger.info(f"Intensity ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {images_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def _create_intensity_image(self, intensity_matrix: np.ndarray, save_path: Path, title: str):
        """Intensity ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        plt.figure(figsize=(12, 8))
        
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©
        log_intensity = np.log1p(np.abs(intensity_matrix))
        
        plt.imshow(log_intensity, cmap='hot', aspect='auto')
        plt.title(f'{title}\nShape: {intensity_matrix.shape}', fontsize=10)
        plt.xlabel('Sample Index')
        plt.ylabel('Ping Index')
        plt.colorbar(label='Log Intensity')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

    def create_object_location_overlay(self) -> bool:
        """ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ì— ì¶”ì¶œëœ ê°ì²´ ìœ„ì¹˜ë¥¼ ì˜¤ë²„ë ˆì´"""
        if self.annotation_image is None or not self.object_locations:
            logger.warning("ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë˜ëŠ” ê°ì²´ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        try:
            overlay_dir = self.figures_path / 'object_locations'
            overlay_dir.mkdir(exist_ok=True)
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬
            image_with_overlay = self.annotation_image.copy()
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))
            
            # ì›ë³¸ ì´ë¯¸ì§€
            ax1.imshow(self.annotation_image)
            ax1.set_title('Original Annotation Image', fontsize=14)
            ax1.axis('off')
            
            # ê°ì²´ ìœ„ì¹˜ í‘œì‹œëœ ì´ë¯¸ì§€
            ax2.imshow(image_with_overlay)
            ax2.set_title(f'Detected Objects: {len(self.object_locations)}', fontsize=14)
            
            # ê°ì§€ëœ ê°ì²´ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            for obj in self.object_locations:
                rect = patches.Rectangle(
                    (obj['x'], obj['y']), 
                    obj['width'], 
                    obj['height'],
                    linewidth=2, 
                    edgecolor='lime', 
                    facecolor='none'
                )
                ax2.add_patch(rect)
                
                # ID í‘œì‹œ
                ax2.text(
                    obj['center_x'], 
                    obj['y'] - 5,
                    f"ID:{obj['id']}", 
                    color='yellow',
                    fontsize=10,
                    ha='center',
                    fontweight='bold'
                )
            
            ax2.axis('off')
            
            plt.tight_layout()
            plt.savefig(overlay_dir / 'object_detection_overlay.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            # ê°ì²´ ì •ë³´ JSONìœ¼ë¡œ ì €ì¥
            with open(overlay_dir / 'detected_objects.json', 'w') as f:
                json.dump(self.object_locations, f, indent=2, default=str)
            
            logger.info(f"ê°ì²´ ìœ„ì¹˜ ì˜¤ë²„ë ˆì´ ìƒì„± ì™„ë£Œ: {overlay_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def run_complete_pipeline(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=== í–¥ìƒëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ ===")
        
        try:
            # 1. ê¸°ë³¸ ë°ì´í„° ë¡œë“œ
            logger.info("1. ê¸°ë³¸ ë°ì´í„° ë¡œë“œ")
            if not self.load_location_data():
                logger.error("GPS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                return False
                
            if not self.load_annotation_image():
                logger.error("ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            # 2. ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ
            logger.info("2. ì–´ë…¸í…Œì´ì…˜ì—ì„œ ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ")
            self.extract_object_locations_from_annotation()
            
            # 3. ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ intensity ë°ì´í„° ì¶”ì¶œ
            logger.info("3. ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ intensity ë°ì´í„° ì¶”ì¶œ")
            if not self.process_all_datasets():
                logger.error("ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
            
            # 4. ë°ì´í„° ì €ì¥
            logger.info("4. ì¶”ì¶œëœ ë°ì´í„° ì €ì¥")
            if not self.save_intensity_data():
                logger.error("ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # 5. ì´ë¯¸ì§€ ìƒì„±
            logger.info("5. Intensity ì´ë¯¸ì§€ ìƒì„±")
            if not self.generate_intensity_images():
                logger.error("ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # 6. ê°ì²´ ìœ„ì¹˜ ì˜¤ë²„ë ˆì´ ìƒì„±
            logger.info("6. ê°ì²´ ìœ„ì¹˜ ì˜¤ë²„ë ˆì´ ìƒì„±")
            if not self.create_object_location_overlay():
                logger.error("ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨")
                return False
            
            logger.info("=== ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ===")
            self._print_summary()
            
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False

    def _print_summary(self):
        """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"ğŸ“ GPS ìœ„ì¹˜ ë°ì´í„°: {len(self.gps_data) if self.gps_data is not None else 0}ê°œ ìœ„ì¹˜")
        print(f"ğŸ¯ ê°ì§€ëœ ê°ì²´: {len(self.object_locations)}ê°œ")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°ì…‹: {len(self.intensity_data)}ê°œ")
        
        total_original = sum(len(data['original_data']) for data in self.intensity_data.values())
        total_simulation = sum(len(data['simulation_data']) for data in self.intensity_data.values())
        
        print(f"ğŸ“ Original íŒŒì¼: {total_original}ê°œ")
        print(f"ğŸ”¬ Simulation íŒŒì¼: {total_simulation}ê°œ")
        
        print(f"\nğŸ’¾ ì¶œë ¥ ìœ„ì¹˜:")
        print(f"- Intensity ë°ì´í„°: {self.output_path / 'intensity_data'}")
        print(f"- ì´ë¯¸ì§€: {self.figures_path / 'intensity_images'}")
        print(f"- ê°ì²´ ìœ„ì¹˜: {self.figures_path / 'object_locations'}")
        
        print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = EnhancedDataPipeline()
    
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("âœ… ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return 0
    else:
        print("âŒ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return 1


if __name__ == "__main__":
    exit(main())