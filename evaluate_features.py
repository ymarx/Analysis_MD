#!/usr/bin/env python3
"""
ìƒ˜í”Œ ë°ì´í„°ë¥¼ í™œìš©í•œ íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€

ì‹¤ì œ ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ë°ì´í„°ë¥¼ ëŒ€ì‹ í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„°ë¡œ 
ê° íŠ¹ì§• ì¶”ì¶œê¸°ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
"""

import sys
import numpy as np
# import cv2  # OpenCVëŠ” ì„ íƒì‚¬í•­ìœ¼ë¡œ ì²˜ë¦¬
import matplotlib.pyplot as plt
from pathlib import Path
import time
import logging
from typing import Dict, List, Tuple
import json
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from src.feature_extraction.hog_extractor import MultiScaleHOGExtractor, AdaptiveHOGExtractor
from src.feature_extraction.lbp_extractor import ComprehensiveLBPExtractor
from src.feature_extraction.gabor_extractor import GaborFeatureExtractor, AdaptiveGaborExtractor
from src.feature_extraction.sfs_extractor import EnhancedSfSExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FeatureEvaluator:
    """íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ê¸°"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŠ¹ì§• ì¶”ì¶œê¸° ì´ˆê¸°í™”
        self.extractors = {
            'HOG_MultiScale': MultiScaleHOGExtractor(),
            'HOG_Adaptive': AdaptiveHOGExtractor(),
            'LBP_Comprehensive': ComprehensiveLBPExtractor(),
            'Gabor_Standard': GaborFeatureExtractor(n_frequencies=4, n_orientations=6),
            'Gabor_Adaptive': AdaptiveGaborExtractor(),
            'SfS_Enhanced': EnhancedSfSExtractor()
        }
        
        self.results = {}
        
        logger.info(f"íŠ¹ì§• ì¶”ì¶œ í‰ê°€ê¸° ì´ˆê¸°í™” - {len(self.extractors)}ê°œ ì¶”ì¶œê¸°")
    
    def generate_sample_sonar_data(self, num_samples: int = 50) -> Tuple[List[np.ndarray], List[int]]:
        """
        ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ì´ë“œìŠ¤ìº” ì†Œë‚˜ ë°ì´í„° ìƒì„±
        
        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            Tuple[List[np.ndarray], List[int]]: (ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸, ë¼ë²¨ ë¦¬ìŠ¤íŠ¸)
        """
        logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ì†Œë‚˜ ë°ì´í„° ìƒì„±: {num_samples}ê°œ ìƒ˜í”Œ")
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
        images = []
        labels = []
        
        # ì–‘ì„± ìƒ˜í”Œ ìƒì„± (ê¸°ë¬¼ í¬í•¨)
        for i in range(num_samples // 2):
            # ë² ì´ìŠ¤ í•´ì €ë©´ ìƒì„± (ë‚®ì€ ë°˜ì‚¬ê°•ë„)
            base_intensity = 0.2 + 0.1 * np.random.random()
            image = np.random.normal(base_intensity, 0.05, (128, 128))
            
            # ê¸°ë¬¼ ì¶”ê°€ (ë†’ì€ ë°˜ì‚¬ê°•ë„ì˜ ì›í˜•/íƒ€ì›í˜• ê°ì²´)
            center_x = np.random.randint(30, 98)
            center_y = np.random.randint(30, 98)
            
            # ê¸°ë¬¼ í¬ê¸°ì™€ í˜•íƒœ ëœë¤í™”
            radius_x = np.random.randint(8, 20)
            radius_y = np.random.randint(8, 20)
            
            # íƒ€ì›í˜• ë§ˆìŠ¤í¬ ìƒì„±
            y, x = np.ogrid[:128, :128]
            mask = ((x - center_x) / radius_x)**2 + ((y - center_y) / radius_y)**2 <= 1
            
            # ê¸°ë¬¼ ë°˜ì‚¬ê°•ë„ (0.6-0.9 ë²”ìœ„)
            object_intensity = 0.6 + 0.3 * np.random.random()
            image[mask] = object_intensity + np.random.normal(0, 0.1, np.sum(mask))
            
            # ìŒí–¥ ê·¸ë¦¼ì ì¶”ê°€ (ê¸°ë¬¼ ë’¤ìª½)
            shadow_length = np.random.randint(15, 35)
            shadow_start_y = center_y + radius_y + 2
            shadow_end_y = min(128, shadow_start_y + shadow_length)
            shadow_x_start = max(0, center_x - radius_x//2)
            shadow_x_end = min(128, center_x + radius_x//2)
            
            if shadow_end_y < 128:
                shadow_intensity = 0.05 + 0.05 * np.random.random()
                image[shadow_start_y:shadow_end_y, shadow_x_start:shadow_x_end] = shadow_intensity
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = np.random.normal(0, 0.02, image.shape)
            image += noise
            image = np.clip(image, 0, 1)
            
            images.append(image)
            labels.append(1)  # ê¸°ë¬¼ ìˆìŒ
        
        # ìŒì„± ìƒ˜í”Œ ìƒì„± (ë°°ê²½ë§Œ)
        for i in range(num_samples - num_samples // 2):
            # ë‹¤ì–‘í•œ í•´ì €ë©´ íƒ€ì… ì‹œë®¬ë ˆì´ì…˜
            terrain_type = np.random.choice(['sand', 'mud', 'rock'])
            
            if terrain_type == 'sand':
                base_intensity = 0.4 + 0.2 * np.random.random()
                texture_noise = np.random.normal(0, 0.08, (128, 128))
            elif terrain_type == 'mud':
                base_intensity = 0.2 + 0.1 * np.random.random()
                texture_noise = np.random.normal(0, 0.03, (128, 128))
            else:  # rock
                base_intensity = 0.3 + 0.3 * np.random.random()
                texture_noise = np.random.normal(0, 0.12, (128, 128))
            
            image = np.full((128, 128), base_intensity) + texture_noise
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ì§€í˜• ë³€í™” ì¶”ê°€
            for _ in range(np.random.randint(2, 6)):
                blob_center_x = np.random.randint(10, 118)
                blob_center_y = np.random.randint(10, 118)
                blob_radius = np.random.randint(5, 15)
                
                y, x = np.ogrid[:128, :128]
                blob_mask = (x - blob_center_x)**2 + (y - blob_center_y)**2 <= blob_radius**2
                
                intensity_variation = np.random.uniform(-0.1, 0.1)
                image[blob_mask] += intensity_variation
            
            # ì „ì²´ ë…¸ì´ì¦ˆ
            noise = np.random.normal(0, 0.015, image.shape)
            image += noise
            image = np.clip(image, 0, 1)
            
            images.append(image)
            labels.append(0)  # ê¸°ë¬¼ ì—†ìŒ
        
        # ë°ì´í„° ì„ê¸°
        combined = list(zip(images, labels))
        np.random.shuffle(combined)
        images, labels = zip(*combined)
        
        logger.info(f"ë°ì´í„° ìƒì„± ì™„ë£Œ - ì–‘ì„±: {labels.count(1)}, ìŒì„±: {labels.count(0)}")
        
        return list(images), list(labels)
    
    def extract_features_with_timing(self, extractor_name: str, extractor, images: List[np.ndarray]) -> Dict:
        """
        íƒ€ì´ë°ì„ í¬í•¨í•œ íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            extractor_name: ì¶”ì¶œê¸° ì´ë¦„
            extractor: íŠ¹ì§• ì¶”ì¶œê¸° ê°ì²´
            images: ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì¶”ì¶œ ê²°ê³¼ ë° ì„±ëŠ¥ ì§€í‘œ
        """
        logger.info(f"{extractor_name} íŠ¹ì§• ì¶”ì¶œ ì‹œì‘")
        
        features_list = []
        extraction_times = []
        successful_extractions = 0
        
        start_total_time = time.time()
        
        for i, image in enumerate(images):
            try:
                start_time = time.time()
                
                # ì¶”ì¶œê¸°ë³„ë¡œ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
                if 'HOG' in extractor_name:
                    if 'Adaptive' in extractor_name:
                        features = extractor.extract_adaptive_features(image)
                    else:
                        features = extractor.extract_combined_features(image)
                elif 'LBP' in extractor_name:
                    features = extractor.extract_comprehensive_features(image)
                elif 'Gabor' in extractor_name:
                    if 'Adaptive' in extractor_name:
                        features = extractor.extract_adaptive_features(image)
                    else:
                        features = extractor.extract_comprehensive_features(image)
                elif 'SfS' in extractor_name:
                    features = extractor.extract_comprehensive_sfs_features(image)
                else:
                    logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¶”ì¶œê¸° íƒ€ì…: {extractor_name}")
                    continue
                
                end_time = time.time()
                extraction_time = (end_time - start_time) * 1000  # ms
                
                if len(features) > 0:
                    features_list.append(features)
                    extraction_times.append(extraction_time)
                    successful_extractions += 1
                else:
                    logger.warning(f"{extractor_name}: ë¹ˆ íŠ¹ì§• ë²¡í„° - ì´ë¯¸ì§€ {i}")
                    
            except Exception as e:
                logger.error(f"{extractor_name} ì¶”ì¶œ ì‹¤íŒ¨ - ì´ë¯¸ì§€ {i}: {e}")
                continue
        
        total_time = (time.time() - start_total_time) * 1000  # ms
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        if features_list:
            feature_matrix = np.array(features_list)
            
            results = {
                'extractor_name': extractor_name,
                'successful_extractions': successful_extractions,
                'total_images': len(images),
                'success_rate': successful_extractions / len(images),
                'feature_dimensions': feature_matrix.shape[1],
                'avg_extraction_time_ms': np.mean(extraction_times),
                'std_extraction_time_ms': np.std(extraction_times),
                'total_time_ms': total_time,
                'feature_statistics': {
                    'mean': np.mean(feature_matrix, axis=0).tolist(),
                    'std': np.std(feature_matrix, axis=0).tolist(),
                    'min': np.min(feature_matrix, axis=0).tolist(),
                    'max': np.max(feature_matrix, axis=0).tolist()
                },
                'features': feature_matrix
            }
            
            logger.info(f"{extractor_name} ì™„ë£Œ - ì„±ê³µë¥ : {results['success_rate']:.2%}, "
                       f"íŠ¹ì§• ì°¨ì›: {results['feature_dimensions']}, "
                       f"í‰ê·  ì‹œê°„: {results['avg_extraction_time_ms']:.2f}ms")
        else:
            results = {
                'extractor_name': extractor_name,
                'successful_extractions': 0,
                'total_images': len(images),
                'success_rate': 0.0,
                'error': 'No features extracted'
            }
            logger.error(f"{extractor_name}: ëª¨ë“  íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨")
        
        return results
    
    def evaluate_feature_quality(self, features: np.ndarray, labels: List[int]) -> Dict:
        """
        íŠ¹ì§• í’ˆì§ˆ í‰ê°€
        
        Args:
            features: íŠ¹ì§• í–‰ë ¬
            labels: í´ë˜ìŠ¤ ë ˆì´ë¸”
            
        Returns:
            Dict: í’ˆì§ˆ ì§€í‘œ
        """
        from sklearn.metrics import silhouette_score
        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
        from sklearn.model_selection import cross_val_score
        from sklearn.ensemble import RandomForestClassifier
        
        quality_metrics = {}
        
        try:
            # í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬ë„ (ì‹¤ë£¨ì—£ ì ìˆ˜)
            if len(np.unique(labels)) > 1 and len(features) > 1:
                silhouette = silhouette_score(features, labels)
                quality_metrics['silhouette_score'] = silhouette
            
            # ì„ í˜• íŒë³„ ê°€ëŠ¥ì„±
            if len(np.unique(labels)) > 1:
                lda = LinearDiscriminantAnalysis()
                try:
                    lda.fit(features, labels)
                    quality_metrics['lda_score'] = lda.score(features, labels)
                except Exception as e:
                    quality_metrics['lda_score'] = 0.0
                    logger.warning(f"LDA í‰ê°€ ì‹¤íŒ¨: {e}")
            
            # ëœë¤ í¬ë ˆìŠ¤íŠ¸ êµì°¨ ê²€ì¦
            if len(features) >= 10:  # ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”
                rf = RandomForestClassifier(n_estimators=10, random_state=42)
                cv_scores = cross_val_score(rf, features, labels, cv=min(5, len(features)//2))
                quality_metrics['rf_cv_mean'] = np.mean(cv_scores)
                quality_metrics['rf_cv_std'] = np.std(cv_scores)
            
            # íŠ¹ì§• ë‹¤ì–‘ì„± (ë¶„ì‚° ê¸°ë°˜)
            feature_variance = np.var(features, axis=0)
            quality_metrics['feature_diversity'] = np.mean(feature_variance)
            quality_metrics['feature_stability'] = 1.0 / (1.0 + np.std(feature_variance))
            
            # ì •ê·œì„± í…ŒìŠ¤íŠ¸ (Shapiro-Wilk í…ŒìŠ¤íŠ¸ì˜ ê°„ì†Œí™” ë²„ì „)
            # íŠ¹ì§•ì´ ë„ˆë¬´ ì¹˜ìš°ì³ ìˆì§€ ì•Šì€ì§€ í™•ì¸
            skewness_scores = []
            for i in range(min(10, features.shape[1])):  # ì²˜ìŒ 10ê°œ íŠ¹ì§•ë§Œ í…ŒìŠ¤íŠ¸
                feature_col = features[:, i]
                mean_val = np.mean(feature_col)
                std_val = np.std(feature_col)
                if std_val > 0:
                    skewness = np.mean(((feature_col - mean_val) / std_val) ** 3)
                    skewness_scores.append(abs(skewness))
            
            if skewness_scores:
                quality_metrics['avg_skewness'] = np.mean(skewness_scores)
            
        except Exception as e:
            logger.error(f"íŠ¹ì§• í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            quality_metrics['evaluation_error'] = str(e)
        
        return quality_metrics
    
    def run_comprehensive_evaluation(self, num_samples: int = 50):
        """ì¢…í•©ì ì¸ íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰"""
        logger.info("=== ì¢…í•©ì ì¸ íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ ===")
        
        # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        images, labels = self.generate_sample_sonar_data(num_samples)
        
        # 2. ê° ì¶”ì¶œê¸°ë³„ ì„±ëŠ¥ í‰ê°€
        for extractor_name, extractor in self.extractors.items():
            logger.info(f"\n--- {extractor_name} í‰ê°€ ì‹œì‘ ---")
            
            # íŠ¹ì§• ì¶”ì¶œ ë° íƒ€ì´ë°
            extraction_result = self.extract_features_with_timing(extractor_name, extractor, images)
            
            # íŠ¹ì§• í’ˆì§ˆ í‰ê°€ (ì„±ê³µí•œ ê²½ìš°ì—ë§Œ)
            if extraction_result.get('features') is not None:
                features = extraction_result['features']
                # ì„±ê³µí•œ íŠ¹ì§•ì— ëŒ€ì‘í•˜ëŠ” ë¼ë²¨ë§Œ ì‚¬ìš©
                successful_labels = labels[:len(features)]
                
                quality_metrics = self.evaluate_feature_quality(features, successful_labels)
                extraction_result['quality_metrics'] = quality_metrics
            
            self.results[extractor_name] = extraction_result
        
        # 3. ê²°ê³¼ ì €ì¥ ë° ìš”ì•½
        self.save_results()
        self.generate_comparison_report()
        self.create_visualizations()
        
        logger.info("=== íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ ===")
    
    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        results_to_save = {}
        
        for name, result in self.results.items():
            # NumPy ë°°ì—´ì€ ì œì™¸í•˜ê³  ì €ì¥
            save_result = {k: v for k, v in result.items() if k != 'features'}
            
            # í†µê³„ ì •ë³´ëŠ” ê°„ì†Œí™”
            if 'feature_statistics' in save_result:
                stats = save_result['feature_statistics']
                save_result['feature_statistics'] = {
                    'mean_avg': float(np.mean(stats['mean'])) if stats['mean'] else 0,
                    'std_avg': float(np.mean(stats['std'])) if stats['std'] else 0,
                    'range_avg': float(np.mean(np.array(stats['max']) - np.array(stats['min']))) if stats['max'] and stats['min'] else 0
                }
            
            results_to_save[name] = save_result
        
        # ê²°ê³¼ ì €ì¥
        results_file = self.output_dir / 'feature_extraction_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
    
    def generate_comparison_report(self):
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_file = self.output_dir / 'feature_extraction_report.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**í‰ê°€ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ìš”ì•½ í…Œì´ë¸”
            f.write("## ğŸ“Š ì„±ëŠ¥ ìš”ì•½\n\n")
            f.write("| ì¶”ì¶œê¸° | ì„±ê³µë¥  | íŠ¹ì§• ì°¨ì› | í‰ê·  ì‹œê°„(ms) | ì‹¤ë£¨ì—£ ì ìˆ˜ | RF êµì°¨ê²€ì¦ |\n")
            f.write("|--------|-------|-----------|-------------|------------|------------|\n")
            
            for name, result in self.results.items():
                success_rate = result.get('success_rate', 0) * 100
                dimensions = result.get('feature_dimensions', 0)
                avg_time = result.get('avg_extraction_time_ms', 0)
                
                quality = result.get('quality_metrics', {})
                silhouette = quality.get('silhouette_score', 0)
                rf_cv = quality.get('rf_cv_mean', 0)
                
                f.write(f"| {name} | {success_rate:.1f}% | {dimensions} | {avg_time:.2f} | {silhouette:.3f} | {rf_cv:.3f} |\n")
            
            f.write("\n")
            
            # ìƒì„¸ ê²°ê³¼
            f.write("## ğŸ” ìƒì„¸ í‰ê°€ ê²°ê³¼\n\n")
            
            for name, result in self.results.items():
                f.write(f"### {name}\n\n")
                
                if 'error' in result:
                    f.write(f"âŒ **ì˜¤ë¥˜ ë°œìƒ**: {result['error']}\n\n")
                    continue
                
                f.write(f"- **ì„±ê³µë¥ **: {result.get('success_rate', 0)*100:.1f}%\n")
                f.write(f"- **íŠ¹ì§• ì°¨ì›**: {result.get('feature_dimensions', 0):,}ê°œ\n")
                f.write(f"- **í‰ê·  ì¶”ì¶œ ì‹œê°„**: {result.get('avg_extraction_time_ms', 0):.2f} Â± {result.get('std_extraction_time_ms', 0):.2f}ms\n")
                f.write(f"- **ì´ ì²˜ë¦¬ ì‹œê°„**: {result.get('total_time_ms', 0):.2f}ms\n")
                
                # í’ˆì§ˆ ì§€í‘œ
                quality = result.get('quality_metrics', {})
                if quality:
                    f.write(f"- **ì‹¤ë£¨ì—£ ì ìˆ˜**: {quality.get('silhouette_score', 0):.3f}\n")
                    f.write(f"- **LDA ì ìˆ˜**: {quality.get('lda_score', 0):.3f}\n")
                    f.write(f"- **RF êµì°¨ê²€ì¦**: {quality.get('rf_cv_mean', 0):.3f} Â± {quality.get('rf_cv_std', 0):.3f}\n")
                    f.write(f"- **íŠ¹ì§• ë‹¤ì–‘ì„±**: {quality.get('feature_diversity', 0):.6f}\n")
                    f.write(f"- **íŠ¹ì§• ì•ˆì •ì„±**: {quality.get('feature_stability', 0):.3f}\n")
                
                f.write("\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n")
            
            # ìµœê³  ì„±ëŠ¥ ì¶”ì¶œê¸° ì°¾ê¸°
            best_overall = None
            best_score = -1
            
            for name, result in self.results.items():
                if 'quality_metrics' in result:
                    quality = result['quality_metrics']
                    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì‹¤ë£¨ì—£ + RF êµì°¨ê²€ì¦)
                    score = quality.get('silhouette_score', 0) + quality.get('rf_cv_mean', 0)
                    if score > best_score:
                        best_score = score
                        best_overall = name
            
            if best_overall:
                f.write(f"ğŸ† **ìµœê³  ì„±ëŠ¥**: {best_overall}\n")
                
            # ì†ë„ë³„ ì¶”ì²œ
            fastest = min(self.results.items(), 
                         key=lambda x: x[1].get('avg_extraction_time_ms', float('inf')))
            f.write(f"âš¡ **ìµœê³  ì†ë„**: {fastest[0]} ({fastest[1].get('avg_extraction_time_ms', 0):.2f}ms)\n")
            
            # ì°¨ì›ë³„ ì¶”ì²œ
            highest_dim = max(self.results.items(), 
                             key=lambda x: x[1].get('feature_dimensions', 0))
            f.write(f"ğŸ“Š **ìµœê³  ì°¨ì›**: {highest_dim[0]} ({highest_dim[1].get('feature_dimensions', 0)}ì°¨ì›)\n")
        
        logger.info(f"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")
    
    def create_visualizations(self):
        """ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìƒì„±"""
        try:
            # í•œê¸€ í°íŠ¸ ì„¤ì •
            plt.rcParams['font.family'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # 1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            extractors = []
            success_rates = []
            dimensions = []
            times = []
            silhouette_scores = []
            
            for name, result in self.results.items():
                if 'error' not in result:
                    extractors.append(name.replace('_', '\n'))
                    success_rates.append(result.get('success_rate', 0) * 100)
                    dimensions.append(result.get('feature_dimensions', 0))
                    times.append(result.get('avg_extraction_time_ms', 0))
                    
                    quality = result.get('quality_metrics', {})
                    silhouette_scores.append(quality.get('silhouette_score', 0))
            
            # ì„±ê³µë¥ 
            ax1.bar(extractors, success_rates, color='skyblue')
            ax1.set_title('Feature Extraction Success Rate')
            ax1.set_ylabel('Success Rate (%)')
            ax1.tick_params(axis='x', rotation=45)
            
            # íŠ¹ì§• ì°¨ì›
            ax2.bar(extractors, dimensions, color='lightgreen')
            ax2.set_title('Feature Dimensions')
            ax2.set_ylabel('Number of Features')
            ax2.tick_params(axis='x', rotation=45)
            
            # ì¶”ì¶œ ì‹œê°„
            ax3.bar(extractors, times, color='orange')
            ax3.set_title('Average Extraction Time')
            ax3.set_ylabel('Time (ms)')
            ax3.tick_params(axis='x', rotation=45)
            
            # ì‹¤ë£¨ì—£ ì ìˆ˜
            ax4.bar(extractors, silhouette_scores, color='pink')
            ax4.set_title('Silhouette Score (Feature Quality)')
            ax4.set_ylabel('Silhouette Score')
            ax4.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(self.output_dir / 'feature_extraction_comparison.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("ì‹œê°í™” ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = Path("data/results/feature_evaluation")
    
    # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
    evaluator = FeatureEvaluator(output_dir)
    evaluator.run_comprehensive_evaluation(num_samples=60)
    
    print(f"\nğŸ‰ íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ í™•ì¸: {output_dir}")
    print(f"ğŸ“ˆ ë¦¬í¬íŠ¸: {output_dir / 'feature_extraction_report.md'}")
    print(f"ğŸ” ìƒì„¸ ê²°ê³¼: {output_dir / 'feature_extraction_results.json'}")


if __name__ == "__main__":
    main()