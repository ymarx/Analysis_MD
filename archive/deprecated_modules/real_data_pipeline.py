#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„°ë¥¼ ì´ìš©í•œ ê¸°ë¢° íƒì§€ íŒŒì´í”„ë¼ì¸

ê¸°ì¡´ ìƒ˜í”Œ ë°ì´í„° ëŒ€ì‹  datasets í´ë”ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬
intensity data íŒ¨í‚· ì¶”ì¶œ, ê¸°ë¬¼ ìœ„ì¹˜ ë§¤í•‘, íŠ¹ì§• ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import sys
import numpy as np
import pandas as pd
import cv2
from pathlib import Path
import json
import logging
from datetime import datetime
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from typing import List, Tuple, Dict, Optional
import warnings

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from config.paths import PathManager
    from src.data_processing.xtf_reader import XTFReader
    from src.data_processing.sonar_data_processor import SonarDataProcessor
    from src.feature_extraction.basic_features import BasicFeatureExtractor
except ImportError as e:
    print(f"Warning: Some modules not available: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class RealDataPipeline:
    """ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.path_manager = PathManager()
        self.datasets_path = self.path_manager.datasets
        self.output_path = self.path_manager.processed_data
        
        # GPS ë°ì´í„°ì™€ ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ
        self.gps_data = self._load_gps_data()
        self.annotation_image = self._load_annotation_image()
        self.object_locations = self._extract_object_locations()
        
    def _load_gps_data(self) -> Optional[pd.DataFrame]:
        """GPS ë°ì´í„° ë¡œë“œ"""
        gps_file = self.datasets_path / 'Location_MDGPS.xlsx'
        try:
            df = pd.read_excel(gps_file)
            logger.info(f"GPS ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]}ê°œ ì¢Œí‘œ")
            return df
        except Exception as e:
            logger.error(f"GPS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_annotation_image(self) -> Optional[np.ndarray]:
        """ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ"""
        # BMPë¥¼ PNGë¡œ ë³€í™˜í–ˆìœ¼ë¯€ë¡œ PNG íŒŒì¼ ì‚¬ìš©
        annotation_file = self.datasets_path / 'PH_annotation.png'
        try:
            image = cv2.imread(str(annotation_file))
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                logger.info(f"ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {image.shape}")
                return image
            else:
                logger.error("ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return None
        except Exception as e:
            logger.error(f"ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_object_locations(self) -> List[Dict]:
        """ì–´ë…¸í…Œì´ì…˜ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ"""
        if self.annotation_image is None:
            return []
        
        locations = []
        try:
            # ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ ê°ì§€ (BGRì—ì„œ RGBë¡œ ë³€í™˜í–ˆìœ¼ë¯€ë¡œ RGB ê¸°ì¤€)
            # ë¹¨ê°„ìƒ‰ ë²”ìœ„ ì„¤ì •
            red_lower = np.array([200, 0, 0])
            red_upper = np.array([255, 100, 100])
            
            # ë¹¨ê°„ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
            mask = cv2.inRange(self.annotation_image, red_lower, red_upper)
            
            # ì»¨íˆ¬ì–´ ì°¾ê¸°
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            for i, contour in enumerate(contours):
                x, y, w, h = cv2.boundingRect(contour)
                if w > 10 and h > 10:  # ìµœì†Œ í¬ê¸° í•„í„°
                    locations.append({
                        'id': i + 1,
                        'bbox': (x, y, w, h),
                        'center': (x + w//2, y + h//2)
                    })
            
            logger.info(f"ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(locations)}")
            return locations
            
        except Exception as e:
            logger.error(f"ê°ì²´ ìœ„ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def list_available_datasets(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡"""
        datasets = []
        for dataset_dir in self.datasets_path.iterdir():
            if dataset_dir.is_dir() and not dataset_dir.name.startswith('.'):
                datasets.append(dataset_dir.name)
        return datasets
    
    def extract_intensity_data(self, dataset_name: str, data_type: str = 'original') -> Dict:
        """Intensity ë°ì´í„° ì¶”ì¶œ"""
        logger.info(f"Intensity ë°ì´í„° ì¶”ì¶œ ì‹œì‘: {dataset_name} ({data_type})")
        
        dataset_path = self.datasets_path / dataset_name / data_type
        if not dataset_path.exists():
            logger.error(f"ë°ì´í„°ì…‹ ê²½ë¡œ ì—†ìŒ: {dataset_path}")
            return {'error': f'Dataset path not found: {dataset_path}'}
        
        # XTF íŒŒì¼ ì°¾ê¸°
        xtf_files = list(dataset_path.glob('*.xtf'))
        if not xtf_files:
            logger.error(f"XTF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_path}")
            return {'error': f'No XTF files found in {dataset_path}'}
        
        results = {}
        for xtf_file in xtf_files:
            try:
                logger.info(f"XTF íŒŒì¼ ì²˜ë¦¬ ì¤‘: {xtf_file.name}")
                
                # XTF ë¦¬ë” ì‚¬ìš© (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                try:
                    reader = XTFReader(str(xtf_file))
                    intensity_data = reader.read_intensity_data()
                    
                    if intensity_data:
                        results[xtf_file.name] = {
                            'file_path': str(xtf_file),
                            'data_shape': intensity_data.shape if hasattr(intensity_data, 'shape') else 'unknown',
                            'extraction_time': datetime.now().isoformat(),
                            'success': True
                        }
                        
                        # ë°ì´í„° ì €ì¥
                        output_file = self.output_path / f"{xtf_file.stem}_intensity.npy"
                        np.save(output_file, intensity_data)
                        results[xtf_file.name]['output_file'] = str(output_file)
                        
                    else:
                        results[xtf_file.name] = {'error': 'No intensity data extracted', 'success': False}
                        
                except Exception as e:
                    logger.warning(f"XTF ë¦¬ë” ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")
                    # ê¸°ë³¸ ì²˜ë¦¬ ë°©ë²•
                    results[xtf_file.name] = {
                        'error': f'XTF reader failed: {e}',
                        'file_path': str(xtf_file),
                        'success': False
                    }
                    
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {xtf_file.name}: {e}")
                results[xtf_file.name] = {'error': str(e), 'success': False}
        
        return results
    
    def map_objects_to_pixels(self) -> List[Dict]:
        """GPS ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë§¤í•‘"""
        if self.gps_data is None or not self.object_locations:
            logger.error("GPS ë°ì´í„° ë˜ëŠ” ê°ì²´ ìœ„ì¹˜ ì •ë³´ ì—†ìŒ")
            return []
        
        mapped_objects = []
        
        # GPS ë°ì´í„°ì™€ ê°ì§€ëœ ê°ì²´ ë§¤ì¹­
        for i, (_, gps_row) in enumerate(self.gps_data.iterrows()):
            if i < len(self.object_locations):
                obj_loc = self.object_locations[i]
                
                mapped_objects.append({
                    'point_id': gps_row['ì •ì '],
                    'gps_lat': gps_row['ìœ„ë„'],
                    'gps_lon': gps_row['ê²½ë„'],
                    'pixel_center': obj_loc['center'],
                    'pixel_bbox': obj_loc['bbox'],
                    'annotation_id': obj_loc['id']
                })
        
        logger.info(f"ë§¤í•‘ëœ ê°ì²´ ìˆ˜: {len(mapped_objects)}")
        return mapped_objects
    
    def visualize_objects(self, save_path: Optional[Path] = None) -> None:
        """ê°ì²´ ìœ„ì¹˜ ì‹œê°í™”"""
        if self.annotation_image is None or not self.object_locations:
            logger.error("ì‹œê°í™”í•  ë°ì´í„° ì—†ìŒ")
            return
        
        fig, ax = plt.subplots(1, 1, figsize=(15, 20))
        ax.imshow(self.annotation_image)
        ax.set_title('ê¸°ë¬¼ ìœ„ì¹˜ ë° ë°”ìš´ë”© ë°•ìŠ¤', fontsize=16)
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        for obj_loc in self.object_locations:
            x, y, w, h = obj_loc['bbox']
            center_x, center_y = obj_loc['center']
            
            # ë°”ìš´ë”© ë°•ìŠ¤
            rect = patches.Rectangle((x, y), w, h, linewidth=2, 
                                   edgecolor='yellow', facecolor='none')
            ax.add_patch(rect)
            
            # ì¤‘ì‹¬ì 
            ax.plot(center_x, center_y, 'ro', markersize=8)
            ax.text(center_x + 10, center_y, f"ID:{obj_loc['id']}", 
                   fontsize=10, color='white', 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor='black', alpha=0.7))
        
        ax.axis('off')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"ì‹œê°í™” ê²°ê³¼ ì €ì¥: {save_path}")
        
        plt.show()
    
    def extract_features_from_objects(self, intensity_data: np.ndarray) -> List[Dict]:
        """ê°ì§€ëœ ê°ì²´ë“¤ë¡œë¶€í„° íŠ¹ì§• ì¶”ì¶œ"""
        if not self.object_locations:
            logger.error("ê°ì²´ ìœ„ì¹˜ ì •ë³´ ì—†ìŒ")
            return []
        
        features = []
        try:
            feature_extractor = BasicFeatureExtractor()
            
            for obj_loc in self.object_locations:
                x, y, w, h = obj_loc['bbox']
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ ì¶”ì¶œ (ì•ˆì „í•œ ë²”ìœ„ ë‚´ì—ì„œ)
                y1, y2 = max(0, y), min(intensity_data.shape[0], y + h)
                x1, x2 = max(0, x), min(intensity_data.shape[1], x + w)
                
                if y2 > y1 and x2 > x1:
                    roi = intensity_data[y1:y2, x1:x2]
                    
                    # 96x96ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
                    roi_resized = cv2.resize(roi, (96, 96))
                    
                    # íŠ¹ì§• ì¶”ì¶œ
                    obj_features = feature_extractor.extract_features(roi_resized)
                    
                    features.append({
                        'object_id': obj_loc['id'],
                        'bbox': obj_loc['bbox'],
                        'features': obj_features,
                        'roi_shape': roi.shape
                    })
            
            logger.info(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(features)}ê°œ ê°ì²´")
            return features
            
        except Exception as e:
            logger.error(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def run_complete_pipeline(self) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=== ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        results = {
            'start_time': datetime.now().isoformat(),
            'datasets_processed': [],
            'object_mapping': [],
            'feature_extraction': [],
            'errors': []
        }
        
        try:
            # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡
            datasets = self.list_available_datasets()
            logger.info(f"ë°œê²¬ëœ ë°ì´í„°ì…‹: {len(datasets)}ê°œ")
            
            # 2. ê° ë°ì´í„°ì…‹ì—ì„œ intensity ë°ì´í„° ì¶”ì¶œ
            for dataset in datasets:
                if 'Location_MDGPS' in dataset or 'PH_annotation' in dataset:
                    continue  # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì€ ìŠ¤í‚µ
                
                logger.info(f"ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘: {dataset}")
                
                # Original ë°ì´í„° ì²˜ë¦¬
                original_results = self.extract_intensity_data(dataset, 'original')
                
                # Simulation ë°ì´í„° ì²˜ë¦¬ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
                simulation_results = {}
                simulation_path = self.datasets_path / dataset / 'simulation'
                if simulation_path.exists():
                    for sim_dir in simulation_path.iterdir():
                        if sim_dir.is_dir():
                            sim_results = self.extract_intensity_data(dataset, f'simulation/{sim_dir.name}')
                            simulation_results[sim_dir.name] = sim_results
                
                dataset_result = {
                    'dataset_name': dataset,
                    'original': original_results,
                    'simulation': simulation_results,
                    'processing_time': datetime.now().isoformat()
                }
                
                results['datasets_processed'].append(dataset_result)
            
            # 3. ê°ì²´ ìœ„ì¹˜ ë§¤í•‘
            object_mapping = self.map_objects_to_pixels()
            results['object_mapping'] = object_mapping
            
            # 4. ì‹œê°í™” ìƒì„±
            viz_path = self.output_path / 'object_visualization.png'
            self.visualize_objects(viz_path)
            results['visualization_path'] = str(viz_path)
            
            # 5. ê²°ê³¼ ì €ì¥
            results_file = self.output_path / 'real_data_pipeline_results.json'
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            results['results_file'] = str(results_file)
            results['end_time'] = datetime.now().isoformat()
            
            logger.info("=== ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            
        except Exception as e:
            error_msg = f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            results['errors'].append(error_msg)
        
        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        pipeline = RealDataPipeline()
        
        print("ğŸš€ ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: {pipeline.datasets_path}")
        print(f"ğŸ“Š GPS ë°ì´í„°: {pipeline.gps_data.shape[0] if pipeline.gps_data is not None else 0}ê°œ ì¢Œí‘œ")
        print(f"ğŸ¯ ê°ì§€ëœ ê°ì²´: {len(pipeline.object_locations)}ê°œ")
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        results = pipeline.run_complete_pipeline()
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        print(f"ğŸ“‹ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹: {len(results['datasets_processed'])}ê°œ")
        print(f"ğŸ¯ ë§¤í•‘ëœ ê°ì²´: {len(results['object_mapping'])}ê°œ")
        
        if results['errors']:
            print(f"âš ï¸ ì˜¤ë¥˜: {len(results['errors'])}ê°œ")
            for error in results['errors']:
                print(f"   - {error}")
        
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼: {results.get('results_file', 'N/A')}")
        print(f"ğŸ–¼ï¸ ì‹œê°í™”: {results.get('visualization_path', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()